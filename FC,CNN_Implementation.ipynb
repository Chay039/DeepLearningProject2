{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcvvFRP5iT-x",
        "outputId": "d67990d2-308a-4c7a-f9fe-dcd1bc7ac08a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Configuration: {'hidden_sizes': [32, 16], 'num_layers': 2, 'lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
            "Epoch 1, Batch 100, Loss: 2.2925168442726136\n",
            "Epoch 1, Batch 200, Loss: 2.260127696990967\n",
            "Epoch 1, Batch 300, Loss: 2.227011594772339\n",
            "Epoch 1, Batch 400, Loss: 2.190540807247162\n",
            "Epoch 2, Batch 100, Loss: 2.121371352672577\n",
            "Epoch 2, Batch 200, Loss: 2.071970522403717\n",
            "Epoch 2, Batch 300, Loss: 2.0217204797267914\n",
            "Epoch 2, Batch 400, Loss: 1.9669514966011048\n",
            "Epoch 3, Batch 100, Loss: 1.8560229694843293\n",
            "Epoch 3, Batch 200, Loss: 1.7918583726882935\n",
            "Epoch 3, Batch 300, Loss: 1.7303901398181916\n",
            "Epoch 3, Batch 400, Loss: 1.6620946264266967\n",
            "Epoch 4, Batch 100, Loss: 1.5573035025596618\n",
            "Epoch 4, Batch 200, Loss: 1.5056550598144531\n",
            "Epoch 4, Batch 300, Loss: 1.454721747636795\n",
            "Epoch 4, Batch 400, Loss: 1.4043286514282227\n",
            "Epoch 5, Batch 100, Loss: 1.3301444196701049\n",
            "Epoch 5, Batch 200, Loss: 1.2845047998428345\n",
            "Epoch 5, Batch 300, Loss: 1.2425997507572175\n",
            "Epoch 5, Batch 400, Loss: 1.1993910145759583\n",
            "Epoch 6, Batch 100, Loss: 1.1546500098705292\n",
            "Epoch 6, Batch 200, Loss: 1.1215438413619996\n",
            "Epoch 6, Batch 300, Loss: 1.0892946642637253\n",
            "Epoch 6, Batch 400, Loss: 1.0591434049606323\n",
            "Epoch 7, Batch 100, Loss: 1.0222900652885436\n",
            "Epoch 7, Batch 200, Loss: 1.0032746082544326\n",
            "Epoch 7, Batch 300, Loss: 0.9886372327804566\n",
            "Epoch 7, Batch 400, Loss: 0.9669069880247116\n",
            "Epoch 8, Batch 100, Loss: 0.9340995246171951\n",
            "Epoch 8, Batch 200, Loss: 0.9190254855155945\n",
            "Epoch 8, Batch 300, Loss: 0.9112782979011536\n",
            "Epoch 8, Batch 400, Loss: 0.913913152217865\n",
            "Epoch 9, Batch 100, Loss: 0.8782834899425507\n",
            "Epoch 9, Batch 200, Loss: 0.8725041401386261\n",
            "Epoch 9, Batch 300, Loss: 0.8632405281066895\n",
            "Epoch 9, Batch 400, Loss: 0.8469460254907608\n",
            "Epoch 10, Batch 100, Loss: 0.8416064178943634\n",
            "Epoch 10, Batch 200, Loss: 0.8226908558607101\n",
            "Epoch 10, Batch 300, Loss: 0.8183408695459365\n",
            "Epoch 10, Batch 400, Loss: 0.7976102179288864\n",
            "Epoch 11, Batch 100, Loss: 0.7894983565807343\n",
            "Epoch 11, Batch 200, Loss: 0.7880689245462418\n",
            "Epoch 11, Batch 300, Loss: 0.777189495563507\n",
            "Epoch 11, Batch 400, Loss: 0.7709035676717758\n",
            "Epoch 12, Batch 100, Loss: 0.7662254416942597\n",
            "Epoch 12, Batch 200, Loss: 0.7543136560916901\n",
            "Epoch 12, Batch 300, Loss: 0.7508822286128998\n",
            "Epoch 12, Batch 400, Loss: 0.749118789434433\n",
            "Epoch 13, Batch 100, Loss: 0.7321028250455857\n",
            "Epoch 13, Batch 200, Loss: 0.7188611876964569\n",
            "Epoch 13, Batch 300, Loss: 0.7402015030384064\n",
            "Epoch 13, Batch 400, Loss: 0.7406255918741226\n",
            "Epoch 14, Batch 100, Loss: 0.7200433099269867\n",
            "Epoch 14, Batch 200, Loss: 0.7247459661960601\n",
            "Epoch 14, Batch 300, Loss: 0.7068713760375976\n",
            "Epoch 14, Batch 400, Loss: 0.7126455914974212\n",
            "Epoch 15, Batch 100, Loss: 0.7003362256288529\n",
            "Epoch 15, Batch 200, Loss: 0.6916496205329895\n",
            "Epoch 15, Batch 300, Loss: 0.7090744262933731\n",
            "Epoch 15, Batch 400, Loss: 0.683155098259449\n",
            "Epoch 16, Batch 100, Loss: 0.6913852727413178\n",
            "Epoch 16, Batch 200, Loss: 0.6920366126298905\n",
            "Epoch 16, Batch 300, Loss: 0.6675476992130279\n",
            "Epoch 16, Batch 400, Loss: 0.6745835256576538\n",
            "Epoch 17, Batch 100, Loss: 0.6748454856872559\n",
            "Epoch 17, Batch 200, Loss: 0.6732176476716996\n",
            "Epoch 17, Batch 300, Loss: 0.6828460982441902\n",
            "Epoch 17, Batch 400, Loss: 0.6676033839583397\n",
            "Epoch 18, Batch 100, Loss: 0.6661281496286392\n",
            "Epoch 18, Batch 200, Loss: 0.667268031835556\n",
            "Epoch 18, Batch 300, Loss: 0.6598647746443749\n",
            "Epoch 18, Batch 400, Loss: 0.6538948562741279\n",
            "Epoch 19, Batch 100, Loss: 0.6525631046295166\n",
            "Epoch 19, Batch 200, Loss: 0.6610637849569321\n",
            "Epoch 19, Batch 300, Loss: 0.6511424162983894\n",
            "Epoch 19, Batch 400, Loss: 0.6396785709261894\n",
            "Epoch 20, Batch 100, Loss: 0.6435851603746414\n",
            "Epoch 20, Batch 200, Loss: 0.6444713851809502\n",
            "Epoch 20, Batch 300, Loss: 0.6410772889852524\n",
            "Epoch 20, Batch 400, Loss: 0.6410266584157944\n",
            "Epoch 21, Batch 100, Loss: 0.6275150090456009\n",
            "Epoch 21, Batch 200, Loss: 0.6432940810918808\n",
            "Epoch 21, Batch 300, Loss: 0.6337429392337799\n",
            "Epoch 21, Batch 400, Loss: 0.6162007182836533\n",
            "Epoch 22, Batch 100, Loss: 0.6245453256368637\n",
            "Epoch 22, Batch 200, Loss: 0.6237555128335953\n",
            "Epoch 22, Batch 300, Loss: 0.6324657294154167\n",
            "Epoch 22, Batch 400, Loss: 0.6183563274145126\n",
            "Epoch 23, Batch 100, Loss: 0.6316860166192054\n",
            "Epoch 23, Batch 200, Loss: 0.6156688287854195\n",
            "Epoch 23, Batch 300, Loss: 0.6148865926265716\n",
            "Epoch 23, Batch 400, Loss: 0.6057849398255348\n",
            "Epoch 24, Batch 100, Loss: 0.6180603075027465\n",
            "Epoch 24, Batch 200, Loss: 0.6081821265816688\n",
            "Epoch 24, Batch 300, Loss: 0.6150045612454415\n",
            "Epoch 24, Batch 400, Loss: 0.6068038722872734\n",
            "Epoch 25, Batch 100, Loss: 0.6023602494597435\n",
            "Epoch 25, Batch 200, Loss: 0.6009210526943207\n",
            "Epoch 25, Batch 300, Loss: 0.6082182124257087\n",
            "Epoch 25, Batch 400, Loss: 0.6030689680576324\n",
            "Finished Training\n",
            "Accuracy on test set: 0.7702%\n",
            "\n",
            "Configuration: {'hidden_sizes': [32, 16], 'num_layers': 2, 'lr': 0.002, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
            "Epoch 1, Batch 100, Loss: 0.982436625957489\n",
            "Epoch 1, Batch 200, Loss: 0.569617218375206\n",
            "Epoch 1, Batch 300, Loss: 0.5421225446462631\n",
            "Epoch 1, Batch 400, Loss: 0.5060300460457802\n",
            "Epoch 2, Batch 100, Loss: 0.46187920302152635\n",
            "Epoch 2, Batch 200, Loss: 0.4424548587203026\n",
            "Epoch 2, Batch 300, Loss: 0.4384930911660194\n",
            "Epoch 2, Batch 400, Loss: 0.4233595314621925\n",
            "Epoch 3, Batch 100, Loss: 0.41383891731500627\n",
            "Epoch 3, Batch 200, Loss: 0.4131854224205017\n",
            "Epoch 3, Batch 300, Loss: 0.4048860704898834\n",
            "Epoch 3, Batch 400, Loss: 0.39478542923927307\n",
            "Epoch 4, Batch 100, Loss: 0.38274293422698974\n",
            "Epoch 4, Batch 200, Loss: 0.3869190815091133\n",
            "Epoch 4, Batch 300, Loss: 0.37171661615371704\n",
            "Epoch 4, Batch 400, Loss: 0.3810315914452076\n",
            "Epoch 5, Batch 100, Loss: 0.3458718428015709\n",
            "Epoch 5, Batch 200, Loss: 0.3662691982090473\n",
            "Epoch 5, Batch 300, Loss: 0.3714490810036659\n",
            "Epoch 5, Batch 400, Loss: 0.3661379262804985\n",
            "Epoch 6, Batch 100, Loss: 0.3450982443988323\n",
            "Epoch 6, Batch 200, Loss: 0.3515057355165482\n",
            "Epoch 6, Batch 300, Loss: 0.34539093568921087\n",
            "Epoch 6, Batch 400, Loss: 0.34951842844486236\n",
            "Epoch 7, Batch 100, Loss: 0.3377384172379971\n",
            "Epoch 7, Batch 200, Loss: 0.33258975833654403\n",
            "Epoch 7, Batch 300, Loss: 0.35070960968732834\n",
            "Epoch 7, Batch 400, Loss: 0.3392328850924969\n",
            "Epoch 8, Batch 100, Loss: 0.32192090824246405\n",
            "Epoch 8, Batch 200, Loss: 0.3252722974121571\n",
            "Epoch 8, Batch 300, Loss: 0.32214790388941766\n",
            "Epoch 8, Batch 400, Loss: 0.3157237645983696\n",
            "Epoch 9, Batch 100, Loss: 0.31436872884631156\n",
            "Epoch 9, Batch 200, Loss: 0.301532901674509\n",
            "Epoch 9, Batch 300, Loss: 0.33189373210072515\n",
            "Epoch 9, Batch 400, Loss: 0.32143883913755417\n",
            "Epoch 10, Batch 100, Loss: 0.3027922423183918\n",
            "Epoch 10, Batch 200, Loss: 0.3136942557990551\n",
            "Epoch 10, Batch 300, Loss: 0.3134975190460682\n",
            "Epoch 10, Batch 400, Loss: 0.30522810176014903\n",
            "Epoch 11, Batch 100, Loss: 0.2957848536968231\n",
            "Epoch 11, Batch 200, Loss: 0.3016514524817467\n",
            "Epoch 11, Batch 300, Loss: 0.3059278604388237\n",
            "Epoch 11, Batch 400, Loss: 0.30202966019511224\n",
            "Epoch 12, Batch 100, Loss: 0.29242673233151434\n",
            "Epoch 12, Batch 200, Loss: 0.30622469037771227\n",
            "Epoch 12, Batch 300, Loss: 0.291538412719965\n",
            "Epoch 12, Batch 400, Loss: 0.2974171908199787\n",
            "Epoch 13, Batch 100, Loss: 0.29505543380975724\n",
            "Epoch 13, Batch 200, Loss: 0.2945936058461666\n",
            "Epoch 13, Batch 300, Loss: 0.2881960095465183\n",
            "Epoch 13, Batch 400, Loss: 0.2924600078165531\n",
            "Epoch 14, Batch 100, Loss: 0.29075952857732773\n",
            "Epoch 14, Batch 200, Loss: 0.2961833743751049\n",
            "Epoch 14, Batch 300, Loss: 0.2852428996562958\n",
            "Epoch 14, Batch 400, Loss: 0.2798289829492569\n",
            "Epoch 15, Batch 100, Loss: 0.28064083829522135\n",
            "Epoch 15, Batch 200, Loss: 0.2908783532679081\n",
            "Epoch 15, Batch 300, Loss: 0.2901826013624668\n",
            "Epoch 15, Batch 400, Loss: 0.27275872498750686\n",
            "Epoch 16, Batch 100, Loss: 0.26477448411285875\n",
            "Epoch 16, Batch 200, Loss: 0.26946855172514916\n",
            "Epoch 16, Batch 300, Loss: 0.28565216779708863\n",
            "Epoch 16, Batch 400, Loss: 0.28710269510746\n",
            "Epoch 17, Batch 100, Loss: 0.2741333144903183\n",
            "Epoch 17, Batch 200, Loss: 0.2778399197757244\n",
            "Epoch 17, Batch 300, Loss: 0.2803647609055042\n",
            "Epoch 17, Batch 400, Loss: 0.2660803438723087\n",
            "Epoch 18, Batch 100, Loss: 0.26117623373866083\n",
            "Epoch 18, Batch 200, Loss: 0.27118772208690645\n",
            "Epoch 18, Batch 300, Loss: 0.280943988263607\n",
            "Epoch 18, Batch 400, Loss: 0.2668847991526127\n",
            "Epoch 19, Batch 100, Loss: 0.2772806447744369\n",
            "Epoch 19, Batch 200, Loss: 0.2742424659430981\n",
            "Epoch 19, Batch 300, Loss: 0.2539467678964138\n",
            "Epoch 19, Batch 400, Loss: 0.2770113733410835\n",
            "Epoch 20, Batch 100, Loss: 0.2611443315446377\n",
            "Epoch 20, Batch 200, Loss: 0.2613059450685978\n",
            "Epoch 20, Batch 300, Loss: 0.2652589812874794\n",
            "Epoch 20, Batch 400, Loss: 0.2702234861254692\n",
            "Epoch 21, Batch 100, Loss: 0.2517353888601065\n",
            "Epoch 21, Batch 200, Loss: 0.2637541109323502\n",
            "Epoch 21, Batch 300, Loss: 0.2576222674548626\n",
            "Epoch 21, Batch 400, Loss: 0.2735062627494335\n",
            "Epoch 22, Batch 100, Loss: 0.25139459893107413\n",
            "Epoch 22, Batch 200, Loss: 0.25813942864537237\n",
            "Epoch 22, Batch 300, Loss: 0.2689820794761181\n",
            "Epoch 22, Batch 400, Loss: 0.264071152806282\n",
            "Epoch 23, Batch 100, Loss: 0.2512292142212391\n",
            "Epoch 23, Batch 200, Loss: 0.26351671740412713\n",
            "Epoch 23, Batch 300, Loss: 0.2595859287679195\n",
            "Epoch 23, Batch 400, Loss: 0.25600834265351297\n",
            "Epoch 24, Batch 100, Loss: 0.24136319153010846\n",
            "Epoch 24, Batch 200, Loss: 0.25021552622318266\n",
            "Epoch 24, Batch 300, Loss: 0.2550483323633671\n",
            "Epoch 24, Batch 400, Loss: 0.25924112178385256\n",
            "Epoch 25, Batch 100, Loss: 0.24693903252482413\n",
            "Epoch 25, Batch 200, Loss: 0.25079639345407484\n",
            "Epoch 25, Batch 300, Loss: 0.26405744984745977\n",
            "Epoch 25, Batch 400, Loss: 0.24384374514222146\n",
            "Finished Training\n",
            "Accuracy on test set: 0.8701%\n",
            "\n",
            "Configuration: {'hidden_sizes': [32, 16], 'num_layers': 2, 'lr': 0.003, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
            "Epoch 1, Batch 100, Loss: 0.9256009829044342\n",
            "Epoch 1, Batch 200, Loss: 0.5927867066860198\n",
            "Epoch 1, Batch 300, Loss: 0.5288626384735108\n",
            "Epoch 1, Batch 400, Loss: 0.4981854572892189\n",
            "Epoch 2, Batch 100, Loss: 0.4396559715270996\n",
            "Epoch 2, Batch 200, Loss: 0.44837865501642227\n",
            "Epoch 2, Batch 300, Loss: 0.43220997601747513\n",
            "Epoch 2, Batch 400, Loss: 0.41145885080099104\n",
            "Epoch 3, Batch 100, Loss: 0.3904407638311386\n",
            "Epoch 3, Batch 200, Loss: 0.4036158362030983\n",
            "Epoch 3, Batch 300, Loss: 0.3919336876273155\n",
            "Epoch 3, Batch 400, Loss: 0.3898182365298271\n",
            "Epoch 4, Batch 100, Loss: 0.3704865923523903\n",
            "Epoch 4, Batch 200, Loss: 0.3791062016785145\n",
            "Epoch 4, Batch 300, Loss: 0.36532885327935216\n",
            "Epoch 4, Batch 400, Loss: 0.36761827290058136\n",
            "Epoch 5, Batch 100, Loss: 0.34167580991983415\n",
            "Epoch 5, Batch 200, Loss: 0.35484137311577796\n",
            "Epoch 5, Batch 300, Loss: 0.36425278887152673\n",
            "Epoch 5, Batch 400, Loss: 0.3588022966682911\n",
            "Epoch 6, Batch 100, Loss: 0.3454316048324108\n",
            "Epoch 6, Batch 200, Loss: 0.328222786039114\n",
            "Epoch 6, Batch 300, Loss: 0.35563870891928673\n",
            "Epoch 6, Batch 400, Loss: 0.3545277884602547\n",
            "Epoch 7, Batch 100, Loss: 0.33542198181152344\n",
            "Epoch 7, Batch 200, Loss: 0.34251406833529474\n",
            "Epoch 7, Batch 300, Loss: 0.33990223929286\n",
            "Epoch 7, Batch 400, Loss: 0.34302246674895287\n",
            "Epoch 8, Batch 100, Loss: 0.3394816218316555\n",
            "Epoch 8, Batch 200, Loss: 0.33469047516584394\n",
            "Epoch 8, Batch 300, Loss: 0.3351168882846832\n",
            "Epoch 8, Batch 400, Loss: 0.32200563594698906\n",
            "Epoch 9, Batch 100, Loss: 0.3264123371243477\n",
            "Epoch 9, Batch 200, Loss: 0.32801827043294907\n",
            "Epoch 9, Batch 300, Loss: 0.32039635092020036\n",
            "Epoch 9, Batch 400, Loss: 0.31508727848529816\n",
            "Epoch 10, Batch 100, Loss: 0.3234846158325672\n",
            "Epoch 10, Batch 200, Loss: 0.30873005613684656\n",
            "Epoch 10, Batch 300, Loss: 0.3183508178591728\n",
            "Epoch 10, Batch 400, Loss: 0.3194288395345211\n",
            "Epoch 11, Batch 100, Loss: 0.2954339239001274\n",
            "Epoch 11, Batch 200, Loss: 0.3202909465134144\n",
            "Epoch 11, Batch 300, Loss: 0.3166842789947987\n",
            "Epoch 11, Batch 400, Loss: 0.3180764290690422\n",
            "Epoch 12, Batch 100, Loss: 0.3192411483824253\n",
            "Epoch 12, Batch 200, Loss: 0.3129242667555809\n",
            "Epoch 12, Batch 300, Loss: 0.30702934473752974\n",
            "Epoch 12, Batch 400, Loss: 0.29923468559980393\n",
            "Epoch 13, Batch 100, Loss: 0.29247366517782214\n",
            "Epoch 13, Batch 200, Loss: 0.2960757976770401\n",
            "Epoch 13, Batch 300, Loss: 0.3109598375856876\n",
            "Epoch 13, Batch 400, Loss: 0.31135886564850807\n",
            "Epoch 14, Batch 100, Loss: 0.2924822556972504\n",
            "Epoch 14, Batch 200, Loss: 0.30652785420417783\n",
            "Epoch 14, Batch 300, Loss: 0.2964476077258587\n",
            "Epoch 14, Batch 400, Loss: 0.30039628967642784\n",
            "Epoch 15, Batch 100, Loss: 0.28867981269955634\n",
            "Epoch 15, Batch 200, Loss: 0.2997562621533871\n",
            "Epoch 15, Batch 300, Loss: 0.29059995159506796\n",
            "Epoch 15, Batch 400, Loss: 0.29508707016706465\n",
            "Epoch 16, Batch 100, Loss: 0.28768667742609977\n",
            "Epoch 16, Batch 200, Loss: 0.29397170290350916\n",
            "Epoch 16, Batch 300, Loss: 0.2804783795773983\n",
            "Epoch 16, Batch 400, Loss: 0.2908857476711273\n",
            "Epoch 17, Batch 100, Loss: 0.27191120214760306\n",
            "Epoch 17, Batch 200, Loss: 0.28983610957860945\n",
            "Epoch 17, Batch 300, Loss: 0.29097859278321264\n",
            "Epoch 17, Batch 400, Loss: 0.2955657202005386\n",
            "Epoch 18, Batch 100, Loss: 0.28128135576844215\n",
            "Epoch 18, Batch 200, Loss: 0.2843724028766155\n",
            "Epoch 18, Batch 300, Loss: 0.2858914569020271\n",
            "Epoch 18, Batch 400, Loss: 0.2878016881644726\n",
            "Epoch 19, Batch 100, Loss: 0.27015185847878453\n",
            "Epoch 19, Batch 200, Loss: 0.2784324012696743\n",
            "Epoch 19, Batch 300, Loss: 0.28322006404399874\n",
            "Epoch 19, Batch 400, Loss: 0.2868493364751339\n",
            "Epoch 20, Batch 100, Loss: 0.276693577170372\n",
            "Epoch 20, Batch 200, Loss: 0.28327229276299476\n",
            "Epoch 20, Batch 300, Loss: 0.2742093050479889\n",
            "Epoch 20, Batch 400, Loss: 0.2867664958536625\n",
            "Epoch 21, Batch 100, Loss: 0.27214411422610285\n",
            "Epoch 21, Batch 200, Loss: 0.27539201959967613\n",
            "Epoch 21, Batch 300, Loss: 0.2784965726733208\n",
            "Epoch 21, Batch 400, Loss: 0.28171816274523737\n",
            "Epoch 22, Batch 100, Loss: 0.2700877180695534\n",
            "Epoch 22, Batch 200, Loss: 0.27106705695390704\n",
            "Epoch 22, Batch 300, Loss: 0.27772036358714103\n",
            "Epoch 22, Batch 400, Loss: 0.28201603159308436\n",
            "Epoch 23, Batch 100, Loss: 0.2664992716908455\n",
            "Epoch 23, Batch 200, Loss: 0.2772434692084789\n",
            "Epoch 23, Batch 300, Loss: 0.27103048980236055\n",
            "Epoch 23, Batch 400, Loss: 0.2686145535111427\n",
            "Epoch 24, Batch 100, Loss: 0.26395806677639483\n",
            "Epoch 24, Batch 200, Loss: 0.2780212281644344\n",
            "Epoch 24, Batch 300, Loss: 0.27476917326450345\n",
            "Epoch 24, Batch 400, Loss: 0.27039082437753675\n",
            "Epoch 25, Batch 100, Loss: 0.2546482031047344\n",
            "Epoch 25, Batch 200, Loss: 0.2637072879076004\n",
            "Epoch 25, Batch 300, Loss: 0.273445413261652\n",
            "Epoch 25, Batch 400, Loss: 0.2673516886681318\n",
            "Finished Training\n",
            "Accuracy on test set: 0.8585%\n",
            "\n",
            "Configuration: {'hidden_sizes': [64, 32, 16], 'num_layers': 3, 'lr': 0.002, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
            "Epoch 1, Batch 100, Loss: 0.973546599149704\n",
            "Epoch 1, Batch 200, Loss: 0.5721301633119583\n",
            "Epoch 1, Batch 300, Loss: 0.5303671014308929\n",
            "Epoch 1, Batch 400, Loss: 0.47579881429672244\n",
            "Epoch 2, Batch 100, Loss: 0.4382833716273308\n",
            "Epoch 2, Batch 200, Loss: 0.4242999193072319\n",
            "Epoch 2, Batch 300, Loss: 0.41616522908210757\n",
            "Epoch 2, Batch 400, Loss: 0.40486423298716545\n",
            "Epoch 3, Batch 100, Loss: 0.37852149158716203\n",
            "Epoch 3, Batch 200, Loss: 0.38148816123604773\n",
            "Epoch 3, Batch 300, Loss: 0.36659391775727274\n",
            "Epoch 3, Batch 400, Loss: 0.3672616671025753\n",
            "Epoch 4, Batch 100, Loss: 0.35003127336502077\n",
            "Epoch 4, Batch 200, Loss: 0.35522723957896235\n",
            "Epoch 4, Batch 300, Loss: 0.345880871117115\n",
            "Epoch 4, Batch 400, Loss: 0.3452038303017616\n",
            "Epoch 5, Batch 100, Loss: 0.3254935333132744\n",
            "Epoch 5, Batch 200, Loss: 0.3291200885176659\n",
            "Epoch 5, Batch 300, Loss: 0.34897280544042586\n",
            "Epoch 5, Batch 400, Loss: 0.3312483811378479\n",
            "Epoch 6, Batch 100, Loss: 0.3134669470787048\n",
            "Epoch 6, Batch 200, Loss: 0.32236302256584165\n",
            "Epoch 6, Batch 300, Loss: 0.31227194488048554\n",
            "Epoch 6, Batch 400, Loss: 0.316369342058897\n",
            "Epoch 7, Batch 100, Loss: 0.3008954244852066\n",
            "Epoch 7, Batch 200, Loss: 0.29770299419760704\n",
            "Epoch 7, Batch 300, Loss: 0.2995704048871994\n",
            "Epoch 7, Batch 400, Loss: 0.3140756358206272\n",
            "Epoch 8, Batch 100, Loss: 0.2872983635962009\n",
            "Epoch 8, Batch 200, Loss: 0.29576615899801256\n",
            "Epoch 8, Batch 300, Loss: 0.2925255087018013\n",
            "Epoch 8, Batch 400, Loss: 0.3018505756556988\n",
            "Epoch 9, Batch 100, Loss: 0.29010301172733305\n",
            "Epoch 9, Batch 200, Loss: 0.28873901516199113\n",
            "Epoch 9, Batch 300, Loss: 0.2855152077972889\n",
            "Epoch 9, Batch 400, Loss: 0.28427225902676584\n",
            "Epoch 10, Batch 100, Loss: 0.2757037253677845\n",
            "Epoch 10, Batch 200, Loss: 0.28205508485436437\n",
            "Epoch 10, Batch 300, Loss: 0.27318431466817855\n",
            "Epoch 10, Batch 400, Loss: 0.2795326516032219\n",
            "Epoch 11, Batch 100, Loss: 0.26783647388219833\n",
            "Epoch 11, Batch 200, Loss: 0.26230939865112307\n",
            "Epoch 11, Batch 300, Loss: 0.2733230938017368\n",
            "Epoch 11, Batch 400, Loss: 0.2733636751770973\n",
            "Epoch 12, Batch 100, Loss: 0.25445542991161346\n",
            "Epoch 12, Batch 200, Loss: 0.2810921511054039\n",
            "Epoch 12, Batch 300, Loss: 0.25971159726381304\n",
            "Epoch 12, Batch 400, Loss: 0.2714606983214617\n",
            "Epoch 13, Batch 100, Loss: 0.24400927737355232\n",
            "Epoch 13, Batch 200, Loss: 0.25784872502088546\n",
            "Epoch 13, Batch 300, Loss: 0.27563052490353585\n",
            "Epoch 13, Batch 400, Loss: 0.25679103456437585\n",
            "Epoch 14, Batch 100, Loss: 0.24008587263524533\n",
            "Epoch 14, Batch 200, Loss: 0.24665792226791383\n",
            "Epoch 14, Batch 300, Loss: 0.25674360647797584\n",
            "Epoch 14, Batch 400, Loss: 0.2614006422460079\n",
            "Epoch 15, Batch 100, Loss: 0.2475243102759123\n",
            "Epoch 15, Batch 200, Loss: 0.24260676845908166\n",
            "Epoch 15, Batch 300, Loss: 0.24161748461425303\n",
            "Epoch 15, Batch 400, Loss: 0.25116115346550943\n",
            "Epoch 16, Batch 100, Loss: 0.24265982806682587\n",
            "Epoch 16, Batch 200, Loss: 0.24160940170288087\n",
            "Epoch 16, Batch 300, Loss: 0.23764003857970237\n",
            "Epoch 16, Batch 400, Loss: 0.24382326550781727\n",
            "Epoch 17, Batch 100, Loss: 0.2319453153759241\n",
            "Epoch 17, Batch 200, Loss: 0.23378556303679943\n",
            "Epoch 17, Batch 300, Loss: 0.2480912620574236\n",
            "Epoch 17, Batch 400, Loss: 0.24478166356682776\n",
            "Epoch 18, Batch 100, Loss: 0.22232935957610608\n",
            "Epoch 18, Batch 200, Loss: 0.24244238883256913\n",
            "Epoch 18, Batch 300, Loss: 0.2329872428625822\n",
            "Epoch 18, Batch 400, Loss: 0.24049368374049662\n",
            "Epoch 19, Batch 100, Loss: 0.22551217131316662\n",
            "Epoch 19, Batch 200, Loss: 0.2245452669262886\n",
            "Epoch 19, Batch 300, Loss: 0.22700489640235902\n",
            "Epoch 19, Batch 400, Loss: 0.23466871321201324\n",
            "Epoch 20, Batch 100, Loss: 0.21821154706180096\n",
            "Epoch 20, Batch 200, Loss: 0.213337709158659\n",
            "Epoch 20, Batch 300, Loss: 0.23258069075644017\n",
            "Epoch 20, Batch 400, Loss: 0.23009257934987545\n",
            "Epoch 21, Batch 100, Loss: 0.20986653804779054\n",
            "Epoch 21, Batch 200, Loss: 0.209977770075202\n",
            "Epoch 21, Batch 300, Loss: 0.22002058297395707\n",
            "Epoch 21, Batch 400, Loss: 0.22075218118727208\n",
            "Epoch 22, Batch 100, Loss: 0.2116349820047617\n",
            "Epoch 22, Batch 200, Loss: 0.20127381138503553\n",
            "Epoch 22, Batch 300, Loss: 0.21772458858788013\n",
            "Epoch 22, Batch 400, Loss: 0.2170521153509617\n",
            "Epoch 23, Batch 100, Loss: 0.20269668072462083\n",
            "Epoch 23, Batch 200, Loss: 0.21019865632057189\n",
            "Epoch 23, Batch 300, Loss: 0.2046725933998823\n",
            "Epoch 23, Batch 400, Loss: 0.22252734154462814\n",
            "Epoch 24, Batch 100, Loss: 0.2147839441895485\n",
            "Epoch 24, Batch 200, Loss: 0.20320792235434054\n",
            "Epoch 24, Batch 300, Loss: 0.2113198284059763\n",
            "Epoch 24, Batch 400, Loss: 0.2048357828706503\n",
            "Epoch 25, Batch 100, Loss: 0.22111017033457755\n",
            "Epoch 25, Batch 200, Loss: 0.20647869624197482\n",
            "Epoch 25, Batch 300, Loss: 0.20063625402748586\n",
            "Epoch 25, Batch 400, Loss: 0.2027498907595873\n",
            "Finished Training\n",
            "Accuracy on test set: 0.8741%\n",
            "\n",
            "Configuration: {'hidden_sizes': [64, 32, 16], 'num_layers': 3, 'lr': 0.003, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
            "Epoch 1, Batch 100, Loss: 0.9915519931912422\n",
            "Epoch 1, Batch 200, Loss: 0.604322310090065\n",
            "Epoch 1, Batch 300, Loss: 0.5211849626898766\n",
            "Epoch 1, Batch 400, Loss: 0.49236049860715864\n",
            "Epoch 2, Batch 100, Loss: 0.44723584443330766\n",
            "Epoch 2, Batch 200, Loss: 0.42525883704423906\n",
            "Epoch 2, Batch 300, Loss: 0.41678031474351884\n",
            "Epoch 2, Batch 400, Loss: 0.41398792147636415\n",
            "Epoch 3, Batch 100, Loss: 0.3896709859371185\n",
            "Epoch 3, Batch 200, Loss: 0.3888764579594135\n",
            "Epoch 3, Batch 300, Loss: 0.3730653882026672\n",
            "Epoch 3, Batch 400, Loss: 0.3734764590859413\n",
            "Epoch 4, Batch 100, Loss: 0.3663224107027054\n",
            "Epoch 4, Batch 200, Loss: 0.35622296810150145\n",
            "Epoch 4, Batch 300, Loss: 0.3653016112744808\n",
            "Epoch 4, Batch 400, Loss: 0.3465016843378544\n",
            "Epoch 5, Batch 100, Loss: 0.3453875966370106\n",
            "Epoch 5, Batch 200, Loss: 0.3353191715478897\n",
            "Epoch 5, Batch 300, Loss: 0.32845954969525337\n",
            "Epoch 5, Batch 400, Loss: 0.3394746582210064\n",
            "Epoch 6, Batch 100, Loss: 0.3220832838118076\n",
            "Epoch 6, Batch 200, Loss: 0.325222472846508\n",
            "Epoch 6, Batch 300, Loss: 0.33061759546399117\n",
            "Epoch 6, Batch 400, Loss: 0.32118464425206183\n",
            "Epoch 7, Batch 100, Loss: 0.32422117233276365\n",
            "Epoch 7, Batch 200, Loss: 0.31051990419626235\n",
            "Epoch 7, Batch 300, Loss: 0.31427367106080056\n",
            "Epoch 7, Batch 400, Loss: 0.31327348098158836\n",
            "Epoch 8, Batch 100, Loss: 0.3142610940337181\n",
            "Epoch 8, Batch 200, Loss: 0.30696325331926344\n",
            "Epoch 8, Batch 300, Loss: 0.30229632675647733\n",
            "Epoch 8, Batch 400, Loss: 0.3012302269041538\n",
            "Epoch 9, Batch 100, Loss: 0.2987965908646584\n",
            "Epoch 9, Batch 200, Loss: 0.2912455989420414\n",
            "Epoch 9, Batch 300, Loss: 0.29722617417573927\n",
            "Epoch 9, Batch 400, Loss: 0.30383009880781175\n",
            "Epoch 10, Batch 100, Loss: 0.2791403129696846\n",
            "Epoch 10, Batch 200, Loss: 0.29673413395881654\n",
            "Epoch 10, Batch 300, Loss: 0.29496029287576675\n",
            "Epoch 10, Batch 400, Loss: 0.2952029429376125\n",
            "Epoch 11, Batch 100, Loss: 0.274562720656395\n",
            "Epoch 11, Batch 200, Loss: 0.29164652243256567\n",
            "Epoch 11, Batch 300, Loss: 0.2830164685845375\n",
            "Epoch 11, Batch 400, Loss: 0.29276459209620953\n",
            "Epoch 12, Batch 100, Loss: 0.2778229807317257\n",
            "Epoch 12, Batch 200, Loss: 0.2755871251225471\n",
            "Epoch 12, Batch 300, Loss: 0.28056334614753725\n",
            "Epoch 12, Batch 400, Loss: 0.27596958801150323\n",
            "Epoch 13, Batch 100, Loss: 0.26262824922800065\n",
            "Epoch 13, Batch 200, Loss: 0.26110555708408356\n",
            "Epoch 13, Batch 300, Loss: 0.27313573271036146\n",
            "Epoch 13, Batch 400, Loss: 0.28600175186991694\n",
            "Epoch 14, Batch 100, Loss: 0.26588722988963126\n",
            "Epoch 14, Batch 200, Loss: 0.2664772829413414\n",
            "Epoch 14, Batch 300, Loss: 0.26656580954790116\n",
            "Epoch 14, Batch 400, Loss: 0.25836138918995855\n",
            "Epoch 15, Batch 100, Loss: 0.2590427942574024\n",
            "Epoch 15, Batch 200, Loss: 0.25872560918331144\n",
            "Epoch 15, Batch 300, Loss: 0.2620584032684565\n",
            "Epoch 15, Batch 400, Loss: 0.2562550187110901\n",
            "Epoch 16, Batch 100, Loss: 0.24683580711483954\n",
            "Epoch 16, Batch 200, Loss: 0.24608636297285558\n",
            "Epoch 16, Batch 300, Loss: 0.25478279739618304\n",
            "Epoch 16, Batch 400, Loss: 0.2698825491964817\n",
            "Epoch 17, Batch 100, Loss: 0.2474426069110632\n",
            "Epoch 17, Batch 200, Loss: 0.2617986963689327\n",
            "Epoch 17, Batch 300, Loss: 0.24962924011051654\n",
            "Epoch 17, Batch 400, Loss: 0.24854101955890656\n",
            "Epoch 18, Batch 100, Loss: 0.23306893058121203\n",
            "Epoch 18, Batch 200, Loss: 0.2501233496516943\n",
            "Epoch 18, Batch 300, Loss: 0.2616816780716181\n",
            "Epoch 18, Batch 400, Loss: 0.24896884575486183\n",
            "Epoch 19, Batch 100, Loss: 0.2320753761380911\n",
            "Epoch 19, Batch 200, Loss: 0.23999943673610688\n",
            "Epoch 19, Batch 300, Loss: 0.24245654806494712\n",
            "Epoch 19, Batch 400, Loss: 0.2515012766420841\n",
            "Epoch 20, Batch 100, Loss: 0.23794902101159096\n",
            "Epoch 20, Batch 200, Loss: 0.24997467711567878\n",
            "Epoch 20, Batch 300, Loss: 0.2364655827730894\n",
            "Epoch 20, Batch 400, Loss: 0.23852344274520873\n",
            "Epoch 21, Batch 100, Loss: 0.23323142163455488\n",
            "Epoch 21, Batch 200, Loss: 0.24571457937359809\n",
            "Epoch 21, Batch 300, Loss: 0.23353119015693666\n",
            "Epoch 21, Batch 400, Loss: 0.2440690780431032\n",
            "Epoch 22, Batch 100, Loss: 0.22751933678984643\n",
            "Epoch 22, Batch 200, Loss: 0.22263158313930034\n",
            "Epoch 22, Batch 300, Loss: 0.24353451296687126\n",
            "Epoch 22, Batch 400, Loss: 0.261592675819993\n",
            "Epoch 23, Batch 100, Loss: 0.21811606794595717\n",
            "Epoch 23, Batch 200, Loss: 0.21989619232714175\n",
            "Epoch 23, Batch 300, Loss: 0.22767058044672012\n",
            "Epoch 23, Batch 400, Loss: 0.24105194367468358\n",
            "Epoch 24, Batch 100, Loss: 0.21771934315562247\n",
            "Epoch 24, Batch 200, Loss: 0.22684150837361813\n",
            "Epoch 24, Batch 300, Loss: 0.2306735908985138\n",
            "Epoch 24, Batch 400, Loss: 0.2394757829606533\n",
            "Epoch 25, Batch 100, Loss: 0.22444547571241855\n",
            "Epoch 25, Batch 200, Loss: 0.2198512287437916\n",
            "Epoch 25, Batch 300, Loss: 0.2253047987818718\n",
            "Epoch 25, Batch 400, Loss: 0.22225803770124913\n",
            "Finished Training\n",
            "Accuracy on test set: 0.8716%\n",
            "\n",
            "Configuration: {'hidden_sizes': [128, 64, 32, 16], 'num_layers': 4, 'lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
            "Epoch 1, Batch 100, Loss: 2.3116284298896788\n",
            "Epoch 1, Batch 200, Loss: 2.3110689544677734\n",
            "Epoch 1, Batch 300, Loss: 2.309134514331818\n",
            "Epoch 1, Batch 400, Loss: 2.309288351535797\n",
            "Epoch 2, Batch 100, Loss: 2.3063105940818787\n",
            "Epoch 2, Batch 200, Loss: 2.3060704946517943\n",
            "Epoch 2, Batch 300, Loss: 2.3058998155593873\n",
            "Epoch 2, Batch 400, Loss: 2.3041533184051515\n",
            "Epoch 3, Batch 100, Loss: 2.304005038738251\n",
            "Epoch 3, Batch 200, Loss: 2.299566524028778\n",
            "Epoch 3, Batch 300, Loss: 2.300366396903992\n",
            "Epoch 3, Batch 400, Loss: 2.299050190448761\n",
            "Epoch 4, Batch 100, Loss: 2.2990753865242004\n",
            "Epoch 4, Batch 200, Loss: 2.2968611097335816\n",
            "Epoch 4, Batch 300, Loss: 2.2963296508789064\n",
            "Epoch 4, Batch 400, Loss: 2.296058700084686\n",
            "Epoch 5, Batch 100, Loss: 2.2930340361595154\n",
            "Epoch 5, Batch 200, Loss: 2.293728415966034\n",
            "Epoch 5, Batch 300, Loss: 2.293088126182556\n",
            "Epoch 5, Batch 400, Loss: 2.291114540100098\n",
            "Epoch 6, Batch 100, Loss: 2.289185631275177\n",
            "Epoch 6, Batch 200, Loss: 2.288461136817932\n",
            "Epoch 6, Batch 300, Loss: 2.2863769793510436\n",
            "Epoch 6, Batch 400, Loss: 2.2860513710975647\n",
            "Epoch 7, Batch 100, Loss: 2.2842434239387512\n",
            "Epoch 7, Batch 200, Loss: 2.282759277820587\n",
            "Epoch 7, Batch 300, Loss: 2.280972044467926\n",
            "Epoch 7, Batch 400, Loss: 2.2799886417388917\n",
            "Epoch 8, Batch 100, Loss: 2.279428403377533\n",
            "Epoch 8, Batch 200, Loss: 2.2761545467376707\n",
            "Epoch 8, Batch 300, Loss: 2.2745420432090757\n",
            "Epoch 8, Batch 400, Loss: 2.2728955292701722\n",
            "Epoch 9, Batch 100, Loss: 2.269711494445801\n",
            "Epoch 9, Batch 200, Loss: 2.269283607006073\n",
            "Epoch 9, Batch 300, Loss: 2.266641290187836\n",
            "Epoch 9, Batch 400, Loss: 2.2641570711135866\n",
            "Epoch 10, Batch 100, Loss: 2.2614187359809876\n",
            "Epoch 10, Batch 200, Loss: 2.2586127018928526\n",
            "Epoch 10, Batch 300, Loss: 2.2569234275817873\n",
            "Epoch 10, Batch 400, Loss: 2.2550207924842836\n",
            "Epoch 11, Batch 100, Loss: 2.250116379261017\n",
            "Epoch 11, Batch 200, Loss: 2.246736032962799\n",
            "Epoch 11, Batch 300, Loss: 2.2444595551490782\n",
            "Epoch 11, Batch 400, Loss: 2.238952267169952\n",
            "Epoch 12, Batch 100, Loss: 2.2328377294540407\n",
            "Epoch 12, Batch 200, Loss: 2.229391849040985\n",
            "Epoch 12, Batch 300, Loss: 2.2247260546684267\n",
            "Epoch 12, Batch 400, Loss: 2.218771421909332\n",
            "Epoch 13, Batch 100, Loss: 2.2090367174148557\n",
            "Epoch 13, Batch 200, Loss: 2.200017988681793\n",
            "Epoch 13, Batch 300, Loss: 2.191906623840332\n",
            "Epoch 13, Batch 400, Loss: 2.1858460569381712\n",
            "Epoch 14, Batch 100, Loss: 2.170041744709015\n",
            "Epoch 14, Batch 200, Loss: 2.1597198009490968\n",
            "Epoch 14, Batch 300, Loss: 2.1465292954444886\n",
            "Epoch 14, Batch 400, Loss: 2.1348721766471863\n",
            "Epoch 15, Batch 100, Loss: 2.1182686495780945\n",
            "Epoch 15, Batch 200, Loss: 2.1001148223876953\n",
            "Epoch 15, Batch 300, Loss: 2.082241587638855\n",
            "Epoch 15, Batch 400, Loss: 2.069138069152832\n",
            "Epoch 16, Batch 100, Loss: 2.0423297703266146\n",
            "Epoch 16, Batch 200, Loss: 2.0210299730300902\n",
            "Epoch 16, Batch 300, Loss: 1.9916776061058044\n",
            "Epoch 16, Batch 400, Loss: 1.9759735703468322\n",
            "Epoch 17, Batch 100, Loss: 1.9398196291923524\n",
            "Epoch 17, Batch 200, Loss: 1.913860113620758\n",
            "Epoch 17, Batch 300, Loss: 1.8935663270950318\n",
            "Epoch 17, Batch 400, Loss: 1.8524216258525847\n",
            "Epoch 18, Batch 100, Loss: 1.81481196641922\n",
            "Epoch 18, Batch 200, Loss: 1.7876560807228088\n",
            "Epoch 18, Batch 300, Loss: 1.7693827509880067\n",
            "Epoch 18, Batch 400, Loss: 1.7385340774059295\n",
            "Epoch 19, Batch 100, Loss: 1.6909533870220184\n",
            "Epoch 19, Batch 200, Loss: 1.668776479959488\n",
            "Epoch 19, Batch 300, Loss: 1.6241829478740692\n",
            "Epoch 19, Batch 400, Loss: 1.5965750288963318\n",
            "Epoch 20, Batch 100, Loss: 1.5497752392292024\n",
            "Epoch 20, Batch 200, Loss: 1.5267263495922088\n",
            "Epoch 20, Batch 300, Loss: 1.4873090136051177\n",
            "Epoch 20, Batch 400, Loss: 1.4577548480033875\n",
            "Epoch 21, Batch 100, Loss: 1.4065904343128204\n",
            "Epoch 21, Batch 200, Loss: 1.3922678482532502\n",
            "Epoch 21, Batch 300, Loss: 1.359531157016754\n",
            "Epoch 21, Batch 400, Loss: 1.3177501034736634\n",
            "Epoch 22, Batch 100, Loss: 1.2814219355583192\n",
            "Epoch 22, Batch 200, Loss: 1.267405103445053\n",
            "Epoch 22, Batch 300, Loss: 1.2370035564899444\n",
            "Epoch 22, Batch 400, Loss: 1.2160193622112274\n",
            "Epoch 23, Batch 100, Loss: 1.179481453895569\n",
            "Epoch 23, Batch 200, Loss: 1.1539359974861145\n",
            "Epoch 23, Batch 300, Loss: 1.1372701156139373\n",
            "Epoch 23, Batch 400, Loss: 1.1210240590572358\n",
            "Epoch 24, Batch 100, Loss: 1.0905419582128524\n",
            "Epoch 24, Batch 200, Loss: 1.0635490489006043\n",
            "Epoch 24, Batch 300, Loss: 1.0638394391536712\n",
            "Epoch 24, Batch 400, Loss: 1.0362498670816422\n",
            "Epoch 25, Batch 100, Loss: 1.0065029418468476\n",
            "Epoch 25, Batch 200, Loss: 1.013561760187149\n",
            "Epoch 25, Batch 300, Loss: 0.9791064095497132\n",
            "Epoch 25, Batch 400, Loss: 0.9845047628879547\n",
            "Finished Training\n",
            "Accuracy on test set: 0.6741%\n",
            "\n",
            "Configuration: {'hidden_sizes': [128, 64, 32, 16], 'num_layers': 4, 'lr': 0.003, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
            "Epoch 1, Batch 100, Loss: 1.0744447934627532\n",
            "Epoch 1, Batch 200, Loss: 0.6200274714827537\n",
            "Epoch 1, Batch 300, Loss: 0.525015652179718\n",
            "Epoch 1, Batch 400, Loss: 0.49243716061115267\n",
            "Epoch 2, Batch 100, Loss: 0.4365201753377914\n",
            "Epoch 2, Batch 200, Loss: 0.4287949204444885\n",
            "Epoch 2, Batch 300, Loss: 0.4179976727068424\n",
            "Epoch 2, Batch 400, Loss: 0.4111949193477631\n",
            "Epoch 3, Batch 100, Loss: 0.3803730630874634\n",
            "Epoch 3, Batch 200, Loss: 0.37775603055953977\n",
            "Epoch 3, Batch 300, Loss: 0.3761074621975422\n",
            "Epoch 3, Batch 400, Loss: 0.380692028850317\n",
            "Epoch 4, Batch 100, Loss: 0.3510742823779583\n",
            "Epoch 4, Batch 200, Loss: 0.34764106750488283\n",
            "Epoch 4, Batch 300, Loss: 0.34912295937538146\n",
            "Epoch 4, Batch 400, Loss: 0.34841938883066176\n",
            "Epoch 5, Batch 100, Loss: 0.32447137221693995\n",
            "Epoch 5, Batch 200, Loss: 0.3298909440636635\n",
            "Epoch 5, Batch 300, Loss: 0.33161870762705803\n",
            "Epoch 5, Batch 400, Loss: 0.3284281352162361\n",
            "Epoch 6, Batch 100, Loss: 0.3203257989883423\n",
            "Epoch 6, Batch 200, Loss: 0.31660077676177023\n",
            "Epoch 6, Batch 300, Loss: 0.31366152107715606\n",
            "Epoch 6, Batch 400, Loss: 0.308766987323761\n",
            "Epoch 7, Batch 100, Loss: 0.31111774265766146\n",
            "Epoch 7, Batch 200, Loss: 0.310973529368639\n",
            "Epoch 7, Batch 300, Loss: 0.2934749056398869\n",
            "Epoch 7, Batch 400, Loss: 0.30016844749450683\n",
            "Epoch 8, Batch 100, Loss: 0.28918895319104193\n",
            "Epoch 8, Batch 200, Loss: 0.29534876987338066\n",
            "Epoch 8, Batch 300, Loss: 0.2882038243114948\n",
            "Epoch 8, Batch 400, Loss: 0.29010349735617635\n",
            "Epoch 9, Batch 100, Loss: 0.2697037386894226\n",
            "Epoch 9, Batch 200, Loss: 0.2972300483286381\n",
            "Epoch 9, Batch 300, Loss: 0.2821832451224327\n",
            "Epoch 9, Batch 400, Loss: 0.2952281320095062\n",
            "Epoch 10, Batch 100, Loss: 0.26708251625299456\n",
            "Epoch 10, Batch 200, Loss: 0.2771319729089737\n",
            "Epoch 10, Batch 300, Loss: 0.2733310228586197\n",
            "Epoch 10, Batch 400, Loss: 0.27817723631858826\n",
            "Epoch 11, Batch 100, Loss: 0.26534130960702895\n",
            "Epoch 11, Batch 200, Loss: 0.2618703584372997\n",
            "Epoch 11, Batch 300, Loss: 0.27766094252467155\n",
            "Epoch 11, Batch 400, Loss: 0.2734550516307354\n",
            "Epoch 12, Batch 100, Loss: 0.24638789221644403\n",
            "Epoch 12, Batch 200, Loss: 0.2674904881417751\n",
            "Epoch 12, Batch 300, Loss: 0.25997614435851574\n",
            "Epoch 12, Batch 400, Loss: 0.2723487664759159\n",
            "Epoch 13, Batch 100, Loss: 0.2506101068109274\n",
            "Epoch 13, Batch 200, Loss: 0.2589463895559311\n",
            "Epoch 13, Batch 300, Loss: 0.27704065665602684\n",
            "Epoch 13, Batch 400, Loss: 0.2523829233646393\n",
            "Epoch 14, Batch 100, Loss: 0.24196334302425385\n",
            "Epoch 14, Batch 200, Loss: 0.24489302270114421\n",
            "Epoch 14, Batch 300, Loss: 0.2535667023062706\n",
            "Epoch 14, Batch 400, Loss: 0.2558625217527151\n",
            "Epoch 15, Batch 100, Loss: 0.23985549114644528\n",
            "Epoch 15, Batch 200, Loss: 0.2447525630146265\n",
            "Epoch 15, Batch 300, Loss: 0.24386419087648392\n",
            "Epoch 15, Batch 400, Loss: 0.25186963498592374\n",
            "Epoch 16, Batch 100, Loss: 0.23775806099176408\n",
            "Epoch 16, Batch 200, Loss: 0.24434171319007875\n",
            "Epoch 16, Batch 300, Loss: 0.22934386879205704\n",
            "Epoch 16, Batch 400, Loss: 0.23207126230001449\n",
            "Epoch 17, Batch 100, Loss: 0.23149561636149885\n",
            "Epoch 17, Batch 200, Loss: 0.23071749299764632\n",
            "Epoch 17, Batch 300, Loss: 0.2405939208716154\n",
            "Epoch 17, Batch 400, Loss: 0.23291843749582766\n",
            "Epoch 18, Batch 100, Loss: 0.2247910349071026\n",
            "Epoch 18, Batch 200, Loss: 0.23504641585052014\n",
            "Epoch 18, Batch 300, Loss: 0.22792913630604744\n",
            "Epoch 18, Batch 400, Loss: 0.2265376429259777\n",
            "Epoch 19, Batch 100, Loss: 0.21451227307319642\n",
            "Epoch 19, Batch 200, Loss: 0.22328531809151173\n",
            "Epoch 19, Batch 300, Loss: 0.2212130720168352\n",
            "Epoch 19, Batch 400, Loss: 0.24058931469917297\n",
            "Epoch 20, Batch 100, Loss: 0.20733494609594344\n",
            "Epoch 20, Batch 200, Loss: 0.22011351369321347\n",
            "Epoch 20, Batch 300, Loss: 0.2221337554603815\n",
            "Epoch 20, Batch 400, Loss: 0.23326995007693768\n",
            "Epoch 21, Batch 100, Loss: 0.21202655591070652\n",
            "Epoch 21, Batch 200, Loss: 0.21977203086018562\n",
            "Epoch 21, Batch 300, Loss: 0.2097905096411705\n",
            "Epoch 21, Batch 400, Loss: 0.2216677848994732\n",
            "Epoch 22, Batch 100, Loss: 0.20606037944555283\n",
            "Epoch 22, Batch 200, Loss: 0.21229019671678542\n",
            "Epoch 22, Batch 300, Loss: 0.21399017304182053\n",
            "Epoch 22, Batch 400, Loss: 0.23034160323441027\n",
            "Epoch 23, Batch 100, Loss: 0.19874220989644528\n",
            "Epoch 23, Batch 200, Loss: 0.21120807081460952\n",
            "Epoch 23, Batch 300, Loss: 0.21752234682440758\n",
            "Epoch 23, Batch 400, Loss: 0.21078920654952527\n",
            "Epoch 24, Batch 100, Loss: 0.20073365069925786\n",
            "Epoch 24, Batch 200, Loss: 0.1986231389641762\n",
            "Epoch 24, Batch 300, Loss: 0.20166655093431474\n",
            "Epoch 24, Batch 400, Loss: 0.21962822817265987\n",
            "Epoch 25, Batch 100, Loss: 0.20083893530070782\n",
            "Epoch 25, Batch 200, Loss: 0.2057219409942627\n",
            "Epoch 25, Batch 300, Loss: 0.20970994368195534\n",
            "Epoch 25, Batch 400, Loss: 0.22086599379777908\n",
            "Finished Training\n",
            "Accuracy on test set: 0.8815%\n",
            "\n",
            "Configuration: {'hidden_sizes': [256, 128, 64, 32, 16], 'num_layers': 5, 'lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
            "Epoch 1, Batch 100, Loss: 2.3126458430290224\n",
            "Epoch 1, Batch 200, Loss: 2.312876863479614\n",
            "Epoch 1, Batch 300, Loss: 2.3126877975463866\n",
            "Epoch 1, Batch 400, Loss: 2.3112857723236084\n",
            "Epoch 2, Batch 100, Loss: 2.3110036969184877\n",
            "Epoch 2, Batch 200, Loss: 2.3101431941986084\n",
            "Epoch 2, Batch 300, Loss: 2.311999247074127\n",
            "Epoch 2, Batch 400, Loss: 2.311077573299408\n",
            "Epoch 3, Batch 100, Loss: 2.3111588740348816\n",
            "Epoch 3, Batch 200, Loss: 2.3090776562690736\n",
            "Epoch 3, Batch 300, Loss: 2.3106309080123903\n",
            "Epoch 3, Batch 400, Loss: 2.3084289693832396\n",
            "Epoch 4, Batch 100, Loss: 2.308640854358673\n",
            "Epoch 4, Batch 200, Loss: 2.308828613758087\n",
            "Epoch 4, Batch 300, Loss: 2.308056664466858\n",
            "Epoch 4, Batch 400, Loss: 2.3083087396621704\n",
            "Epoch 5, Batch 100, Loss: 2.3079640221595765\n",
            "Epoch 5, Batch 200, Loss: 2.30602591753006\n",
            "Epoch 5, Batch 300, Loss: 2.3072564125061037\n",
            "Epoch 5, Batch 400, Loss: 2.305779366493225\n",
            "Epoch 6, Batch 100, Loss: 2.3054754304885865\n",
            "Epoch 6, Batch 200, Loss: 2.3065798473358154\n",
            "Epoch 6, Batch 300, Loss: 2.3054238677024843\n",
            "Epoch 6, Batch 400, Loss: 2.3063155221939087\n",
            "Epoch 7, Batch 100, Loss: 2.304858531951904\n",
            "Epoch 7, Batch 200, Loss: 2.3044842314720153\n",
            "Epoch 7, Batch 300, Loss: 2.304494421482086\n",
            "Epoch 7, Batch 400, Loss: 2.3051236867904663\n",
            "Epoch 8, Batch 100, Loss: 2.303805401325226\n",
            "Epoch 8, Batch 200, Loss: 2.3032486820220948\n",
            "Epoch 8, Batch 300, Loss: 2.3020741367340087\n",
            "Epoch 8, Batch 400, Loss: 2.303704044818878\n",
            "Epoch 9, Batch 100, Loss: 2.3031170630455016\n",
            "Epoch 9, Batch 200, Loss: 2.3029584217071535\n",
            "Epoch 9, Batch 300, Loss: 2.3025669932365416\n",
            "Epoch 9, Batch 400, Loss: 2.3011529088020324\n",
            "Epoch 10, Batch 100, Loss: 2.3016847252845762\n",
            "Epoch 10, Batch 200, Loss: 2.3015624117851257\n",
            "Epoch 10, Batch 300, Loss: 2.3009413194656374\n",
            "Epoch 10, Batch 400, Loss: 2.2997970032691955\n",
            "Epoch 11, Batch 100, Loss: 2.301383922100067\n",
            "Epoch 11, Batch 200, Loss: 2.3001335000991823\n",
            "Epoch 11, Batch 300, Loss: 2.2993642497062683\n",
            "Epoch 11, Batch 400, Loss: 2.298822543621063\n",
            "Epoch 12, Batch 100, Loss: 2.2977763557434083\n",
            "Epoch 12, Batch 200, Loss: 2.298865084648132\n",
            "Epoch 12, Batch 300, Loss: 2.2980051469802856\n",
            "Epoch 12, Batch 400, Loss: 2.299651939868927\n",
            "Epoch 13, Batch 100, Loss: 2.2980288434028626\n",
            "Epoch 13, Batch 200, Loss: 2.297122323513031\n",
            "Epoch 13, Batch 300, Loss: 2.29670090675354\n",
            "Epoch 13, Batch 400, Loss: 2.2969158625602724\n",
            "Epoch 14, Batch 100, Loss: 2.29571368932724\n",
            "Epoch 14, Batch 200, Loss: 2.2967526578903197\n",
            "Epoch 14, Batch 300, Loss: 2.295190467834473\n",
            "Epoch 14, Batch 400, Loss: 2.294619288444519\n",
            "Epoch 15, Batch 100, Loss: 2.2935363388061525\n",
            "Epoch 15, Batch 200, Loss: 2.29455406665802\n",
            "Epoch 15, Batch 300, Loss: 2.294620921611786\n",
            "Epoch 15, Batch 400, Loss: 2.2923417949676512\n",
            "Epoch 16, Batch 100, Loss: 2.2925981855392457\n",
            "Epoch 16, Batch 200, Loss: 2.292377097606659\n",
            "Epoch 16, Batch 300, Loss: 2.2919726753234864\n",
            "Epoch 16, Batch 400, Loss: 2.2924008846282957\n",
            "Epoch 17, Batch 100, Loss: 2.2916234874725343\n",
            "Epoch 17, Batch 200, Loss: 2.289923129081726\n",
            "Epoch 17, Batch 300, Loss: 2.2899385595321657\n",
            "Epoch 17, Batch 400, Loss: 2.290177960395813\n",
            "Epoch 18, Batch 100, Loss: 2.288154110908508\n",
            "Epoch 18, Batch 200, Loss: 2.28858767747879\n",
            "Epoch 18, Batch 300, Loss: 2.28895250082016\n",
            "Epoch 18, Batch 400, Loss: 2.2873425698280334\n",
            "Epoch 19, Batch 100, Loss: 2.286252474784851\n",
            "Epoch 19, Batch 200, Loss: 2.2867079305648805\n",
            "Epoch 19, Batch 300, Loss: 2.2858978629112245\n",
            "Epoch 19, Batch 400, Loss: 2.285256848335266\n",
            "Epoch 20, Batch 100, Loss: 2.283848850727081\n",
            "Epoch 20, Batch 200, Loss: 2.2830444884300234\n",
            "Epoch 20, Batch 300, Loss: 2.282983362674713\n",
            "Epoch 20, Batch 400, Loss: 2.2838202691078187\n",
            "Epoch 21, Batch 100, Loss: 2.2821732640266417\n",
            "Epoch 21, Batch 200, Loss: 2.2813850331306456\n",
            "Epoch 21, Batch 300, Loss: 2.2809606480598448\n",
            "Epoch 21, Batch 400, Loss: 2.2781348848342895\n",
            "Epoch 22, Batch 100, Loss: 2.2783275938034055\n",
            "Epoch 22, Batch 200, Loss: 2.2778939175605775\n",
            "Epoch 22, Batch 300, Loss: 2.2769728088378907\n",
            "Epoch 22, Batch 400, Loss: 2.275002615451813\n",
            "Epoch 23, Batch 100, Loss: 2.2748500299453736\n",
            "Epoch 23, Batch 200, Loss: 2.273617377281189\n",
            "Epoch 23, Batch 300, Loss: 2.272588322162628\n",
            "Epoch 23, Batch 400, Loss: 2.2716973185539246\n",
            "Epoch 24, Batch 100, Loss: 2.2703949117660525\n",
            "Epoch 24, Batch 200, Loss: 2.2682406520843506\n",
            "Epoch 24, Batch 300, Loss: 2.267251570224762\n",
            "Epoch 24, Batch 400, Loss: 2.266987752914429\n",
            "Epoch 25, Batch 100, Loss: 2.265328187942505\n",
            "Epoch 25, Batch 200, Loss: 2.263575713634491\n",
            "Epoch 25, Batch 300, Loss: 2.2621651816368105\n",
            "Epoch 25, Batch 400, Loss: 2.2591841316223142\n",
            "Finished Training\n",
            "Accuracy on test set: 0.2122%\n",
            "\n",
            "Configuration: {'hidden_sizes': [256, 128, 64, 32, 16], 'num_layers': 5, 'lr': 0.002, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
            "Epoch 1, Batch 100, Loss: 1.090692532658577\n",
            "Epoch 1, Batch 200, Loss: 0.6137946072220802\n",
            "Epoch 1, Batch 300, Loss: 0.5429005753993988\n",
            "Epoch 1, Batch 400, Loss: 0.5016538551449776\n",
            "Epoch 2, Batch 100, Loss: 0.4386858049035072\n",
            "Epoch 2, Batch 200, Loss: 0.43702789932489394\n",
            "Epoch 2, Batch 300, Loss: 0.42874293237924577\n",
            "Epoch 2, Batch 400, Loss: 0.40710512578487396\n",
            "Epoch 3, Batch 100, Loss: 0.3776438561081886\n",
            "Epoch 3, Batch 200, Loss: 0.35995994582772256\n",
            "Epoch 3, Batch 300, Loss: 0.3626257196068764\n",
            "Epoch 3, Batch 400, Loss: 0.3587684382498264\n",
            "Epoch 4, Batch 100, Loss: 0.33803698271512983\n",
            "Epoch 4, Batch 200, Loss: 0.3395091299712658\n",
            "Epoch 4, Batch 300, Loss: 0.333633576631546\n",
            "Epoch 4, Batch 400, Loss: 0.3402233558893204\n",
            "Epoch 5, Batch 100, Loss: 0.32081747889518736\n",
            "Epoch 5, Batch 200, Loss: 0.3189161628484726\n",
            "Epoch 5, Batch 300, Loss: 0.31174046158790586\n",
            "Epoch 5, Batch 400, Loss: 0.3250055772066116\n",
            "Epoch 6, Batch 100, Loss: 0.2908549113571644\n",
            "Epoch 6, Batch 200, Loss: 0.29516455352306364\n",
            "Epoch 6, Batch 300, Loss: 0.30496130108833314\n",
            "Epoch 6, Batch 400, Loss: 0.301533140540123\n",
            "Epoch 7, Batch 100, Loss: 0.28955136612057686\n",
            "Epoch 7, Batch 200, Loss: 0.2834912833571434\n",
            "Epoch 7, Batch 300, Loss: 0.2911785852909088\n",
            "Epoch 7, Batch 400, Loss: 0.295794812142849\n",
            "Epoch 8, Batch 100, Loss: 0.274104270786047\n",
            "Epoch 8, Batch 200, Loss: 0.2762191192805767\n",
            "Epoch 8, Batch 300, Loss: 0.27313237383961675\n",
            "Epoch 8, Batch 400, Loss: 0.272685691639781\n",
            "Epoch 9, Batch 100, Loss: 0.26034042716026307\n",
            "Epoch 9, Batch 200, Loss: 0.26126039505004883\n",
            "Epoch 9, Batch 300, Loss: 0.26060948967933656\n",
            "Epoch 9, Batch 400, Loss: 0.27228420197963715\n",
            "Epoch 10, Batch 100, Loss: 0.25376667514443396\n",
            "Epoch 10, Batch 200, Loss: 0.2581610681861639\n",
            "Epoch 10, Batch 300, Loss: 0.24916059993207454\n",
            "Epoch 10, Batch 400, Loss: 0.26584629371762275\n",
            "Epoch 11, Batch 100, Loss: 0.24675571128726007\n",
            "Epoch 11, Batch 200, Loss: 0.23772500574588776\n",
            "Epoch 11, Batch 300, Loss: 0.24954188615083694\n",
            "Epoch 11, Batch 400, Loss: 0.26742549739778043\n",
            "Epoch 12, Batch 100, Loss: 0.22377848722040652\n",
            "Epoch 12, Batch 200, Loss: 0.2380821132659912\n",
            "Epoch 12, Batch 300, Loss: 0.236091548204422\n",
            "Epoch 12, Batch 400, Loss: 0.24728044658899306\n",
            "Epoch 13, Batch 100, Loss: 0.21880087964236736\n",
            "Epoch 13, Batch 200, Loss: 0.23065645270049573\n",
            "Epoch 13, Batch 300, Loss: 0.23648150727152825\n",
            "Epoch 13, Batch 400, Loss: 0.2346985110640526\n",
            "Epoch 14, Batch 100, Loss: 0.21585667178034781\n",
            "Epoch 14, Batch 200, Loss: 0.23336552642285824\n",
            "Epoch 14, Batch 300, Loss: 0.22368859626352788\n",
            "Epoch 14, Batch 400, Loss: 0.2337471640855074\n",
            "Epoch 15, Batch 100, Loss: 0.2114973760396242\n",
            "Epoch 15, Batch 200, Loss: 0.21251974314451216\n",
            "Epoch 15, Batch 300, Loss: 0.21521285496652126\n",
            "Epoch 15, Batch 400, Loss: 0.22373077988624573\n",
            "Epoch 16, Batch 100, Loss: 0.20005235619843006\n",
            "Epoch 16, Batch 200, Loss: 0.2094790057092905\n",
            "Epoch 16, Batch 300, Loss: 0.21944795399904252\n",
            "Epoch 16, Batch 400, Loss: 0.20170546174049378\n",
            "Epoch 17, Batch 100, Loss: 0.22039465092122554\n",
            "Epoch 17, Batch 200, Loss: 0.20922826208174228\n",
            "Epoch 17, Batch 300, Loss: 0.20546641863882542\n",
            "Epoch 17, Batch 400, Loss: 0.20546036303043366\n",
            "Epoch 18, Batch 100, Loss: 0.18898609921336174\n",
            "Epoch 18, Batch 200, Loss: 0.19147968992590905\n",
            "Epoch 18, Batch 300, Loss: 0.20250615827739238\n",
            "Epoch 18, Batch 400, Loss: 0.1982545956969261\n",
            "Epoch 19, Batch 100, Loss: 0.19311867706477642\n",
            "Epoch 19, Batch 200, Loss: 0.19325342275202273\n",
            "Epoch 19, Batch 300, Loss: 0.1928352925926447\n",
            "Epoch 19, Batch 400, Loss: 0.198779773786664\n",
            "Epoch 20, Batch 100, Loss: 0.1764830444008112\n",
            "Epoch 20, Batch 200, Loss: 0.18848406106233598\n",
            "Epoch 20, Batch 300, Loss: 0.17398927479982376\n",
            "Epoch 20, Batch 400, Loss: 0.19111301250755786\n",
            "Epoch 21, Batch 100, Loss: 0.1654809721186757\n",
            "Epoch 21, Batch 200, Loss: 0.18837153911590576\n",
            "Epoch 21, Batch 300, Loss: 0.1836794412881136\n",
            "Epoch 21, Batch 400, Loss: 0.18652925677597523\n",
            "Epoch 22, Batch 100, Loss: 0.17043536104261875\n",
            "Epoch 22, Batch 200, Loss: 0.1689127243310213\n",
            "Epoch 22, Batch 300, Loss: 0.17667702123522758\n",
            "Epoch 22, Batch 400, Loss: 0.18240470372140408\n",
            "Epoch 23, Batch 100, Loss: 0.15809326738119125\n",
            "Epoch 23, Batch 200, Loss: 0.15789419531822205\n",
            "Epoch 23, Batch 300, Loss: 0.17704321593046188\n",
            "Epoch 23, Batch 400, Loss: 0.18098031476140022\n",
            "Epoch 24, Batch 100, Loss: 0.16577802792191507\n",
            "Epoch 24, Batch 200, Loss: 0.1689863971620798\n",
            "Epoch 24, Batch 300, Loss: 0.15465741068124772\n",
            "Epoch 24, Batch 400, Loss: 0.16243196085095404\n",
            "Epoch 25, Batch 100, Loss: 0.14774966940283776\n",
            "Epoch 25, Batch 200, Loss: 0.15279884699732066\n",
            "Epoch 25, Batch 300, Loss: 0.1525097056478262\n",
            "Epoch 25, Batch 400, Loss: 0.1715933959186077\n",
            "Finished Training\n",
            "Accuracy on test set: 0.8854%\n",
            "\n",
            "Configuration: {'hidden_sizes': [64, 64, 64], 'num_layers': 3, 'lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
            "Epoch 1, Batch 100, Loss: 2.3047761130332947\n",
            "Epoch 1, Batch 200, Loss: 2.301880235671997\n",
            "Epoch 1, Batch 300, Loss: 2.29953293800354\n",
            "Epoch 1, Batch 400, Loss: 2.294192168712616\n",
            "Epoch 2, Batch 100, Loss: 2.2865214276313783\n",
            "Epoch 2, Batch 200, Loss: 2.2793438148498537\n",
            "Epoch 2, Batch 300, Loss: 2.2753363108634947\n",
            "Epoch 2, Batch 400, Loss: 2.267886230945587\n",
            "Epoch 3, Batch 100, Loss: 2.257269277572632\n",
            "Epoch 3, Batch 200, Loss: 2.2479237842559816\n",
            "Epoch 3, Batch 300, Loss: 2.238950536251068\n",
            "Epoch 3, Batch 400, Loss: 2.229322350025177\n",
            "Epoch 4, Batch 100, Loss: 2.2106937050819395\n",
            "Epoch 4, Batch 200, Loss: 2.198648228645325\n",
            "Epoch 4, Batch 300, Loss: 2.1837998175621034\n",
            "Epoch 4, Batch 400, Loss: 2.164594633579254\n",
            "Epoch 5, Batch 100, Loss: 2.133440992832184\n",
            "Epoch 5, Batch 200, Loss: 2.1120814800262453\n",
            "Epoch 5, Batch 300, Loss: 2.085607522726059\n",
            "Epoch 5, Batch 400, Loss: 2.052343474626541\n",
            "Epoch 6, Batch 100, Loss: 1.997860209941864\n",
            "Epoch 6, Batch 200, Loss: 1.9630334210395812\n",
            "Epoch 6, Batch 300, Loss: 1.9198383569717408\n",
            "Epoch 6, Batch 400, Loss: 1.8699353921413422\n",
            "Epoch 7, Batch 100, Loss: 1.79434792637825\n",
            "Epoch 7, Batch 200, Loss: 1.7454899883270263\n",
            "Epoch 7, Batch 300, Loss: 1.6938034164905549\n",
            "Epoch 7, Batch 400, Loss: 1.633130441904068\n",
            "Epoch 8, Batch 100, Loss: 1.5548061442375183\n",
            "Epoch 8, Batch 200, Loss: 1.4993836534023286\n",
            "Epoch 8, Batch 300, Loss: 1.4553448128700257\n",
            "Epoch 8, Batch 400, Loss: 1.4021957695484162\n",
            "Epoch 9, Batch 100, Loss: 1.321472146511078\n",
            "Epoch 9, Batch 200, Loss: 1.2776842272281648\n",
            "Epoch 9, Batch 300, Loss: 1.2409539484977723\n",
            "Epoch 9, Batch 400, Loss: 1.1947546899318695\n",
            "Epoch 10, Batch 100, Loss: 1.1387534749507904\n",
            "Epoch 10, Batch 200, Loss: 1.106843501329422\n",
            "Epoch 10, Batch 300, Loss: 1.0811301171779633\n",
            "Epoch 10, Batch 400, Loss: 1.0609413081407546\n",
            "Epoch 11, Batch 100, Loss: 1.0193026787042618\n",
            "Epoch 11, Batch 200, Loss: 0.9950692105293274\n",
            "Epoch 11, Batch 300, Loss: 0.9700541335344315\n",
            "Epoch 11, Batch 400, Loss: 0.9568223142623902\n",
            "Epoch 12, Batch 100, Loss: 0.9291992151737213\n",
            "Epoch 12, Batch 200, Loss: 0.917958527803421\n",
            "Epoch 12, Batch 300, Loss: 0.8992990845441818\n",
            "Epoch 12, Batch 400, Loss: 0.8864675480127334\n",
            "Epoch 13, Batch 100, Loss: 0.8687913829088211\n",
            "Epoch 13, Batch 200, Loss: 0.8563103115558625\n",
            "Epoch 13, Batch 300, Loss: 0.8499237847328186\n",
            "Epoch 13, Batch 400, Loss: 0.8286400538682938\n",
            "Epoch 14, Batch 100, Loss: 0.823006619811058\n",
            "Epoch 14, Batch 200, Loss: 0.8148741573095322\n",
            "Epoch 14, Batch 300, Loss: 0.8029835200309754\n",
            "Epoch 14, Batch 400, Loss: 0.8028878694772721\n",
            "Epoch 15, Batch 100, Loss: 0.8070418763160706\n",
            "Epoch 15, Batch 200, Loss: 0.7803115153312683\n",
            "Epoch 15, Batch 300, Loss: 0.7612196063995361\n",
            "Epoch 15, Batch 400, Loss: 0.7675884389877319\n",
            "Epoch 16, Batch 100, Loss: 0.7644678419828415\n",
            "Epoch 16, Batch 200, Loss: 0.7472585713863373\n",
            "Epoch 16, Batch 300, Loss: 0.7577773672342301\n",
            "Epoch 16, Batch 400, Loss: 0.7476446151733398\n",
            "Epoch 17, Batch 100, Loss: 0.7270783013105393\n",
            "Epoch 17, Batch 200, Loss: 0.7295379430055619\n",
            "Epoch 17, Batch 300, Loss: 0.730183835029602\n",
            "Epoch 17, Batch 400, Loss: 0.7240853691101075\n",
            "Epoch 18, Batch 100, Loss: 0.7136583292484283\n",
            "Epoch 18, Batch 200, Loss: 0.71633640229702\n",
            "Epoch 18, Batch 300, Loss: 0.7073448646068573\n",
            "Epoch 18, Batch 400, Loss: 0.7080107313394547\n",
            "Epoch 19, Batch 100, Loss: 0.7023232024908066\n",
            "Epoch 19, Batch 200, Loss: 0.690335801243782\n",
            "Epoch 19, Batch 300, Loss: 0.6937583345174789\n",
            "Epoch 19, Batch 400, Loss: 0.6909269386529923\n",
            "Epoch 20, Batch 100, Loss: 0.6848845970630646\n",
            "Epoch 20, Batch 200, Loss: 0.6751715672016144\n",
            "Epoch 20, Batch 300, Loss: 0.6894743806123733\n",
            "Epoch 20, Batch 400, Loss: 0.6773022642731666\n",
            "Epoch 21, Batch 100, Loss: 0.6703093856573105\n",
            "Epoch 21, Batch 200, Loss: 0.6625558799505233\n",
            "Epoch 21, Batch 300, Loss: 0.6664966747164727\n",
            "Epoch 21, Batch 400, Loss: 0.6592117238044739\n",
            "Epoch 22, Batch 100, Loss: 0.6600661295652389\n",
            "Epoch 22, Batch 200, Loss: 0.6559454464912414\n",
            "Epoch 22, Batch 300, Loss: 0.6560679244995117\n",
            "Epoch 22, Batch 400, Loss: 0.641915248632431\n",
            "Epoch 23, Batch 100, Loss: 0.6403363394737244\n",
            "Epoch 23, Batch 200, Loss: 0.6485505628585816\n",
            "Epoch 23, Batch 300, Loss: 0.6518730288743972\n",
            "Epoch 23, Batch 400, Loss: 0.6410369166731834\n",
            "Epoch 24, Batch 100, Loss: 0.6265089109539985\n",
            "Epoch 24, Batch 200, Loss: 0.6325756114721298\n",
            "Epoch 24, Batch 300, Loss: 0.6380698448419571\n",
            "Epoch 24, Batch 400, Loss: 0.6318947044014931\n",
            "Epoch 25, Batch 100, Loss: 0.6266707196831703\n",
            "Epoch 25, Batch 200, Loss: 0.6225038641691207\n",
            "Epoch 25, Batch 300, Loss: 0.6307592448592186\n",
            "Epoch 25, Batch 400, Loss: 0.6212339124083519\n",
            "Finished Training\n",
            "Accuracy on test set: 0.7655%\n",
            "\n",
            "Configuration: {'hidden_sizes': [64, 64, 64], 'num_layers': 3, 'lr': 0.002, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
            "Epoch 1, Batch 100, Loss: 0.8601370292901993\n",
            "Epoch 1, Batch 200, Loss: 0.5375396075844765\n",
            "Epoch 1, Batch 300, Loss: 0.46717301905155184\n",
            "Epoch 1, Batch 400, Loss: 0.4516404861211777\n",
            "Epoch 2, Batch 100, Loss: 0.4220647919178009\n",
            "Epoch 2, Batch 200, Loss: 0.3934920446574688\n",
            "Epoch 2, Batch 300, Loss: 0.38286615163087845\n",
            "Epoch 2, Batch 400, Loss: 0.38001867637038234\n",
            "Epoch 3, Batch 100, Loss: 0.36026360377669336\n",
            "Epoch 3, Batch 200, Loss: 0.3608945968747139\n",
            "Epoch 3, Batch 300, Loss: 0.35608310416340827\n",
            "Epoch 3, Batch 400, Loss: 0.3472119942307472\n",
            "Epoch 4, Batch 100, Loss: 0.3339507542550564\n",
            "Epoch 4, Batch 200, Loss: 0.33906746819615363\n",
            "Epoch 4, Batch 300, Loss: 0.3245075012743473\n",
            "Epoch 4, Batch 400, Loss: 0.32672880589962006\n",
            "Epoch 5, Batch 100, Loss: 0.30778550952672956\n",
            "Epoch 5, Batch 200, Loss: 0.32684449702501295\n",
            "Epoch 5, Batch 300, Loss: 0.3168242925405502\n",
            "Epoch 5, Batch 400, Loss: 0.32445672392845154\n",
            "Epoch 6, Batch 100, Loss: 0.2977383691072464\n",
            "Epoch 6, Batch 200, Loss: 0.30768886059522627\n",
            "Epoch 6, Batch 300, Loss: 0.3034618900716305\n",
            "Epoch 6, Batch 400, Loss: 0.30665507301688194\n",
            "Epoch 7, Batch 100, Loss: 0.28717754244804383\n",
            "Epoch 7, Batch 200, Loss: 0.28648296996951106\n",
            "Epoch 7, Batch 300, Loss: 0.2983360128104687\n",
            "Epoch 7, Batch 400, Loss: 0.29914315983653067\n",
            "Epoch 8, Batch 100, Loss: 0.28082317411899566\n",
            "Epoch 8, Batch 200, Loss: 0.29213354997336866\n",
            "Epoch 8, Batch 300, Loss: 0.2726381753385067\n",
            "Epoch 8, Batch 400, Loss: 0.2966816286742687\n",
            "Epoch 9, Batch 100, Loss: 0.2736960302293301\n",
            "Epoch 9, Batch 200, Loss: 0.27521617487072947\n",
            "Epoch 9, Batch 300, Loss: 0.25747293114662173\n",
            "Epoch 9, Batch 400, Loss: 0.27269781157374384\n",
            "Epoch 10, Batch 100, Loss: 0.25524336531758307\n",
            "Epoch 10, Batch 200, Loss: 0.26221672624349596\n",
            "Epoch 10, Batch 300, Loss: 0.26730156764388086\n",
            "Epoch 10, Batch 400, Loss: 0.25992661774158476\n",
            "Epoch 11, Batch 100, Loss: 0.23917021349072456\n",
            "Epoch 11, Batch 200, Loss: 0.2561994445323944\n",
            "Epoch 11, Batch 300, Loss: 0.2471585117280483\n",
            "Epoch 11, Batch 400, Loss: 0.25847358524799346\n",
            "Epoch 12, Batch 100, Loss: 0.23887470349669457\n",
            "Epoch 12, Batch 200, Loss: 0.2526511088013649\n",
            "Epoch 12, Batch 300, Loss: 0.23951790928840638\n",
            "Epoch 12, Batch 400, Loss: 0.2565608212351799\n",
            "Epoch 13, Batch 100, Loss: 0.2272504021972418\n",
            "Epoch 13, Batch 200, Loss: 0.2321264451742172\n",
            "Epoch 13, Batch 300, Loss: 0.25251989126205443\n",
            "Epoch 13, Batch 400, Loss: 0.2403362974524498\n",
            "Epoch 14, Batch 100, Loss: 0.23771355859935284\n",
            "Epoch 14, Batch 200, Loss: 0.2315713744610548\n",
            "Epoch 14, Batch 300, Loss: 0.2313915853202343\n",
            "Epoch 14, Batch 400, Loss: 0.23902689874172212\n",
            "Epoch 15, Batch 100, Loss: 0.2137925348430872\n",
            "Epoch 15, Batch 200, Loss: 0.23149679034948348\n",
            "Epoch 15, Batch 300, Loss: 0.23674723982810975\n",
            "Epoch 15, Batch 400, Loss: 0.2290185246616602\n",
            "Epoch 16, Batch 100, Loss: 0.22590681210160254\n",
            "Epoch 16, Batch 200, Loss: 0.22163173973560332\n",
            "Epoch 16, Batch 300, Loss: 0.21635442346334458\n",
            "Epoch 16, Batch 400, Loss: 0.23484140820801258\n",
            "Epoch 17, Batch 100, Loss: 0.2031167211383581\n",
            "Epoch 17, Batch 200, Loss: 0.21612385772168635\n",
            "Epoch 17, Batch 300, Loss: 0.21544298112392427\n",
            "Epoch 17, Batch 400, Loss: 0.22341794349253177\n",
            "Epoch 18, Batch 100, Loss: 0.19943720057606698\n",
            "Epoch 18, Batch 200, Loss: 0.20771791011095048\n",
            "Epoch 18, Batch 300, Loss: 0.21816459000110627\n",
            "Epoch 18, Batch 400, Loss: 0.2094977790117264\n",
            "Epoch 19, Batch 100, Loss: 0.2085171902179718\n",
            "Epoch 19, Batch 200, Loss: 0.20150220222771167\n",
            "Epoch 19, Batch 300, Loss: 0.22082928694784643\n",
            "Epoch 19, Batch 400, Loss: 0.21142484717071056\n",
            "Epoch 20, Batch 100, Loss: 0.18567133709788322\n",
            "Epoch 20, Batch 200, Loss: 0.20138608880341052\n",
            "Epoch 20, Batch 300, Loss: 0.20300998501479625\n",
            "Epoch 20, Batch 400, Loss: 0.2189556860178709\n",
            "Epoch 21, Batch 100, Loss: 0.18535384111106395\n",
            "Epoch 21, Batch 200, Loss: 0.1970262509584427\n",
            "Epoch 21, Batch 300, Loss: 0.20806515708565712\n",
            "Epoch 21, Batch 400, Loss: 0.20615908466279506\n",
            "Epoch 22, Batch 100, Loss: 0.18482157453894615\n",
            "Epoch 22, Batch 200, Loss: 0.18900234937667848\n",
            "Epoch 22, Batch 300, Loss: 0.19436347916722296\n",
            "Epoch 22, Batch 400, Loss: 0.2055592180043459\n",
            "Epoch 23, Batch 100, Loss: 0.17915250830352306\n",
            "Epoch 23, Batch 200, Loss: 0.17569046720862388\n",
            "Epoch 23, Batch 300, Loss: 0.19722995638847352\n",
            "Epoch 23, Batch 400, Loss: 0.20134734719991684\n",
            "Epoch 24, Batch 100, Loss: 0.18260560013353824\n",
            "Epoch 24, Batch 200, Loss: 0.17537024788558483\n",
            "Epoch 24, Batch 300, Loss: 0.2000179758667946\n",
            "Epoch 24, Batch 400, Loss: 0.17937346562743187\n",
            "Epoch 25, Batch 100, Loss: 0.17884496420621873\n",
            "Epoch 25, Batch 200, Loss: 0.1741266470402479\n",
            "Epoch 25, Batch 300, Loss: 0.17806215785443782\n",
            "Epoch 25, Batch 400, Loss: 0.1891961032897234\n",
            "Finished Training\n",
            "Accuracy on test set: 0.8853%\n",
            "\n",
            "Configuration: {'hidden_sizes': [64, 64, 64], 'num_layers': 3, 'lr': 0.003, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
            "Epoch 1, Batch 100, Loss: 0.9835871943831443\n",
            "Epoch 1, Batch 200, Loss: 0.5772872787714004\n",
            "Epoch 1, Batch 300, Loss: 0.5041986748576164\n",
            "Epoch 1, Batch 400, Loss: 0.47560941785573957\n",
            "Epoch 2, Batch 100, Loss: 0.43221242547035216\n",
            "Epoch 2, Batch 200, Loss: 0.4309623321890831\n",
            "Epoch 2, Batch 300, Loss: 0.41197296887636187\n",
            "Epoch 2, Batch 400, Loss: 0.4112891574203968\n",
            "Epoch 3, Batch 100, Loss: 0.38429315879940984\n",
            "Epoch 3, Batch 200, Loss: 0.36817333817481995\n",
            "Epoch 3, Batch 300, Loss: 0.3827711808681488\n",
            "Epoch 3, Batch 400, Loss: 0.3769253109395504\n",
            "Epoch 4, Batch 100, Loss: 0.3573181688785553\n",
            "Epoch 4, Batch 200, Loss: 0.35608273386955264\n",
            "Epoch 4, Batch 300, Loss: 0.3474145732820034\n",
            "Epoch 4, Batch 400, Loss: 0.35539364963769915\n",
            "Epoch 5, Batch 100, Loss: 0.3271632133424282\n",
            "Epoch 5, Batch 200, Loss: 0.3358056282997131\n",
            "Epoch 5, Batch 300, Loss: 0.3370425294339657\n",
            "Epoch 5, Batch 400, Loss: 0.3390795324742794\n",
            "Epoch 6, Batch 100, Loss: 0.3154296952486038\n",
            "Epoch 6, Batch 200, Loss: 0.32203468173742295\n",
            "Epoch 6, Batch 300, Loss: 0.31767951548099516\n",
            "Epoch 6, Batch 400, Loss: 0.3176588749885559\n",
            "Epoch 7, Batch 100, Loss: 0.3036430937051773\n",
            "Epoch 7, Batch 200, Loss: 0.30585313349962234\n",
            "Epoch 7, Batch 300, Loss: 0.31189108282327654\n",
            "Epoch 7, Batch 400, Loss: 0.3033776625990868\n",
            "Epoch 8, Batch 100, Loss: 0.2950561444461346\n",
            "Epoch 8, Batch 200, Loss: 0.3028160911798477\n",
            "Epoch 8, Batch 300, Loss: 0.29845586225390436\n",
            "Epoch 8, Batch 400, Loss: 0.2953260585665703\n",
            "Epoch 9, Batch 100, Loss: 0.28944323435425756\n",
            "Epoch 9, Batch 200, Loss: 0.28708164900541305\n",
            "Epoch 9, Batch 300, Loss: 0.29986147850751876\n",
            "Epoch 9, Batch 400, Loss: 0.2860625582933426\n",
            "Epoch 10, Batch 100, Loss: 0.2821280957758427\n",
            "Epoch 10, Batch 200, Loss: 0.2791779075562954\n",
            "Epoch 10, Batch 300, Loss: 0.2954591231048107\n",
            "Epoch 10, Batch 400, Loss: 0.2892274680733681\n",
            "Epoch 11, Batch 100, Loss: 0.2755004808306694\n",
            "Epoch 11, Batch 200, Loss: 0.2857144983112812\n",
            "Epoch 11, Batch 300, Loss: 0.27205031625926496\n",
            "Epoch 11, Batch 400, Loss: 0.29326529026031495\n",
            "Epoch 12, Batch 100, Loss: 0.2587284564971924\n",
            "Epoch 12, Batch 200, Loss: 0.2682848934829235\n",
            "Epoch 12, Batch 300, Loss: 0.27514844238758085\n",
            "Epoch 12, Batch 400, Loss: 0.27613412350416183\n",
            "Epoch 13, Batch 100, Loss: 0.2634016200900078\n",
            "Epoch 13, Batch 200, Loss: 0.2634661784768105\n",
            "Epoch 13, Batch 300, Loss: 0.273508839905262\n",
            "Epoch 13, Batch 400, Loss: 0.27248852580785754\n",
            "Epoch 14, Batch 100, Loss: 0.25802160024642945\n",
            "Epoch 14, Batch 200, Loss: 0.2559886941313744\n",
            "Epoch 14, Batch 300, Loss: 0.2524284194409847\n",
            "Epoch 14, Batch 400, Loss: 0.27416265234351156\n",
            "Epoch 15, Batch 100, Loss: 0.25430700957775115\n",
            "Epoch 15, Batch 200, Loss: 0.2504771727323532\n",
            "Epoch 15, Batch 300, Loss: 0.2612183257192373\n",
            "Epoch 15, Batch 400, Loss: 0.26971883229911325\n",
            "Epoch 16, Batch 100, Loss: 0.25339414447546005\n",
            "Epoch 16, Batch 200, Loss: 0.2601383359730244\n",
            "Epoch 16, Batch 300, Loss: 0.25212290324270725\n",
            "Epoch 16, Batch 400, Loss: 0.2510236756503582\n",
            "Epoch 17, Batch 100, Loss: 0.2607536652684212\n",
            "Epoch 17, Batch 200, Loss: 0.2389759224653244\n",
            "Epoch 17, Batch 300, Loss: 0.2622737318277359\n",
            "Epoch 17, Batch 400, Loss: 0.25189697183668613\n",
            "Epoch 18, Batch 100, Loss: 0.28181504622101783\n",
            "Epoch 18, Batch 200, Loss: 0.23352876618504526\n",
            "Epoch 18, Batch 300, Loss: 0.2443978936970234\n",
            "Epoch 18, Batch 400, Loss: 0.23625380024313927\n",
            "Epoch 19, Batch 100, Loss: 0.23615886703133582\n",
            "Epoch 19, Batch 200, Loss: 0.2450038330256939\n",
            "Epoch 19, Batch 300, Loss: 0.24399372488260268\n",
            "Epoch 19, Batch 400, Loss: 0.24507927775382995\n",
            "Epoch 20, Batch 100, Loss: 0.23124852284789085\n",
            "Epoch 20, Batch 200, Loss: 0.22793915577232837\n",
            "Epoch 20, Batch 300, Loss: 0.24944556646049024\n",
            "Epoch 20, Batch 400, Loss: 0.24525574199855327\n",
            "Epoch 21, Batch 100, Loss: 0.23439843341708183\n",
            "Epoch 21, Batch 200, Loss: 0.23426864124834537\n",
            "Epoch 21, Batch 300, Loss: 0.2299289146065712\n",
            "Epoch 21, Batch 400, Loss: 0.24543723225593567\n",
            "Epoch 22, Batch 100, Loss: 0.22687678396701813\n",
            "Epoch 22, Batch 200, Loss: 0.22734517559409143\n",
            "Epoch 22, Batch 300, Loss: 0.24217265456914902\n",
            "Epoch 22, Batch 400, Loss: 0.2399240442365408\n",
            "Epoch 23, Batch 100, Loss: 0.24046762578189373\n",
            "Epoch 23, Batch 200, Loss: 0.22536108754575251\n",
            "Epoch 23, Batch 300, Loss: 0.23455210238695146\n",
            "Epoch 23, Batch 400, Loss: 0.23037792064249515\n",
            "Epoch 24, Batch 100, Loss: 0.21874114781618118\n",
            "Epoch 24, Batch 200, Loss: 0.22953016348183156\n",
            "Epoch 24, Batch 300, Loss: 0.2358878755569458\n",
            "Epoch 24, Batch 400, Loss: 0.22372353985905646\n",
            "Epoch 25, Batch 100, Loss: 0.22171037457883358\n",
            "Epoch 25, Batch 200, Loss: 0.21701761677861214\n",
            "Epoch 25, Batch 300, Loss: 0.2341761627048254\n",
            "Epoch 25, Batch 400, Loss: 0.23445786893367768\n",
            "Finished Training\n",
            "Accuracy on test set: 0.8722%\n",
            "\n",
            "Configuration: {'hidden_sizes': [128, 128, 128, 128], 'num_layers': 4, 'lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
            "Epoch 1, Batch 100, Loss: 2.3022302269935606\n",
            "Epoch 1, Batch 200, Loss: 2.3018613934516905\n",
            "Epoch 1, Batch 300, Loss: 2.300624523162842\n",
            "Epoch 1, Batch 400, Loss: 2.3003663635253906\n",
            "Epoch 2, Batch 100, Loss: 2.2990410208702086\n",
            "Epoch 2, Batch 200, Loss: 2.299200966358185\n",
            "Epoch 2, Batch 300, Loss: 2.298555519580841\n",
            "Epoch 2, Batch 400, Loss: 2.29742684841156\n",
            "Epoch 3, Batch 100, Loss: 2.296627118587494\n",
            "Epoch 3, Batch 200, Loss: 2.295928852558136\n",
            "Epoch 3, Batch 300, Loss: 2.2948037123680116\n",
            "Epoch 3, Batch 400, Loss: 2.294165782928467\n",
            "Epoch 4, Batch 100, Loss: 2.2925836205482484\n",
            "Epoch 4, Batch 200, Loss: 2.2914776468276976\n",
            "Epoch 4, Batch 300, Loss: 2.2907897663116454\n",
            "Epoch 4, Batch 400, Loss: 2.2901653695106505\n",
            "Epoch 5, Batch 100, Loss: 2.28793577671051\n",
            "Epoch 5, Batch 200, Loss: 2.287272608280182\n",
            "Epoch 5, Batch 300, Loss: 2.285647258758545\n",
            "Epoch 5, Batch 400, Loss: 2.284406740665436\n",
            "Epoch 6, Batch 100, Loss: 2.2818410348892213\n",
            "Epoch 6, Batch 200, Loss: 2.280924699306488\n",
            "Epoch 6, Batch 300, Loss: 2.279308512210846\n",
            "Epoch 6, Batch 400, Loss: 2.2775773215293884\n",
            "Epoch 7, Batch 100, Loss: 2.2745745038986205\n",
            "Epoch 7, Batch 200, Loss: 2.2733424162864684\n",
            "Epoch 7, Batch 300, Loss: 2.2711304593086243\n",
            "Epoch 7, Batch 400, Loss: 2.267838532924652\n",
            "Epoch 8, Batch 100, Loss: 2.264338493347168\n",
            "Epoch 8, Batch 200, Loss: 2.2619142270088197\n",
            "Epoch 8, Batch 300, Loss: 2.2585155749320984\n",
            "Epoch 8, Batch 400, Loss: 2.256171667575836\n",
            "Epoch 9, Batch 100, Loss: 2.2494408798217775\n",
            "Epoch 9, Batch 200, Loss: 2.245600106716156\n",
            "Epoch 9, Batch 300, Loss: 2.240698552131653\n",
            "Epoch 9, Batch 400, Loss: 2.2351645159721376\n",
            "Epoch 10, Batch 100, Loss: 2.2256575775146485\n",
            "Epoch 10, Batch 200, Loss: 2.218793177604675\n",
            "Epoch 10, Batch 300, Loss: 2.210209231376648\n",
            "Epoch 10, Batch 400, Loss: 2.2021949696540832\n",
            "Epoch 11, Batch 100, Loss: 2.1852986240386962\n",
            "Epoch 11, Batch 200, Loss: 2.1712865829467773\n",
            "Epoch 11, Batch 300, Loss: 2.15528555393219\n",
            "Epoch 11, Batch 400, Loss: 2.1441461396217347\n",
            "Epoch 12, Batch 100, Loss: 2.114407913684845\n",
            "Epoch 12, Batch 200, Loss: 2.09408442735672\n",
            "Epoch 12, Batch 300, Loss: 2.0758651173114777\n",
            "Epoch 12, Batch 400, Loss: 2.048510590791702\n",
            "Epoch 13, Batch 100, Loss: 2.008423662185669\n",
            "Epoch 13, Batch 200, Loss: 1.9852921080589294\n",
            "Epoch 13, Batch 300, Loss: 1.9658156609535218\n",
            "Epoch 13, Batch 400, Loss: 1.9376575756072998\n",
            "Epoch 14, Batch 100, Loss: 1.8872225093841553\n",
            "Epoch 14, Batch 200, Loss: 1.8566110467910766\n",
            "Epoch 14, Batch 300, Loss: 1.8169344198703765\n",
            "Epoch 14, Batch 400, Loss: 1.7921954047679902\n",
            "Epoch 15, Batch 100, Loss: 1.7405871427059174\n",
            "Epoch 15, Batch 200, Loss: 1.7172046411037445\n",
            "Epoch 15, Batch 300, Loss: 1.6841924977302551\n",
            "Epoch 15, Batch 400, Loss: 1.6495029354095458\n",
            "Epoch 16, Batch 100, Loss: 1.6018736803531646\n",
            "Epoch 16, Batch 200, Loss: 1.5802832460403442\n",
            "Epoch 16, Batch 300, Loss: 1.5483658802509308\n",
            "Epoch 16, Batch 400, Loss: 1.5148568594455718\n",
            "Epoch 17, Batch 100, Loss: 1.4644261944293975\n",
            "Epoch 17, Batch 200, Loss: 1.4247052025794984\n",
            "Epoch 17, Batch 300, Loss: 1.398174296617508\n",
            "Epoch 17, Batch 400, Loss: 1.3569051897525788\n",
            "Epoch 18, Batch 100, Loss: 1.3132390367984772\n",
            "Epoch 18, Batch 200, Loss: 1.2725500178337097\n",
            "Epoch 18, Batch 300, Loss: 1.2502844321727753\n",
            "Epoch 18, Batch 400, Loss: 1.2261902058124543\n",
            "Epoch 19, Batch 100, Loss: 1.1758538043498994\n",
            "Epoch 19, Batch 200, Loss: 1.1627813839912415\n",
            "Epoch 19, Batch 300, Loss: 1.1368313598632813\n",
            "Epoch 19, Batch 400, Loss: 1.1264129692316056\n",
            "Epoch 20, Batch 100, Loss: 1.0885869324207307\n",
            "Epoch 20, Batch 200, Loss: 1.0692019349336623\n",
            "Epoch 20, Batch 300, Loss: 1.0554475104808807\n",
            "Epoch 20, Batch 400, Loss: 1.0392008090019227\n",
            "Epoch 21, Batch 100, Loss: 1.0178279656171798\n",
            "Epoch 21, Batch 200, Loss: 1.0107205414772034\n",
            "Epoch 21, Batch 300, Loss: 0.9918511283397674\n",
            "Epoch 21, Batch 400, Loss: 0.9899219220876694\n",
            "Epoch 22, Batch 100, Loss: 0.9789696031808853\n",
            "Epoch 22, Batch 200, Loss: 0.9554212045669556\n",
            "Epoch 22, Batch 300, Loss: 0.9352288609743118\n",
            "Epoch 22, Batch 400, Loss: 0.9291570979356766\n",
            "Epoch 23, Batch 100, Loss: 0.9214887541532516\n",
            "Epoch 23, Batch 200, Loss: 0.9053680276870728\n",
            "Epoch 23, Batch 300, Loss: 0.9054962384700775\n",
            "Epoch 23, Batch 400, Loss: 0.9009668809175492\n",
            "Epoch 24, Batch 100, Loss: 0.8862072610855103\n",
            "Epoch 24, Batch 200, Loss: 0.8684137356281281\n",
            "Epoch 24, Batch 300, Loss: 0.8699960064888\n",
            "Epoch 24, Batch 400, Loss: 0.8646697145700455\n",
            "Epoch 25, Batch 100, Loss: 0.8507764178514481\n",
            "Epoch 25, Batch 200, Loss: 0.8510135978460311\n",
            "Epoch 25, Batch 300, Loss: 0.8304517078399658\n",
            "Epoch 25, Batch 400, Loss: 0.8349782365560532\n",
            "Finished Training\n",
            "Accuracy on test set: 0.6959%\n",
            "\n",
            "Configuration: {'hidden_sizes': [128, 128, 128, 128], 'num_layers': 4, 'lr': 0.003, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
            "Epoch 1, Batch 100, Loss: 1.1865643149614333\n",
            "Epoch 1, Batch 200, Loss: 0.6198864212632179\n",
            "Epoch 1, Batch 300, Loss: 0.5426698741316796\n",
            "Epoch 1, Batch 400, Loss: 0.4989906145632267\n",
            "Epoch 2, Batch 100, Loss: 0.4525280460715294\n",
            "Epoch 2, Batch 200, Loss: 0.4460716581344604\n",
            "Epoch 2, Batch 300, Loss: 0.43154088377952576\n",
            "Epoch 2, Batch 400, Loss: 0.43078156873583795\n",
            "Epoch 3, Batch 100, Loss: 0.39866105616092684\n",
            "Epoch 3, Batch 200, Loss: 0.3902939882874489\n",
            "Epoch 3, Batch 300, Loss: 0.3885317081212997\n",
            "Epoch 3, Batch 400, Loss: 0.3843399238586426\n",
            "Epoch 4, Batch 100, Loss: 0.36960828825831415\n",
            "Epoch 4, Batch 200, Loss: 0.35199898064136503\n",
            "Epoch 4, Batch 300, Loss: 0.3561901608109474\n",
            "Epoch 4, Batch 400, Loss: 0.36485876560211183\n",
            "Epoch 5, Batch 100, Loss: 0.34091196477413177\n",
            "Epoch 5, Batch 200, Loss: 0.34133034124970435\n",
            "Epoch 5, Batch 300, Loss: 0.34633205592632293\n",
            "Epoch 5, Batch 400, Loss: 0.34213618919253347\n",
            "Epoch 6, Batch 100, Loss: 0.39096142157912256\n",
            "Epoch 6, Batch 200, Loss: 0.3231501504778862\n",
            "Epoch 6, Batch 300, Loss: 0.3261742924153805\n",
            "Epoch 6, Batch 400, Loss: 0.31537079825997355\n",
            "Epoch 7, Batch 100, Loss: 0.3396731136739254\n",
            "Epoch 7, Batch 200, Loss: 0.3156504389643669\n",
            "Epoch 7, Batch 300, Loss: 0.3118132346868515\n",
            "Epoch 7, Batch 400, Loss: 0.31030437275767325\n",
            "Epoch 8, Batch 100, Loss: 0.3060049565136433\n",
            "Epoch 8, Batch 200, Loss: 0.2959877362847328\n",
            "Epoch 8, Batch 300, Loss: 0.3079076886177063\n",
            "Epoch 8, Batch 400, Loss: 0.31252756297588347\n",
            "Epoch 9, Batch 100, Loss: 0.2861504277586937\n",
            "Epoch 9, Batch 200, Loss: 0.30738275215029714\n",
            "Epoch 9, Batch 300, Loss: 0.30227508783340457\n",
            "Epoch 9, Batch 400, Loss: 0.29655340224504473\n",
            "Epoch 10, Batch 100, Loss: 0.28236974716186525\n",
            "Epoch 10, Batch 200, Loss: 0.299440141171217\n",
            "Epoch 10, Batch 300, Loss: 0.286538138538599\n",
            "Epoch 10, Batch 400, Loss: 0.28900681763887404\n",
            "Epoch 11, Batch 100, Loss: 0.2926308873295784\n",
            "Epoch 11, Batch 200, Loss: 0.29771665498614314\n",
            "Epoch 11, Batch 300, Loss: 0.27028326734900476\n",
            "Epoch 11, Batch 400, Loss: 0.29789597779512406\n",
            "Epoch 12, Batch 100, Loss: 0.2811268588900566\n",
            "Epoch 12, Batch 200, Loss: 0.28492814213037493\n",
            "Epoch 12, Batch 300, Loss: 0.2780178625881672\n",
            "Epoch 12, Batch 400, Loss: 0.27199422255158423\n",
            "Epoch 13, Batch 100, Loss: 0.2684485141187906\n",
            "Epoch 13, Batch 200, Loss: 0.25935985535383227\n",
            "Epoch 13, Batch 300, Loss: 0.28468329034745693\n",
            "Epoch 13, Batch 400, Loss: 0.2868258121609688\n",
            "Epoch 14, Batch 100, Loss: 0.2612378866970539\n",
            "Epoch 14, Batch 200, Loss: 0.2671251295506954\n",
            "Epoch 14, Batch 300, Loss: 0.27375507950782774\n",
            "Epoch 14, Batch 400, Loss: 0.2737796859443188\n",
            "Epoch 15, Batch 100, Loss: 0.2637993597984314\n",
            "Epoch 15, Batch 200, Loss: 0.2817391467094421\n",
            "Epoch 15, Batch 300, Loss: 0.25901474818587306\n",
            "Epoch 15, Batch 400, Loss: 0.3040731067955494\n",
            "Epoch 16, Batch 100, Loss: 0.2413903135061264\n",
            "Epoch 16, Batch 200, Loss: 0.2645035184919834\n",
            "Epoch 16, Batch 300, Loss: 0.24681776255369187\n",
            "Epoch 16, Batch 400, Loss: 0.2587564018368721\n",
            "Epoch 17, Batch 100, Loss: 0.24525442779064177\n",
            "Epoch 17, Batch 200, Loss: 0.25647155359387397\n",
            "Epoch 17, Batch 300, Loss: 0.2655318419635296\n",
            "Epoch 17, Batch 400, Loss: 0.25528824955224994\n",
            "Epoch 18, Batch 100, Loss: 0.2580490228533745\n",
            "Epoch 18, Batch 200, Loss: 0.24969452619552612\n",
            "Epoch 18, Batch 300, Loss: 0.24907976426184178\n",
            "Epoch 18, Batch 400, Loss: 0.25895565807819365\n",
            "Epoch 19, Batch 100, Loss: 0.24216729238629342\n",
            "Epoch 19, Batch 200, Loss: 0.24544296406209468\n",
            "Epoch 19, Batch 300, Loss: 0.24970477029681207\n",
            "Epoch 19, Batch 400, Loss: 0.25934145867824554\n",
            "Epoch 20, Batch 100, Loss: 0.2438752593845129\n",
            "Epoch 20, Batch 200, Loss: 0.2553675113618374\n",
            "Epoch 20, Batch 300, Loss: 0.24793917134404184\n",
            "Epoch 20, Batch 400, Loss: 0.24991065353155137\n",
            "Epoch 21, Batch 100, Loss: 0.2358201538026333\n",
            "Epoch 21, Batch 200, Loss: 0.2426482678204775\n",
            "Epoch 21, Batch 300, Loss: 0.24718299232423305\n",
            "Epoch 21, Batch 400, Loss: 0.26878216207027433\n",
            "Epoch 22, Batch 100, Loss: 0.24354056462645532\n",
            "Epoch 22, Batch 200, Loss: 0.24885616026818752\n",
            "Epoch 22, Batch 300, Loss: 0.23950187675654888\n",
            "Epoch 22, Batch 400, Loss: 0.23282395869493486\n",
            "Epoch 23, Batch 100, Loss: 0.22902634538710118\n",
            "Epoch 23, Batch 200, Loss: 0.24088382869958877\n",
            "Epoch 23, Batch 300, Loss: 0.24985147133469582\n",
            "Epoch 23, Batch 400, Loss: 0.24629355244338513\n",
            "Epoch 24, Batch 100, Loss: 0.23276369728147983\n",
            "Epoch 24, Batch 200, Loss: 0.22568094730377197\n",
            "Epoch 24, Batch 300, Loss: 0.24907992325723172\n",
            "Epoch 24, Batch 400, Loss: 0.24983242720365526\n",
            "Epoch 25, Batch 100, Loss: 0.22176434457302094\n",
            "Epoch 25, Batch 200, Loss: 0.25729078218340873\n",
            "Epoch 25, Batch 300, Loss: 0.23492973789572716\n",
            "Epoch 25, Batch 400, Loss: 0.233053807169199\n",
            "Finished Training\n",
            "Accuracy on test set: 0.8747%\n",
            "\n",
            "Configuration: {'hidden_sizes': [256, 128, 64], 'num_layers': 3, 'lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
            "Epoch 1, Batch 100, Loss: 2.300007019042969\n",
            "Epoch 1, Batch 200, Loss: 2.2939883995056154\n",
            "Epoch 1, Batch 300, Loss: 2.2889122676849367\n",
            "Epoch 1, Batch 400, Loss: 2.281533215045929\n",
            "Epoch 2, Batch 100, Loss: 2.2715063285827637\n",
            "Epoch 2, Batch 200, Loss: 2.2645279383659362\n",
            "Epoch 2, Batch 300, Loss: 2.2577925300598145\n",
            "Epoch 2, Batch 400, Loss: 2.2489514136314392\n",
            "Epoch 3, Batch 100, Loss: 2.233947060108185\n",
            "Epoch 3, Batch 200, Loss: 2.224209771156311\n",
            "Epoch 3, Batch 300, Loss: 2.210404484272003\n",
            "Epoch 3, Batch 400, Loss: 2.1962748837471007\n",
            "Epoch 4, Batch 100, Loss: 2.17010390996933\n",
            "Epoch 4, Batch 200, Loss: 2.15026917219162\n",
            "Epoch 4, Batch 300, Loss: 2.1300992822647093\n",
            "Epoch 4, Batch 400, Loss: 2.106166653633118\n",
            "Epoch 5, Batch 100, Loss: 2.0519576871395113\n",
            "Epoch 5, Batch 200, Loss: 2.0205345797538756\n",
            "Epoch 5, Batch 300, Loss: 1.983008862733841\n",
            "Epoch 5, Batch 400, Loss: 1.9391713869571685\n",
            "Epoch 6, Batch 100, Loss: 1.851942617893219\n",
            "Epoch 6, Batch 200, Loss: 1.8080229103565215\n",
            "Epoch 6, Batch 300, Loss: 1.7550287783145904\n",
            "Epoch 6, Batch 400, Loss: 1.6966262662410736\n",
            "Epoch 7, Batch 100, Loss: 1.5957053864002229\n",
            "Epoch 7, Batch 200, Loss: 1.5453643715381622\n",
            "Epoch 7, Batch 300, Loss: 1.5103387200832368\n",
            "Epoch 7, Batch 400, Loss: 1.4521154797077178\n",
            "Epoch 8, Batch 100, Loss: 1.3756656682491302\n",
            "Epoch 8, Batch 200, Loss: 1.338933299779892\n",
            "Epoch 8, Batch 300, Loss: 1.306531127691269\n",
            "Epoch 8, Batch 400, Loss: 1.2676892268657685\n",
            "Epoch 9, Batch 100, Loss: 1.216565192937851\n",
            "Epoch 9, Batch 200, Loss: 1.1948714673519134\n",
            "Epoch 9, Batch 300, Loss: 1.1622172844409944\n",
            "Epoch 9, Batch 400, Loss: 1.135227584838867\n",
            "Epoch 10, Batch 100, Loss: 1.096103623509407\n",
            "Epoch 10, Batch 200, Loss: 1.070171791911125\n",
            "Epoch 10, Batch 300, Loss: 1.064457165002823\n",
            "Epoch 10, Batch 400, Loss: 1.0528088504076003\n",
            "Epoch 11, Batch 100, Loss: 1.0152689659595489\n",
            "Epoch 11, Batch 200, Loss: 0.9959687709808349\n",
            "Epoch 11, Batch 300, Loss: 0.9767974162101746\n",
            "Epoch 11, Batch 400, Loss: 0.9754059982299804\n",
            "Epoch 12, Batch 100, Loss: 0.9552430558204651\n",
            "Epoch 12, Batch 200, Loss: 0.9358661329746246\n",
            "Epoch 12, Batch 300, Loss: 0.9257787775993347\n",
            "Epoch 12, Batch 400, Loss: 0.9153775310516358\n",
            "Epoch 13, Batch 100, Loss: 0.8983721649646759\n",
            "Epoch 13, Batch 200, Loss: 0.8841676884889602\n",
            "Epoch 13, Batch 300, Loss: 0.8779546982049942\n",
            "Epoch 13, Batch 400, Loss: 0.8683414292335511\n",
            "Epoch 14, Batch 100, Loss: 0.861877326965332\n",
            "Epoch 14, Batch 200, Loss: 0.8453270626068116\n",
            "Epoch 14, Batch 300, Loss: 0.8368972533941269\n",
            "Epoch 14, Batch 400, Loss: 0.8322176623344422\n",
            "Epoch 15, Batch 100, Loss: 0.8133655804395675\n",
            "Epoch 15, Batch 200, Loss: 0.8239936739206314\n",
            "Epoch 15, Batch 300, Loss: 0.793702456355095\n",
            "Epoch 15, Batch 400, Loss: 0.8027258086204528\n",
            "Epoch 16, Batch 100, Loss: 0.7825040036439895\n",
            "Epoch 16, Batch 200, Loss: 0.7856348484754563\n",
            "Epoch 16, Batch 300, Loss: 0.77924833714962\n",
            "Epoch 16, Batch 400, Loss: 0.7731538546085358\n",
            "Epoch 17, Batch 100, Loss: 0.7566016161441803\n",
            "Epoch 17, Batch 200, Loss: 0.7498612850904465\n",
            "Epoch 17, Batch 300, Loss: 0.7612369579076766\n",
            "Epoch 17, Batch 400, Loss: 0.7492358589172363\n",
            "Epoch 18, Batch 100, Loss: 0.7375734329223633\n",
            "Epoch 18, Batch 200, Loss: 0.7341345453262329\n",
            "Epoch 18, Batch 300, Loss: 0.7296041744947434\n",
            "Epoch 18, Batch 400, Loss: 0.7163511681556701\n",
            "Epoch 19, Batch 100, Loss: 0.7240167140960694\n",
            "Epoch 19, Batch 200, Loss: 0.7081672841310501\n",
            "Epoch 19, Batch 300, Loss: 0.6977274018526077\n",
            "Epoch 19, Batch 400, Loss: 0.7055479973554611\n",
            "Epoch 20, Batch 100, Loss: 0.6944521909952164\n",
            "Epoch 20, Batch 200, Loss: 0.7002119207382203\n",
            "Epoch 20, Batch 300, Loss: 0.6865670019388199\n",
            "Epoch 20, Batch 400, Loss: 0.6897586518526078\n",
            "Epoch 21, Batch 100, Loss: 0.6881908595561981\n",
            "Epoch 21, Batch 200, Loss: 0.680499632358551\n",
            "Epoch 21, Batch 300, Loss: 0.6763030409812927\n",
            "Epoch 21, Batch 400, Loss: 0.6681933772563934\n",
            "Epoch 22, Batch 100, Loss: 0.6760769271850586\n",
            "Epoch 22, Batch 200, Loss: 0.6781876564025879\n",
            "Epoch 22, Batch 300, Loss: 0.6534934443235397\n",
            "Epoch 22, Batch 400, Loss: 0.6563638216257095\n",
            "Epoch 23, Batch 100, Loss: 0.6649406927824021\n",
            "Epoch 23, Batch 200, Loss: 0.6486058926582337\n",
            "Epoch 23, Batch 300, Loss: 0.6461158379912376\n",
            "Epoch 23, Batch 400, Loss: 0.6419638264179229\n",
            "Epoch 24, Batch 100, Loss: 0.6559825828671455\n",
            "Epoch 24, Batch 200, Loss: 0.6369056805968285\n",
            "Epoch 24, Batch 300, Loss: 0.6437236231565475\n",
            "Epoch 24, Batch 400, Loss: 0.6418455973267555\n",
            "Epoch 25, Batch 100, Loss: 0.6311460798978805\n",
            "Epoch 25, Batch 200, Loss: 0.6390778627991677\n",
            "Epoch 25, Batch 300, Loss: 0.6342420434951782\n",
            "Epoch 25, Batch 400, Loss: 0.6275668194890023\n",
            "Finished Training\n",
            "Accuracy on test set: 0.7635%\n",
            "\n",
            "Configuration: {'hidden_sizes': [256, 128, 64], 'num_layers': 3, 'lr': 0.003, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
            "Epoch 1, Batch 100, Loss: 1.408346226811409\n",
            "Epoch 1, Batch 200, Loss: 0.5698585528135299\n",
            "Epoch 1, Batch 300, Loss: 0.5008945626020431\n",
            "Epoch 1, Batch 400, Loss: 0.4840191265940666\n",
            "Epoch 2, Batch 100, Loss: 0.4266340979933739\n",
            "Epoch 2, Batch 200, Loss: 0.4270786651968956\n",
            "Epoch 2, Batch 300, Loss: 0.4189547598361969\n",
            "Epoch 2, Batch 400, Loss: 0.3954310113191605\n",
            "Epoch 3, Batch 100, Loss: 0.3720199181139469\n",
            "Epoch 3, Batch 200, Loss: 0.3794648490846157\n",
            "Epoch 3, Batch 300, Loss: 0.3711527317762375\n",
            "Epoch 3, Batch 400, Loss: 0.3691638319194317\n",
            "Epoch 4, Batch 100, Loss: 0.3334555472433567\n",
            "Epoch 4, Batch 200, Loss: 0.3516033360362053\n",
            "Epoch 4, Batch 300, Loss: 0.359540509134531\n",
            "Epoch 4, Batch 400, Loss: 0.3430161418020725\n",
            "Epoch 5, Batch 100, Loss: 0.31013004258275034\n",
            "Epoch 5, Batch 200, Loss: 0.3233588020503521\n",
            "Epoch 5, Batch 300, Loss: 0.32822738602757456\n",
            "Epoch 5, Batch 400, Loss: 0.325675595253706\n",
            "Epoch 6, Batch 100, Loss: 0.3116816924512386\n",
            "Epoch 6, Batch 200, Loss: 0.3052412924170494\n",
            "Epoch 6, Batch 300, Loss: 0.310987389087677\n",
            "Epoch 6, Batch 400, Loss: 0.3089514746516943\n",
            "Epoch 7, Batch 100, Loss: 0.2974756245315075\n",
            "Epoch 7, Batch 200, Loss: 0.29154833272099495\n",
            "Epoch 7, Batch 300, Loss: 0.2986044645309448\n",
            "Epoch 7, Batch 400, Loss: 0.29374470338225367\n",
            "Epoch 8, Batch 100, Loss: 0.29024705193936823\n",
            "Epoch 8, Batch 200, Loss: 0.27732532352209094\n",
            "Epoch 8, Batch 300, Loss: 0.2959649021923542\n",
            "Epoch 8, Batch 400, Loss: 0.288437372893095\n",
            "Epoch 9, Batch 100, Loss: 0.26991754189133643\n",
            "Epoch 9, Batch 200, Loss: 0.27251138642430306\n",
            "Epoch 9, Batch 300, Loss: 0.2764121077954769\n",
            "Epoch 9, Batch 400, Loss: 0.27866059452295305\n",
            "Epoch 10, Batch 100, Loss: 0.26138294592499733\n",
            "Epoch 10, Batch 200, Loss: 0.26494159311056137\n",
            "Epoch 10, Batch 300, Loss: 0.26762558028101924\n",
            "Epoch 10, Batch 400, Loss: 0.2634772643446922\n",
            "Epoch 11, Batch 100, Loss: 0.25602580584585666\n",
            "Epoch 11, Batch 200, Loss: 0.2535783515870571\n",
            "Epoch 11, Batch 300, Loss: 0.25774983808398244\n",
            "Epoch 11, Batch 400, Loss: 0.267242097556591\n",
            "Epoch 12, Batch 100, Loss: 0.2343017940968275\n",
            "Epoch 12, Batch 200, Loss: 0.24656157836318016\n",
            "Epoch 12, Batch 300, Loss: 0.2583613046258688\n",
            "Epoch 12, Batch 400, Loss: 0.26405055716633796\n",
            "Epoch 13, Batch 100, Loss: 0.23793864697217942\n",
            "Epoch 13, Batch 200, Loss: 0.22902317151427268\n",
            "Epoch 13, Batch 300, Loss: 0.25375837936997414\n",
            "Epoch 13, Batch 400, Loss: 0.25407819032669066\n",
            "Epoch 14, Batch 100, Loss: 0.23216951951384546\n",
            "Epoch 14, Batch 200, Loss: 0.236398950740695\n",
            "Epoch 14, Batch 300, Loss: 0.23632058531045913\n",
            "Epoch 14, Batch 400, Loss: 0.24925198495388032\n",
            "Epoch 15, Batch 100, Loss: 0.22047644466161728\n",
            "Epoch 15, Batch 200, Loss: 0.22971701942384243\n",
            "Epoch 15, Batch 300, Loss: 0.23986030504107475\n",
            "Epoch 15, Batch 400, Loss: 0.23711603492498398\n",
            "Epoch 16, Batch 100, Loss: 0.22253271050751208\n",
            "Epoch 16, Batch 200, Loss: 0.22551113307476045\n",
            "Epoch 16, Batch 300, Loss: 0.23093422025442123\n",
            "Epoch 16, Batch 400, Loss: 0.22846237428486346\n",
            "Epoch 17, Batch 100, Loss: 0.22768343649804593\n",
            "Epoch 17, Batch 200, Loss: 0.24040368363261222\n",
            "Epoch 17, Batch 300, Loss: 0.226414610221982\n",
            "Epoch 17, Batch 400, Loss: 0.21714226685464383\n",
            "Epoch 18, Batch 100, Loss: 0.20053867153823376\n",
            "Epoch 18, Batch 200, Loss: 0.226882034316659\n",
            "Epoch 18, Batch 300, Loss: 0.2207757393270731\n",
            "Epoch 18, Batch 400, Loss: 0.2286345511674881\n",
            "Epoch 19, Batch 100, Loss: 0.20021313540637492\n",
            "Epoch 19, Batch 200, Loss: 0.2119758950918913\n",
            "Epoch 19, Batch 300, Loss: 0.21712052538990975\n",
            "Epoch 19, Batch 400, Loss: 0.2203253335505724\n",
            "Epoch 20, Batch 100, Loss: 0.21662193082273007\n",
            "Epoch 20, Batch 200, Loss: 0.20719784528017043\n",
            "Epoch 20, Batch 300, Loss: 0.21368175067007542\n",
            "Epoch 20, Batch 400, Loss: 0.20274416834115982\n",
            "Epoch 21, Batch 100, Loss: 0.20056127578020097\n",
            "Epoch 21, Batch 200, Loss: 0.2246160789579153\n",
            "Epoch 21, Batch 300, Loss: 0.2027703434973955\n",
            "Epoch 21, Batch 400, Loss: 0.20253318198025227\n",
            "Epoch 22, Batch 100, Loss: 0.19143444769084453\n",
            "Epoch 22, Batch 200, Loss: 0.1972915504872799\n",
            "Epoch 22, Batch 300, Loss: 0.20125040970742702\n",
            "Epoch 22, Batch 400, Loss: 0.21396509133279323\n",
            "Epoch 23, Batch 100, Loss: 0.1906813183426857\n",
            "Epoch 23, Batch 200, Loss: 0.19405211932957173\n",
            "Epoch 23, Batch 300, Loss: 0.19943878896534442\n",
            "Epoch 23, Batch 400, Loss: 0.21003158137202263\n",
            "Epoch 24, Batch 100, Loss: 0.2001896495372057\n",
            "Epoch 24, Batch 200, Loss: 0.19532127663493157\n",
            "Epoch 24, Batch 300, Loss: 0.20873283691704272\n",
            "Epoch 24, Batch 400, Loss: 0.20033588275313377\n",
            "Epoch 25, Batch 100, Loss: 0.19240658670663835\n",
            "Epoch 25, Batch 200, Loss: 0.20233915776014327\n",
            "Epoch 25, Batch 300, Loss: 0.20141564503312112\n",
            "Epoch 25, Batch 400, Loss: 0.20620456524193287\n",
            "Finished Training\n",
            "Accuracy on test set: 0.8853%\n",
            "\n",
            "Configuration: {'hidden_sizes': [128, 64], 'num_layers': 2, 'lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
            "Epoch 1, Batch 100, Loss: 2.295090503692627\n",
            "Epoch 1, Batch 200, Loss: 2.2614556670188906\n",
            "Epoch 1, Batch 300, Loss: 2.2276083970069886\n",
            "Epoch 1, Batch 400, Loss: 2.1896003603935243\n",
            "Epoch 2, Batch 100, Loss: 2.127745668888092\n",
            "Epoch 2, Batch 200, Loss: 2.0838791847229006\n",
            "Epoch 2, Batch 300, Loss: 2.0396274387836457\n",
            "Epoch 2, Batch 400, Loss: 1.9925699377059936\n",
            "Epoch 3, Batch 100, Loss: 1.8886431992053985\n",
            "Epoch 3, Batch 200, Loss: 1.828651249408722\n",
            "Epoch 3, Batch 300, Loss: 1.7580558395385741\n",
            "Epoch 3, Batch 400, Loss: 1.6893616795539856\n",
            "Epoch 4, Batch 100, Loss: 1.5732395482063293\n",
            "Epoch 4, Batch 200, Loss: 1.508717453479767\n",
            "Epoch 4, Batch 300, Loss: 1.4522587609291078\n",
            "Epoch 4, Batch 400, Loss: 1.400299402475357\n",
            "Epoch 5, Batch 100, Loss: 1.3187254989147186\n",
            "Epoch 5, Batch 200, Loss: 1.2685830974578858\n",
            "Epoch 5, Batch 300, Loss: 1.2271191585063934\n",
            "Epoch 5, Batch 400, Loss: 1.2016149806976317\n",
            "Epoch 6, Batch 100, Loss: 1.1327918124198915\n",
            "Epoch 6, Batch 200, Loss: 1.1090613067150117\n",
            "Epoch 6, Batch 300, Loss: 1.0778717523813248\n",
            "Epoch 6, Batch 400, Loss: 1.0696690064668655\n",
            "Epoch 7, Batch 100, Loss: 1.0266163992881774\n",
            "Epoch 7, Batch 200, Loss: 1.0064417487382888\n",
            "Epoch 7, Batch 300, Loss: 0.9784144634008407\n",
            "Epoch 7, Batch 400, Loss: 0.9566844111680984\n",
            "Epoch 8, Batch 100, Loss: 0.9328593778610229\n",
            "Epoch 8, Batch 200, Loss: 0.9224097537994385\n",
            "Epoch 8, Batch 300, Loss: 0.8985552167892457\n",
            "Epoch 8, Batch 400, Loss: 0.8942091691493989\n",
            "Epoch 9, Batch 100, Loss: 0.8693340253829956\n",
            "Epoch 9, Batch 200, Loss: 0.8641769045591354\n",
            "Epoch 9, Batch 300, Loss: 0.8493541324138641\n",
            "Epoch 9, Batch 400, Loss: 0.8340972942113877\n",
            "Epoch 10, Batch 100, Loss: 0.8261472791433334\n",
            "Epoch 10, Batch 200, Loss: 0.8114417642354965\n",
            "Epoch 10, Batch 300, Loss: 0.8058699923753738\n",
            "Epoch 10, Batch 400, Loss: 0.8090949034690857\n",
            "Epoch 11, Batch 100, Loss: 0.7918161815404892\n",
            "Epoch 11, Batch 200, Loss: 0.7714157265424728\n",
            "Epoch 11, Batch 300, Loss: 0.7719916713237762\n",
            "Epoch 11, Batch 400, Loss: 0.7686641162633896\n",
            "Epoch 12, Batch 100, Loss: 0.767133212685585\n",
            "Epoch 12, Batch 200, Loss: 0.7491421782970429\n",
            "Epoch 12, Batch 300, Loss: 0.7443410420417785\n",
            "Epoch 12, Batch 400, Loss: 0.736631554365158\n",
            "Epoch 13, Batch 100, Loss: 0.7296745944023132\n",
            "Epoch 13, Batch 200, Loss: 0.7233173429965973\n",
            "Epoch 13, Batch 300, Loss: 0.7252074122428894\n",
            "Epoch 13, Batch 400, Loss: 0.7259903210401535\n",
            "Epoch 14, Batch 100, Loss: 0.7094391489028931\n",
            "Epoch 14, Batch 200, Loss: 0.7098514854907989\n",
            "Epoch 14, Batch 300, Loss: 0.7115876251459121\n",
            "Epoch 14, Batch 400, Loss: 0.7038399666547775\n",
            "Epoch 15, Batch 100, Loss: 0.7083133417367935\n",
            "Epoch 15, Batch 200, Loss: 0.6824790692329407\n",
            "Epoch 15, Batch 300, Loss: 0.6862492060661316\n",
            "Epoch 15, Batch 400, Loss: 0.6790272137522697\n",
            "Epoch 16, Batch 100, Loss: 0.6700997149944306\n",
            "Epoch 16, Batch 200, Loss: 0.6850258493423462\n",
            "Epoch 16, Batch 300, Loss: 0.677731893658638\n",
            "Epoch 16, Batch 400, Loss: 0.6651559028029442\n",
            "Epoch 17, Batch 100, Loss: 0.6542686620354652\n",
            "Epoch 17, Batch 200, Loss: 0.6682739636301994\n",
            "Epoch 17, Batch 300, Loss: 0.6596258175373078\n",
            "Epoch 17, Batch 400, Loss: 0.6587310045957565\n",
            "Epoch 18, Batch 100, Loss: 0.6466411608457565\n",
            "Epoch 18, Batch 200, Loss: 0.6468618613481522\n",
            "Epoch 18, Batch 300, Loss: 0.6541350623965263\n",
            "Epoch 18, Batch 400, Loss: 0.6410695022344589\n",
            "Epoch 19, Batch 100, Loss: 0.6397643169760704\n",
            "Epoch 19, Batch 200, Loss: 0.6407931751012802\n",
            "Epoch 19, Batch 300, Loss: 0.6256072667241096\n",
            "Epoch 19, Batch 400, Loss: 0.6458035564422607\n",
            "Epoch 20, Batch 100, Loss: 0.628377674818039\n",
            "Epoch 20, Batch 200, Loss: 0.6347252669930458\n",
            "Epoch 20, Batch 300, Loss: 0.6174116045236587\n",
            "Epoch 20, Batch 400, Loss: 0.6182751151919365\n",
            "Epoch 21, Batch 100, Loss: 0.61853540122509\n",
            "Epoch 21, Batch 200, Loss: 0.6216177386045456\n",
            "Epoch 21, Batch 300, Loss: 0.6203354185819626\n",
            "Epoch 21, Batch 400, Loss: 0.6023954364657402\n",
            "Epoch 22, Batch 100, Loss: 0.6110340559482574\n",
            "Epoch 22, Batch 200, Loss: 0.6033499759435653\n",
            "Epoch 22, Batch 300, Loss: 0.6134437787532806\n",
            "Epoch 22, Batch 400, Loss: 0.5987810665369033\n",
            "Epoch 23, Batch 100, Loss: 0.5985338148474694\n",
            "Epoch 23, Batch 200, Loss: 0.5961691895127297\n",
            "Epoch 23, Batch 300, Loss: 0.5945754638314247\n",
            "Epoch 23, Batch 400, Loss: 0.5940595009922981\n",
            "Epoch 24, Batch 100, Loss: 0.5875741899013519\n",
            "Epoch 24, Batch 200, Loss: 0.5850302404165268\n",
            "Epoch 24, Batch 300, Loss: 0.5881454917788506\n",
            "Epoch 24, Batch 400, Loss: 0.5967896792292595\n",
            "Epoch 25, Batch 100, Loss: 0.5879078367352486\n",
            "Epoch 25, Batch 200, Loss: 0.5813249757885933\n",
            "Epoch 25, Batch 300, Loss: 0.5801823306083679\n",
            "Epoch 25, Batch 400, Loss: 0.5755363446474075\n",
            "Finished Training\n",
            "Accuracy on test set: 0.7848%\n",
            "\n",
            "Configuration: {'hidden_sizes': [128, 64], 'num_layers': 2, 'lr': 0.003, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
            "Epoch 1, Batch 100, Loss: 0.9307898104190826\n",
            "Epoch 1, Batch 200, Loss: 0.5345565310120582\n",
            "Epoch 1, Batch 300, Loss: 0.48617903620004654\n",
            "Epoch 1, Batch 400, Loss: 0.4561635214090347\n",
            "Epoch 2, Batch 100, Loss: 0.4087515750527382\n",
            "Epoch 2, Batch 200, Loss: 0.4022035636007786\n",
            "Epoch 2, Batch 300, Loss: 0.3977124297618866\n",
            "Epoch 2, Batch 400, Loss: 0.3969546030461788\n",
            "Epoch 3, Batch 100, Loss: 0.36223304077982904\n",
            "Epoch 3, Batch 200, Loss: 0.3551830103993416\n",
            "Epoch 3, Batch 300, Loss: 0.3693029761314392\n",
            "Epoch 3, Batch 400, Loss: 0.3712959824502468\n",
            "Epoch 4, Batch 100, Loss: 0.33569914534687995\n",
            "Epoch 4, Batch 200, Loss: 0.3329558378458023\n",
            "Epoch 4, Batch 300, Loss: 0.3315748780965805\n",
            "Epoch 4, Batch 400, Loss: 0.32675354287028313\n",
            "Epoch 5, Batch 100, Loss: 0.3090363247692585\n",
            "Epoch 5, Batch 200, Loss: 0.3302194894850254\n",
            "Epoch 5, Batch 300, Loss: 0.3133143624663353\n",
            "Epoch 5, Batch 400, Loss: 0.3167885695397854\n",
            "Epoch 6, Batch 100, Loss: 0.2910365842282772\n",
            "Epoch 6, Batch 200, Loss: 0.30842199236154555\n",
            "Epoch 6, Batch 300, Loss: 0.3087552760541439\n",
            "Epoch 6, Batch 400, Loss: 0.29981613129377366\n",
            "Epoch 7, Batch 100, Loss: 0.2884662719070911\n",
            "Epoch 7, Batch 200, Loss: 0.30062454074621203\n",
            "Epoch 7, Batch 300, Loss: 0.27945001229643823\n",
            "Epoch 7, Batch 400, Loss: 0.2982077145576477\n",
            "Epoch 8, Batch 100, Loss: 0.2761786209046841\n",
            "Epoch 8, Batch 200, Loss: 0.29077389657497404\n",
            "Epoch 8, Batch 300, Loss: 0.28436046212911603\n",
            "Epoch 8, Batch 400, Loss: 0.28655161052942274\n",
            "Epoch 9, Batch 100, Loss: 0.26761619314551355\n",
            "Epoch 9, Batch 200, Loss: 0.27184142723679544\n",
            "Epoch 9, Batch 300, Loss: 0.2762046666443348\n",
            "Epoch 9, Batch 400, Loss: 0.2681319396197796\n",
            "Epoch 10, Batch 100, Loss: 0.25390484228730203\n",
            "Epoch 10, Batch 200, Loss: 0.27111037865281107\n",
            "Epoch 10, Batch 300, Loss: 0.27007898449897766\n",
            "Epoch 10, Batch 400, Loss: 0.26383871376514434\n",
            "Epoch 11, Batch 100, Loss: 0.2443943639099598\n",
            "Epoch 11, Batch 200, Loss: 0.2687749224901199\n",
            "Epoch 11, Batch 300, Loss: 0.25652301631867885\n",
            "Epoch 11, Batch 400, Loss: 0.2539860333502293\n",
            "Epoch 12, Batch 100, Loss: 0.24730396658182144\n",
            "Epoch 12, Batch 200, Loss: 0.2540407755225897\n",
            "Epoch 12, Batch 300, Loss: 0.24345037415623666\n",
            "Epoch 12, Batch 400, Loss: 0.26667633444070815\n",
            "Epoch 13, Batch 100, Loss: 0.236605027616024\n",
            "Epoch 13, Batch 200, Loss: 0.24453687205910682\n",
            "Epoch 13, Batch 300, Loss: 0.24675717256963253\n",
            "Epoch 13, Batch 400, Loss: 0.2521244882047176\n",
            "Epoch 14, Batch 100, Loss: 0.23426790319383145\n",
            "Epoch 14, Batch 200, Loss: 0.2370702190697193\n",
            "Epoch 14, Batch 300, Loss: 0.23402568109333516\n",
            "Epoch 14, Batch 400, Loss: 0.2516124777495861\n",
            "Epoch 15, Batch 100, Loss: 0.22437107466161252\n",
            "Epoch 15, Batch 200, Loss: 0.22992704108357429\n",
            "Epoch 15, Batch 300, Loss: 0.23972949281334877\n",
            "Epoch 15, Batch 400, Loss: 0.23675008490681648\n",
            "Epoch 16, Batch 100, Loss: 0.21809381648898124\n",
            "Epoch 16, Batch 200, Loss: 0.2290337582677603\n",
            "Epoch 16, Batch 300, Loss: 0.23201944775879382\n",
            "Epoch 16, Batch 400, Loss: 0.22895156048238277\n",
            "Epoch 17, Batch 100, Loss: 0.21616591081023218\n",
            "Epoch 17, Batch 200, Loss: 0.21863087847828866\n",
            "Epoch 17, Batch 300, Loss: 0.22504477441310883\n",
            "Epoch 17, Batch 400, Loss: 0.2287612807005644\n",
            "Epoch 18, Batch 100, Loss: 0.2047963433712721\n",
            "Epoch 18, Batch 200, Loss: 0.22074226193130017\n",
            "Epoch 18, Batch 300, Loss: 0.2206223725527525\n",
            "Epoch 18, Batch 400, Loss: 0.22853025615215303\n",
            "Epoch 19, Batch 100, Loss: 0.20621639549732207\n",
            "Epoch 19, Batch 200, Loss: 0.21241897754371167\n",
            "Epoch 19, Batch 300, Loss: 0.2199351889640093\n",
            "Epoch 19, Batch 400, Loss: 0.21408792957663536\n",
            "Epoch 20, Batch 100, Loss: 0.2101803968846798\n",
            "Epoch 20, Batch 200, Loss: 0.20992263056337834\n",
            "Epoch 20, Batch 300, Loss: 0.214431823566556\n",
            "Epoch 20, Batch 400, Loss: 0.2138971857726574\n",
            "Epoch 21, Batch 100, Loss: 0.2041256033629179\n",
            "Epoch 21, Batch 200, Loss: 0.2035339717566967\n",
            "Epoch 21, Batch 300, Loss: 0.2149691616743803\n",
            "Epoch 21, Batch 400, Loss: 0.21113586828112602\n",
            "Epoch 22, Batch 100, Loss: 0.1897764638066292\n",
            "Epoch 22, Batch 200, Loss: 0.21073172122240066\n",
            "Epoch 22, Batch 300, Loss: 0.2055674559623003\n",
            "Epoch 22, Batch 400, Loss: 0.20386106498539447\n",
            "Epoch 23, Batch 100, Loss: 0.19196012243628502\n",
            "Epoch 23, Batch 200, Loss: 0.20284041903913022\n",
            "Epoch 23, Batch 300, Loss: 0.1930421545356512\n",
            "Epoch 23, Batch 400, Loss: 0.20707719638943672\n",
            "Epoch 24, Batch 100, Loss: 0.18929614640772344\n",
            "Epoch 24, Batch 200, Loss: 0.19267354272305964\n",
            "Epoch 24, Batch 300, Loss: 0.20118882276117803\n",
            "Epoch 24, Batch 400, Loss: 0.202719716578722\n",
            "Epoch 25, Batch 100, Loss: 0.18489234790205955\n",
            "Epoch 25, Batch 200, Loss: 0.19691835768520832\n",
            "Epoch 25, Batch 300, Loss: 0.19650515154004097\n",
            "Epoch 25, Batch 400, Loss: 0.19954742074012757\n",
            "Finished Training\n",
            "Accuracy on test set: 0.8749%\n",
            "\n",
            "Configuration: {'hidden_sizes': [32], 'num_layers': 1, 'lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
            "Epoch 1, Batch 100, Loss: 2.28890967130661\n",
            "Epoch 1, Batch 200, Loss: 2.183594295978546\n",
            "Epoch 1, Batch 300, Loss: 2.085428856611252\n",
            "Epoch 1, Batch 400, Loss: 1.9833279752731323\n",
            "Epoch 2, Batch 100, Loss: 1.7962309992313386\n",
            "Epoch 2, Batch 200, Loss: 1.6934112095832825\n",
            "Epoch 2, Batch 300, Loss: 1.587259179353714\n",
            "Epoch 2, Batch 400, Loss: 1.4988127589225768\n",
            "Epoch 3, Batch 100, Loss: 1.363088446855545\n",
            "Epoch 3, Batch 200, Loss: 1.2887342882156372\n",
            "Epoch 3, Batch 300, Loss: 1.2258468627929688\n",
            "Epoch 3, Batch 400, Loss: 1.1813433372974396\n",
            "Epoch 4, Batch 100, Loss: 1.0988521683216095\n",
            "Epoch 4, Batch 200, Loss: 1.055054206252098\n",
            "Epoch 4, Batch 300, Loss: 1.0282124876976013\n",
            "Epoch 4, Batch 400, Loss: 1.0011786353588104\n",
            "Epoch 5, Batch 100, Loss: 0.9572431659698486\n",
            "Epoch 5, Batch 200, Loss: 0.9267684662342072\n",
            "Epoch 5, Batch 300, Loss: 0.9066416436433792\n",
            "Epoch 5, Batch 400, Loss: 0.8859992730617523\n",
            "Epoch 6, Batch 100, Loss: 0.8583302420377731\n",
            "Epoch 6, Batch 200, Loss: 0.843957684636116\n",
            "Epoch 6, Batch 300, Loss: 0.8329284340143204\n",
            "Epoch 6, Batch 400, Loss: 0.8278327131271362\n",
            "Epoch 7, Batch 100, Loss: 0.7907582849264145\n",
            "Epoch 7, Batch 200, Loss: 0.7888854676485062\n",
            "Epoch 7, Batch 300, Loss: 0.7823877584934235\n",
            "Epoch 7, Batch 400, Loss: 0.7740275812149048\n",
            "Epoch 8, Batch 100, Loss: 0.7518288296461105\n",
            "Epoch 8, Batch 200, Loss: 0.7407562869787216\n",
            "Epoch 8, Batch 300, Loss: 0.7483560609817504\n",
            "Epoch 8, Batch 400, Loss: 0.7368520802259445\n",
            "Epoch 9, Batch 100, Loss: 0.7229656529426575\n",
            "Epoch 9, Batch 200, Loss: 0.7111250048875809\n",
            "Epoch 9, Batch 300, Loss: 0.7095802617073059\n",
            "Epoch 9, Batch 400, Loss: 0.7100903552770614\n",
            "Epoch 10, Batch 100, Loss: 0.7030128341913223\n",
            "Epoch 10, Batch 200, Loss: 0.6902874600887299\n",
            "Epoch 10, Batch 300, Loss: 0.6843358606100083\n",
            "Epoch 10, Batch 400, Loss: 0.6883830106258393\n",
            "Epoch 11, Batch 100, Loss: 0.6835695934295655\n",
            "Epoch 11, Batch 200, Loss: 0.668622936308384\n",
            "Epoch 11, Batch 300, Loss: 0.6698427641391754\n",
            "Epoch 11, Batch 400, Loss: 0.6622480717301369\n",
            "Epoch 12, Batch 100, Loss: 0.6584248614311218\n",
            "Epoch 12, Batch 200, Loss: 0.6487230476737023\n",
            "Epoch 12, Batch 300, Loss: 0.6628750723600387\n",
            "Epoch 12, Batch 400, Loss: 0.6525995415449143\n",
            "Epoch 13, Batch 100, Loss: 0.6418757489323617\n",
            "Epoch 13, Batch 200, Loss: 0.6422660130262375\n",
            "Epoch 13, Batch 300, Loss: 0.6427865308523179\n",
            "Epoch 13, Batch 400, Loss: 0.63547682762146\n",
            "Epoch 14, Batch 100, Loss: 0.6422295916080475\n",
            "Epoch 14, Batch 200, Loss: 0.6188101652264595\n",
            "Epoch 14, Batch 300, Loss: 0.6386561325192451\n",
            "Epoch 14, Batch 400, Loss: 0.6241503819823265\n",
            "Epoch 15, Batch 100, Loss: 0.6139875075221062\n",
            "Epoch 15, Batch 200, Loss: 0.621243168413639\n",
            "Epoch 15, Batch 300, Loss: 0.6178422343730926\n",
            "Epoch 15, Batch 400, Loss: 0.6091579663753509\n",
            "Epoch 16, Batch 100, Loss: 0.604938276708126\n",
            "Epoch 16, Batch 200, Loss: 0.6032657110691071\n",
            "Epoch 16, Batch 300, Loss: 0.617452263534069\n",
            "Epoch 16, Batch 400, Loss: 0.6088617965579033\n",
            "Epoch 17, Batch 100, Loss: 0.60223781645298\n",
            "Epoch 17, Batch 200, Loss: 0.6003828698396683\n",
            "Epoch 17, Batch 300, Loss: 0.5912843722105027\n",
            "Epoch 17, Batch 400, Loss: 0.6005710023641586\n",
            "Epoch 18, Batch 100, Loss: 0.5997451782226563\n",
            "Epoch 18, Batch 200, Loss: 0.583694027364254\n",
            "Epoch 18, Batch 300, Loss: 0.5893223080039024\n",
            "Epoch 18, Batch 400, Loss: 0.5843954935669899\n",
            "Epoch 19, Batch 100, Loss: 0.5939748948812484\n",
            "Epoch 19, Batch 200, Loss: 0.5820674961805343\n",
            "Epoch 19, Batch 300, Loss: 0.5835345152020455\n",
            "Epoch 19, Batch 400, Loss: 0.5753377237915993\n",
            "Epoch 20, Batch 100, Loss: 0.5825954830646515\n",
            "Epoch 20, Batch 200, Loss: 0.56619354814291\n",
            "Epoch 20, Batch 300, Loss: 0.5684338003396988\n",
            "Epoch 20, Batch 400, Loss: 0.5764379400014877\n",
            "Epoch 21, Batch 100, Loss: 0.5639656925201416\n",
            "Epoch 21, Batch 200, Loss: 0.5743381422758103\n",
            "Epoch 21, Batch 300, Loss: 0.5677907398343086\n",
            "Epoch 21, Batch 400, Loss: 0.5708505925536156\n",
            "Epoch 22, Batch 100, Loss: 0.5608981677889824\n",
            "Epoch 22, Batch 200, Loss: 0.5630398184061051\n",
            "Epoch 22, Batch 300, Loss: 0.5639519834518433\n",
            "Epoch 22, Batch 400, Loss: 0.5622798159718514\n",
            "Epoch 23, Batch 100, Loss: 0.5654526993632316\n",
            "Epoch 23, Batch 200, Loss: 0.5630958399176598\n",
            "Epoch 23, Batch 300, Loss: 0.5542504626512528\n",
            "Epoch 23, Batch 400, Loss: 0.545399848818779\n",
            "Epoch 24, Batch 100, Loss: 0.5687681314349174\n",
            "Epoch 24, Batch 200, Loss: 0.553362082540989\n",
            "Epoch 24, Batch 300, Loss: 0.5402263954281807\n",
            "Epoch 24, Batch 400, Loss: 0.5467722174525261\n",
            "Epoch 25, Batch 100, Loss: 0.5502891632914543\n",
            "Epoch 25, Batch 200, Loss: 0.5481092065572739\n",
            "Epoch 25, Batch 300, Loss: 0.5578062158823013\n",
            "Epoch 25, Batch 400, Loss: 0.538028375506401\n",
            "Finished Training\n",
            "Accuracy on test set: 0.7954%\n",
            "\n",
            "Configuration: {'hidden_sizes': [256], 'num_layers': 1, 'lr': 0.002, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
            "Epoch 1, Batch 100, Loss: 0.6828309881687165\n",
            "Epoch 1, Batch 200, Loss: 0.47805932044982913\n",
            "Epoch 1, Batch 300, Loss: 0.43665516406297683\n",
            "Epoch 1, Batch 400, Loss: 0.4307547870278359\n",
            "Epoch 2, Batch 100, Loss: 0.38220267727971075\n",
            "Epoch 2, Batch 200, Loss: 0.3745302015542984\n",
            "Epoch 2, Batch 300, Loss: 0.37105223327875136\n",
            "Epoch 2, Batch 400, Loss: 0.373955020904541\n",
            "Epoch 3, Batch 100, Loss: 0.331986483335495\n",
            "Epoch 3, Batch 200, Loss: 0.3494704726338387\n",
            "Epoch 3, Batch 300, Loss: 0.3227392089366913\n",
            "Epoch 3, Batch 400, Loss: 0.3457035419344902\n",
            "Epoch 4, Batch 100, Loss: 0.3128202989697456\n",
            "Epoch 4, Batch 200, Loss: 0.31574913397431376\n",
            "Epoch 4, Batch 300, Loss: 0.32238907560706137\n",
            "Epoch 4, Batch 400, Loss: 0.3003281705081463\n",
            "Epoch 5, Batch 100, Loss: 0.2898992639780045\n",
            "Epoch 5, Batch 200, Loss: 0.2890330101549625\n",
            "Epoch 5, Batch 300, Loss: 0.30148094952106474\n",
            "Epoch 5, Batch 400, Loss: 0.28936155036091804\n",
            "Epoch 6, Batch 100, Loss: 0.2636422474682331\n",
            "Epoch 6, Batch 200, Loss: 0.2638701495528221\n",
            "Epoch 6, Batch 300, Loss: 0.27706079944968226\n",
            "Epoch 6, Batch 400, Loss: 0.27682232454419137\n",
            "Epoch 7, Batch 100, Loss: 0.2637272797524929\n",
            "Epoch 7, Batch 200, Loss: 0.2605415624380112\n",
            "Epoch 7, Batch 300, Loss: 0.2607216300070286\n",
            "Epoch 7, Batch 400, Loss: 0.2725223734974861\n",
            "Epoch 8, Batch 100, Loss: 0.25147075712680816\n",
            "Epoch 8, Batch 200, Loss: 0.25709432393312454\n",
            "Epoch 8, Batch 300, Loss: 0.2519235757738352\n",
            "Epoch 8, Batch 400, Loss: 0.25745214805006983\n",
            "Epoch 9, Batch 100, Loss: 0.238996557071805\n",
            "Epoch 9, Batch 200, Loss: 0.2448308327794075\n",
            "Epoch 9, Batch 300, Loss: 0.24556293837726118\n",
            "Epoch 9, Batch 400, Loss: 0.2544429603219032\n",
            "Epoch 10, Batch 100, Loss: 0.2327357979118824\n",
            "Epoch 10, Batch 200, Loss: 0.23548611924052237\n",
            "Epoch 10, Batch 300, Loss: 0.21681107349693776\n",
            "Epoch 10, Batch 400, Loss: 0.2326632709801197\n",
            "Epoch 11, Batch 100, Loss: 0.21784734345972537\n",
            "Epoch 11, Batch 200, Loss: 0.22365390621125697\n",
            "Epoch 11, Batch 300, Loss: 0.21794818952679634\n",
            "Epoch 11, Batch 400, Loss: 0.2347893316298723\n",
            "Epoch 12, Batch 100, Loss: 0.22145034424960613\n",
            "Epoch 12, Batch 200, Loss: 0.21592883937060833\n",
            "Epoch 12, Batch 300, Loss: 0.2212400856614113\n",
            "Epoch 12, Batch 400, Loss: 0.2183946405351162\n",
            "Epoch 13, Batch 100, Loss: 0.2100384806841612\n",
            "Epoch 13, Batch 200, Loss: 0.20467772535979747\n",
            "Epoch 13, Batch 300, Loss: 0.21238294787704945\n",
            "Epoch 13, Batch 400, Loss: 0.21284680366516112\n",
            "Epoch 14, Batch 100, Loss: 0.19268686845898628\n",
            "Epoch 14, Batch 200, Loss: 0.20226383976638318\n",
            "Epoch 14, Batch 300, Loss: 0.20091560311615467\n",
            "Epoch 14, Batch 400, Loss: 0.21215856857597828\n",
            "Epoch 15, Batch 100, Loss: 0.1989502201974392\n",
            "Epoch 15, Batch 200, Loss: 0.19927279956638813\n",
            "Epoch 15, Batch 300, Loss: 0.1950607617199421\n",
            "Epoch 15, Batch 400, Loss: 0.1920087491720915\n",
            "Epoch 16, Batch 100, Loss: 0.1857664444297552\n",
            "Epoch 16, Batch 200, Loss: 0.1985821757465601\n",
            "Epoch 16, Batch 300, Loss: 0.18879346065223218\n",
            "Epoch 16, Batch 400, Loss: 0.19232317082583905\n",
            "Epoch 17, Batch 100, Loss: 0.17829030565917492\n",
            "Epoch 17, Batch 200, Loss: 0.17746001735329628\n",
            "Epoch 17, Batch 300, Loss: 0.17970096088945867\n",
            "Epoch 17, Batch 400, Loss: 0.19502708643674851\n",
            "Epoch 18, Batch 100, Loss: 0.16295864008367061\n",
            "Epoch 18, Batch 200, Loss: 0.1771520482003689\n",
            "Epoch 18, Batch 300, Loss: 0.1851365412771702\n",
            "Epoch 18, Batch 400, Loss: 0.19274164870381355\n",
            "Epoch 19, Batch 100, Loss: 0.15912062659859658\n",
            "Epoch 19, Batch 200, Loss: 0.17463524661958219\n",
            "Epoch 19, Batch 300, Loss: 0.17561502180993557\n",
            "Epoch 19, Batch 400, Loss: 0.18412935301661493\n",
            "Epoch 20, Batch 100, Loss: 0.1585894415155053\n",
            "Epoch 20, Batch 200, Loss: 0.16525073401629925\n",
            "Epoch 20, Batch 300, Loss: 0.17225104004144667\n",
            "Epoch 20, Batch 400, Loss: 0.17244499869644642\n",
            "Epoch 21, Batch 100, Loss: 0.15231821164488793\n",
            "Epoch 21, Batch 200, Loss: 0.17228418610990048\n",
            "Epoch 21, Batch 300, Loss: 0.1671084725111723\n",
            "Epoch 21, Batch 400, Loss: 0.1567833972722292\n",
            "Epoch 22, Batch 100, Loss: 0.15123913452029228\n",
            "Epoch 22, Batch 200, Loss: 0.16440830364823342\n",
            "Epoch 22, Batch 300, Loss: 0.15359776087105273\n",
            "Epoch 22, Batch 400, Loss: 0.16979170680046082\n",
            "Epoch 23, Batch 100, Loss: 0.15236401136964559\n",
            "Epoch 23, Batch 200, Loss: 0.14750270195305348\n",
            "Epoch 23, Batch 300, Loss: 0.1617887756228447\n",
            "Epoch 23, Batch 400, Loss: 0.16349514733999967\n",
            "Epoch 24, Batch 100, Loss: 0.14783808961510658\n",
            "Epoch 24, Batch 200, Loss: 0.14625531747937204\n",
            "Epoch 24, Batch 300, Loss: 0.15012256816029548\n",
            "Epoch 24, Batch 400, Loss: 0.1641965375468135\n",
            "Epoch 25, Batch 100, Loss: 0.1502229478955269\n",
            "Epoch 25, Batch 200, Loss: 0.1498223016038537\n",
            "Epoch 25, Batch 300, Loss: 0.15599401582032443\n",
            "Epoch 25, Batch 400, Loss: 0.15416653484106063\n",
            "Finished Training\n",
            "Accuracy on test set: 0.8822%\n",
            "\n",
            "Best Configuration:\n",
            "Hidden Sizes: [256, 128, 64, 32, 16], Num Layers: 5, Learning Rate: 0.002, Optimizer: Adam, Accuracy: 0.8854\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define a function to create the model with variable hidden layer sizes and number of layers\n",
        "def create_model(hidden_sizes):\n",
        "    layers = []\n",
        "    layers.append(nn.Linear(28 * 28, hidden_sizes[0]))\n",
        "    layers.append(nn.ReLU())\n",
        "    for i in range(1, len(hidden_sizes)):\n",
        "        layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
        "        layers.append(nn.ReLU())\n",
        "    layers.append(nn.Linear(hidden_sizes[-1], 10))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Define training and testing datasets and loaders\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)  # Change batch size to 128\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)   # Change batch size to 128\n",
        "\n",
        "# Define a function to train and evaluate the model\n",
        "def train_and_evaluate_model(model, optimizer, criterion, train_loader, test_loader, num_epochs=25):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs.view(-1, 28 * 28))\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:  # print every 100 mini-batches\n",
        "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            outputs = model(images.view(-1, 28 * 28))\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Accuracy on test set: {accuracy}%')\n",
        "    return accuracy\n",
        "\n",
        "# Define configurations with learning rates and optimizers\n",
        "configs = [\n",
        "    {\"hidden_sizes\": [32, 16], \"num_layers\": 2, \"lr\": 0.001, \"optimizer\": optim.SGD},\n",
        "    {\"hidden_sizes\": [32, 16], \"num_layers\": 2, \"lr\": 0.002, \"optimizer\": optim.Adam},\n",
        "    {\"hidden_sizes\": [32, 16], \"num_layers\": 2, \"lr\": 0.003, \"optimizer\": optim.RMSprop},\n",
        "    {\"hidden_sizes\": [64, 32, 16], \"num_layers\": 3, \"lr\": 0.002, \"optimizer\": optim.Adam},\n",
        "    {\"hidden_sizes\": [64, 32, 16], \"num_layers\": 3, \"lr\": 0.003, \"optimizer\": optim.RMSprop},\n",
        "\n",
        "    {\"hidden_sizes\": [128, 64, 32, 16], \"num_layers\": 4, \"lr\": 0.001, \"optimizer\": optim.SGD},\n",
        "    {\"hidden_sizes\": [128, 64, 32, 16], \"num_layers\": 4, \"lr\": 0.003, \"optimizer\": optim.RMSprop},\n",
        "\n",
        "    {\"hidden_sizes\": [256, 128, 64, 32, 16], \"num_layers\": 5, \"lr\": 0.001, \"optimizer\": optim.SGD},\n",
        "    {\"hidden_sizes\": [256, 128, 64, 32, 16], \"num_layers\": 5, \"lr\": 0.002, \"optimizer\": optim.Adam},\n",
        "\n",
        "    {\"hidden_sizes\": [64, 64, 64], \"num_layers\": 3, \"lr\": 0.001, \"optimizer\": optim.SGD},\n",
        "    {\"hidden_sizes\": [64, 64, 64], \"num_layers\": 3, \"lr\": 0.002, \"optimizer\": optim.Adam},\n",
        "    {\"hidden_sizes\": [64, 64, 64], \"num_layers\": 3, \"lr\": 0.003, \"optimizer\": optim.RMSprop},\n",
        "\n",
        "    {\"hidden_sizes\": [128, 128, 128, 128], \"num_layers\": 4, \"lr\": 0.001, \"optimizer\": optim.SGD},\n",
        "    {\"hidden_sizes\": [128, 128, 128, 128], \"num_layers\": 4, \"lr\": 0.003, \"optimizer\": optim.RMSprop},\n",
        "\n",
        "    {\"hidden_sizes\": [256, 128, 64], \"num_layers\": 3, \"lr\": 0.001, \"optimizer\": optim.SGD},\n",
        "\n",
        "    {\"hidden_sizes\": [256, 128, 64], \"num_layers\": 3, \"lr\": 0.003, \"optimizer\": optim.RMSprop},\n",
        "\n",
        "    {\"hidden_sizes\": [128, 64], \"num_layers\": 2, \"lr\": 0.001, \"optimizer\": optim.SGD},\n",
        "\n",
        "    {\"hidden_sizes\": [128, 64], \"num_layers\": 2, \"lr\": 0.003, \"optimizer\": optim.RMSprop},\n",
        "\n",
        "    {\"hidden_sizes\": [32], \"num_layers\": 1, \"lr\": 0.001, \"optimizer\": optim.SGD},\n",
        "\n",
        "    {\"hidden_sizes\": [256], \"num_layers\": 1, \"lr\": 0.002, \"optimizer\": optim.Adam},\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "# Track accuracy for each configuration\n",
        "accuracies = []\n",
        "\n",
        "# Train and evaluate models for each configuration\n",
        "for config in configs:\n",
        "    print(f\"\\nConfiguration: {config}\")\n",
        "    model = create_model(config[\"hidden_sizes\"])\n",
        "    optimizer = config[\"optimizer\"](model.parameters(), lr=config[\"lr\"])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    accuracy = train_and_evaluate_model(model, optimizer, criterion, train_loader, test_loader)\n",
        "    accuracies.append({\"config\": config, \"accuracy\": accuracy})\n",
        "\n",
        "# Print the highest accuracy configuration\n",
        "best_config = max(accuracies, key=lambda x: x[\"accuracy\"])\n",
        "print(\"\\nBest Configuration:\")\n",
        "print(f\"Hidden Sizes: {best_config['config']['hidden_sizes']}, Num Layers: {best_config['config']['num_layers']}, \"\n",
        "      f\"Learning Rate: {best_config['config']['lr']}, Optimizer: {best_config['config']['optimizer'].__name__}, \"\n",
        "      f\"Accuracy: {best_config['accuracy']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part2"
      ],
      "metadata": {
        "id": "WHIV3M9W9k29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Mapping the labels for the MNIST dataset\n",
        "labels_map = {\n",
        "    0: \"0\", 1: \"1\", 2: \"2\", 3: \"3\", 4: \"4\",\n",
        "    5: \"5\", 6: \"6\", 7: \"7\", 8: \"8\", 9: \"9\"\n",
        "}\n",
        "\n",
        "# Load and preprocess the data\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Reshape data for CNN\n",
        "x_train_cnn = np.expand_dims(x_train, axis=-1)\n",
        "x_test_cnn = np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Define function to create CNN model\n",
        "def create_cnn_model(kernel_size=(3, 3), pool_size=(2, 2), activation='relu'):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size, activation=activation, input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPooling2D(pool_size))\n",
        "    model.add(Conv2D(64, kernel_size, activation=activation))\n",
        "    model.add(MaxPooling2D(pool_size))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation=activation))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# Configurations\n",
        "configurations = [\n",
        "    # Configuration 1\n",
        "    {'kernel_size': (3, 3), 'pool_size': (2, 2), 'activation': 'relu'},\n",
        "    # Configuration 2\n",
        "    {'kernel_size': (3, 3), 'pool_size': (2, 2), 'activation': 'tanh'},\n",
        "    # Configuration 3\n",
        "    {'kernel_size': (5, 5), 'pool_size': (2, 2), 'activation': 'relu'},\n",
        "    # Configuration 4\n",
        "    {'kernel_size': (3, 3), 'pool_size': (3, 3), 'activation': 'relu'},\n",
        "    # Configuration 5\n",
        "    {'kernel_size': (5, 5), 'pool_size': (3, 3), 'activation': 'relu'},\n",
        "    # Configuration 6\n",
        "    {'kernel_size': (3, 3), 'pool_size': (2, 2), 'activation': 'sigmoid'},\n",
        "    # Configuration 7\n",
        "    {'kernel_size': (5, 5), 'pool_size': (2, 2), 'activation': 'tanh'},\n",
        "    # Configuration 8\n",
        "    {'kernel_size': (3, 3), 'pool_size': (2, 2), 'activation': 'relu'},\n",
        "    # Configuration 9\n",
        "    {'kernel_size': (5, 5), 'pool_size': (3, 3), 'activation': 'sigmoid'},\n",
        "    # Configuration 10\n",
        "    {'kernel_size': (3, 3), 'pool_size': (2, 2), 'activation': 'relu'},\n",
        "]\n",
        "\n",
        "# Train and evaluate models\n",
        "for i, config in enumerate(configurations):\n",
        "    print(f\"Training and evaluating Model {i+1} with configuration: {config}\")\n",
        "\n",
        "    # Create the model\n",
        "    model = create_cnn_model(kernel_size=config['kernel_size'], pool_size=config['pool_size'], activation=config['activation'])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(x_train_cnn, y_train, epochs=10, batch_size=128, verbose=0, validation_split=0.2)\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(x_test_cnn, y_test)\n",
        "    print(f'Model {i+1} Test accuracy:', accuracy)\n",
        "\n",
        "    # Plot the test image with predicted and actual labels\n",
        "    image_index = 0  # Change this index to view different images\n",
        "    test_image = x_test[image_index]\n",
        "    test_label = np.argmax(y_test[image_index])\n",
        "    test_image_reshaped = np.expand_dims(test_image, axis=0)\n",
        "    predicted_label = np.argmax(model.predict(test_image_reshaped), axis=-1)\n",
        "\n",
        "    plt.imshow(test_image, cmap='gray')\n",
        "    plt.title(f'Predicted Label: {predicted_label[0]}, Actual Label: {test_label}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nsZg4f7k9mA8",
        "outputId": "cfb20621-501a-46ae-923d-9261256a03c2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Training and evaluating Model 1 with configuration: {'kernel_size': (3, 3), 'pool_size': (2, 2), 'activation': 'relu'}\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.2656 - accuracy: 0.9050\n",
            "Model 1 Test accuracy: 0.9049999713897705\n",
            "1/1 [==============================] - 0s 109ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating Model 2 with configuration: {'kernel_size': (3, 3), 'pool_size': (2, 2), 'activation': 'tanh'}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.2843 - accuracy: 0.9001\n",
            "Model 2 Test accuracy: 0.9000999927520752\n",
            "1/1 [==============================] - 0s 71ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating Model 3 with configuration: {'kernel_size': (5, 5), 'pool_size': (2, 2), 'activation': 'relu'}\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2753 - accuracy: 0.9002\n",
            "Model 3 Test accuracy: 0.9002000093460083\n",
            "1/1 [==============================] - 0s 70ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdnUlEQVR4nO3cf1TW9f3/8Qciv9EkApEyJEot0korfxZWCvkjd7bMaTulTgtdarWZdlony344V5mlDauzaccf23LVsi11Wlpm21pqVi4nmpi/0kyFBIXB9fr+4ZfnRwSE1ztFc/fbOf7hxfW83u+L6w133hdvXmHOOScAACQ1OtU7AAA4fRAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBCFk6hVq1YaOnSo/X/FihUKCwvTihUrTtk+HevYfWwIPXr00KWXXnpCH/NUPI8zWY8ePdSjR48G3ebQoUMVHx9/Qh/zVDyP77szNgqzZ89WWFiY/YuOjlbr1q01evRo7d69+1Tvnpe33npLDz/88Cndh7CwMI0ePfqU7sPJtGnTJg0YMEAJCQmKjY1V9+7dtXz58hPy2J9//rkdgwcOHAj8OE888YT+/Oc/n5B9OlFatWqlfv36nerdOGl2796tYcOGKTk5WTExMerQoYMWLFhwqnfrpDpjo1Bp0qRJmjNnjmbMmKGuXbsqLy9PXbp0UUlJSYPvy7XXXqtDhw7p2muv9Zp766239Mgjj5ykvcK2bdvUpUsXvf/++7rvvvs0efJkHTx4UNnZ2Xrvvfe+8+PPnTtXKSkpkqQ//elPgR/ndIzCmayoqEjdu3fXq6++qtzcXD311FNq0qSJBg4cqPnz55/q3TtpGp/qHTjZevfurSuvvFKSNGLECCUmJmrq1Kl64403NHjw4BpniouLFRcXd8L3pVGjRoqOjj7hj4vv5le/+pUOHDigzz77TG3atJEk3XHHHWrbtq3uvfderV69OvBjO+c0f/583XrrrdqyZYvmzZunESNGnKhdx0n0wgsvaNOmTXr77bd1/fXXS5JGjRqlzp076xe/+IUGDBigyMjIU7yXJ94Zf6ZwrMoXd8uWLZL+733MzZs3q0+fPmrSpIl+8pOfSJJCoZCmTZumzMxMRUdHq3nz5srNzdX+/furPKZzTo899pjOO+88xcbG6rrrrtP69eurbbu23yn885//VJ8+fZSQkKC4uDi1b99ezz77rO3f888/L0lV3g6rdKL38bt444031LdvX6WmpioqKkoZGRl69NFHVVFRUeP9V69era5duyomJkbp6emaOXNmtfuUlpZq4sSJuvDCCxUVFaWWLVtq/PjxKi0trXN/Nm/erM2bN9d5v5UrV+qKK66wIEhSbGys+vfvrzVr1ig/P7/Ox6jNqlWrVFBQoEGDBmnQoEF67733tH379mr3C4VCevbZZ9WuXTtFR0crKSlJN954oz766CNJR1774uJivfzyy3YMVP4OZejQoWrVqlW1x3z44YerHCuSNGvWLF1//fVKTk5WVFSULrnkEuXl5QV+fvWxcuVK3XLLLTr//PPtNbz33nt16NChGu//xRdfKCcnR3FxcUpNTdWkSZN07GLO9T3ua/Lll19qw4YN9drvpKQk+54hHfnBbuDAgfrqq6/07rvv1vkY30dn/JnCsSq/SSQmJtpt5eXlysnJUffu3fXUU08pNjZWkpSbm6vZs2dr2LBhGjt2rLZs2aIZM2Zo7dq1WrVqlSIiIiRJDz30kB577DH16dNHffr00Zo1a5Sdna2ysrI692fp0qXq16+fWrRoobvvvlspKSn6/PPP9Ze//EV33323cnNztXPnTi1dulRz5sypNt8Q+1hfs2fPVnx8vH7+858rPj5e77zzjh566CEVFRXpySefrHLf/fv3q0+fPho4cKAGDx6sV155RaNGjVJkZKR++tOfSjryhd+/f3+9//77uvPOO3XxxRfr008/1TPPPKONGzfW+VbKDTfcIEkqKCg47v1KS0uVkJBQ7fbK42D16tW66KKL6vlZqGrevHnKyMjQVVddpUsvvVSxsbH6/e9/r/vuu6/K/YYPH67Zs2erd+/eGjFihMrLy7Vy5Ur94x//0JVXXqk5c+ZoxIgRuvrqq3XnnXdKkjIyMrz3Jy8vT5mZmerfv78aN26sN998Uz/72c8UCoV01113BXqOdVmwYIFKSko0atQoJSYm6sMPP9T06dO1ffv2au/PV1RU6MYbb1Tnzp3161//WosXL9bEiRNVXl6uSZMm2f3qe9zX5Pbbb9e7775bLTTHKi0tVUxMTLXbjz4uevXq5fOp+H5wZ6hZs2Y5SW7ZsmXu66+/dtu2bXN/+MMfXGJioouJiXHbt293zjk3ZMgQJ8ndf//9VeZXrlzpJLl58+ZVuX3x4sVVbt+zZ4+LjIx0ffv2daFQyO73wAMPOEluyJAhdtvy5cudJLd8+XLnnHPl5eUuPT3dpaWluf3791fZztGPddddd7maXqqTsY+1keTuuuuu496npKSk2m25ubkuNjbWHT582G7LyspyktzTTz9tt5WWlrrLL7/cJScnu7KyMuecc3PmzHGNGjVyK1eurPKYM2fOdJLcqlWr7La0tLRqzyMtLc2lpaXV+dxuuukm16xZM1dUVFTl9i5dujhJ7qmnnqrzMWpSVlbmEhMT3S9/+Uu77dZbb3WXXXZZlfu98847TpIbO3Zstcc4+vWKi4ur8bUaMmRIjc9z4sSJ1Y6bml6jnJwcd8EFF1S5LSsry2VlZdXwrKpKS0tzffv2Pe59atrm5MmTXVhYmNu6davdVvm1OGbMGLstFAq5vn37usjISPf111875+p/3Nf2PCqPv7qMGTPGNWrUyBUUFFS5fdCgQU6SGz16dJ2P8X10xr991LNnTyUlJally5YaNGiQ4uPj9frrr+vcc8+tcr9Ro0ZV+f+CBQt01llnqVevXtq7d6/969ixo+Lj4+3KlGXLlqmsrExjxoypcqp+zz331Llva9eu1ZYtW3TPPfeoWbNmVT527Gl/TRpiH30c/VPVt99+q7179+qaa65RSUlJtdP1xo0bKzc31/4fGRmp3Nxc7dmzx97DX7BggS6++GK1bdu2yvOrPJ2v6+qggoKCOs8SpCOv/YEDB/TjH/9Ya9eu1caNG3XPPffYWze1vc1Rl0WLFumbb76p8rurwYMHa926dVXeunv11VcVFhamiRMnVnuM+hwHPo5+jQoLC7V3715lZWXpiy++UGFh4QndVk3bLC4u1t69e9W1a1c557R27dpq9z/6KrfKq97Kysq0bNkySfU/7muzYsWKOs8SpCO/gwwPD9fAgQP1wQcfaPPmzZo8ebJef/11ScGPi9PdGf/20fPPP6/WrVurcePGat68udq0aaNGjaq2sHHjxjrvvPOq3Jafn6/CwkIlJyfX+Lh79uyRJG3dulWSqr29kJSUVONbEkerfCsr6DX7DbGPPtavX68HH3xQ77zzjoqKiqp87NhvOKmpqdV+md+6dWtJR76Zd+7cWfn5+fr888+VlJRU4/Yqn9931bt3b02fPl3333+/OnToIEm68MIL9fjjj2v8+PGBr52fO3eu0tPTFRUVpU2bNkk68pZPbGys5s2bpyeeeELSkeMgNTVVZ5999gl5PsezatUqTZw4UX//+9+rXYFXWFios84664Rv88svv9RDDz2khQsXVnvP/9jjolGjRrrggguq3Hb0cSHV/7j/rtq3b6/58+dr5MiR6tatmyQpJSVF06ZN06hRo07431ScLs74KFx99dV29VFtoqKiqoUiFAopOTlZ8+bNq3Gmtm9UDel02scDBw4oKytLTZs21aRJk5SRkaHo6GitWbNGEyZMUCgU8n7MUCikdu3aaerUqTV+vGXLlt91t83o0aM1bNgwffLJJ4qMjNTll1+u3/72t5L+75uSj6KiIr355ps6fPhwjb+PmD9/vh5//PETciZQ22Mc+wv+zZs364YbblDbtm01depUtWzZUpGRkXrrrbf0zDPPBHqN6lJRUaFevXpp3759mjBhgtq2bau4uDjt2LFDQ4cODXxcNNRxP2DAAPXv31/r1q1TRUWFOnToYBeKBDkuvg/O+CgElZGRoWXLlqlbt241/rKpUlpamqQjP70c/RPO119/XeeVEJW/KPzss8/Us2fPWu9X2xd9Q+xjfa1YsULffPONXnvttSp/h1F5ldexdu7cWe3S340bN0qSXUmTkZGhdevW6YYbbjjhb6PUJC4uTl26dLH/L1u2TDExMfZToo/XXntNhw8fVl5ens4555wqH/vPf/6jBx98UKtWrVL37t2VkZGhJUuWaN++fcc9W6jtc5CQkFDjH8VVniFWevPNN1VaWqqFCxfq/PPPt9tP1B/p1eTTTz/Vxo0b9fLLL+v222+325cuXVrj/UOhkL744osq33BrOi7qc9yfKJGRkbrqqqvs/5VvYx3va/b77Iz/nUJQAwcOVEVFhR599NFqHysvL7cvwp49eyoiIkLTp0+v8j7ltGnT6txGhw4dlJ6ermnTplX7oj76sSq/cR57n4bYx/oKDw+vtt9lZWX6zW9+U+P9y8vL9cILL1S57wsvvKCkpCR17NhR0pHnt2PHDr300kvV5g8dOqTi4uLj7lN9L0mtyQcffKDXXntNw4cPD/SWyty5c3XBBRdo5MiRGjBgQJV/48aNU3x8vP2ke/PNN8s5V+MfKB57HNT0zT8jI0OFhYX65JNP7LZdu3bZe9+VanqNCgsLNWvWLO/nV181bdM5Z5dc12TGjBlV7jtjxgxFRETY1WT1Pe5rU99LUmuSn5+vmTNnql+/fpwp/K/JyspSbm6uJk+erI8//ljZ2dmKiIhQfn6+FixYoGeffVYDBgxQUlKSxo0bp8mTJ6tfv37q06eP1q5dq0WLFlX7CfFYjRo1Ul5enm666SZdfvnlGjZsmFq0aKENGzZo/fr1WrJkiSTZN8mxY8cqJydH4eHhGjRoUIPs49E++ugjPfbYY9Vu79Gjh7p27aqEhAQNGTJEY8eOVVhYmObMmVPrL/RSU1M1ZcoUFRQUqHXr1vrjH/+ojz/+WC+++KJdTnjbbbfplVde0ciRI7V8+XJ169ZNFRUV2rBhg1555RUtWbLkuG8N1veS1K1bt2rgwIHq37+/UlJStH79es2cOVPt27e39/0rVV4GOWvWrFrXWtq5c6eWL1+usWPH1vjxqKgo5eTkaMGCBXruued03XXX6bbbbtNzzz2n/Px83XjjjQqFQlq5cqWuu+46+8Vrx44dtWzZMk2dOlWpqalKT09Xp06dNGjQIE2YMEE//OEPNXbsWJWUlCgvL0+tW7fWmjVrbLvZ2dmKjIzUTTfdpNzcXB08eFAvvfSSkpOTtWvXruN+jo5n06ZNNR4XV1xxhbKzs5WRkaFx48Zpx44datq0qV599dVaz1Cjo6O1ePFiDRkyRJ06ddKiRYv017/+VQ888IC9LVTf47429b0kVZIuueQS+xuLLVu2KC8vT2effXaNf1NzxjgVlzw1hMpLUv/1r38d935DhgxxcXFxtX78xRdfdB07dnQxMTGuSZMmrl27dm78+PFu586ddp+Kigr3yCOPuBYtWriYmBjXo0cP99lnn1W7TPLYS1Irvf/++65Xr16uSZMmLi4uzrVv395Nnz7dPl5eXu7GjBnjkpKSXFhYWLXL6U7kPtZGUq3/Hn30Ueecc6tWrXKdO3d2MTExLjU11Y0fP94tWbKk2nPOyspymZmZ7qOPPnJdunRx0dHRLi0tzc2YMaPadsvKytyUKVNcZmami4qKcgkJCa5jx47ukUcecYWFhXa/73JJ6r59+9wPfvADl5KS4iIjI116erqbMGFCtUtUnXNu+vTpTpJbvHhxrY/39NNPO0nu7bffrvU+s2fPdpLcG2+84Zw78ho/+eSTrm3bti4yMtIlJSW53r17u9WrV9vMhg0b3LXXXutiYmKqXUr8t7/9zV166aUuMjLStWnTxs2dO7fGS1IXLlzo2rdv76Kjo12rVq3clClT3O9+9zsnyW3ZssXu53NJam3HxfDhw51zzv373/92PXv2dPHx8e6cc85xd9xxh1u3bp2T5GbNmmWPVfm1uHnzZpedne1iY2Nd8+bN3cSJE11FRUW1bdfnuP8ul6Q6d+Ty05YtW7rIyEiXmprqRo4c6Xbv3l2v2e+rMOfqkUsAko68dVFQUKAPP/zwVO8KcFLw9hFQT845rVixQnPnzj3VuwKcNJwpAAAMVx8BAAxRAAAYogAAMEQBAGDqffVRQywzAAA4eepzXRFnCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAABM41O9AwBOL+Hh4d4zoVDIe8Y55z0TVFRUlPdMaWmp98yFF17oPSNJmzZtCjR3MnCmAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAMMqqTgjhYWFNchMkNVBzz33XO8ZSerSpYv3zKJFi7xniouLvWdOd0FWPA3i5ptvDjQ3ZcqUE7wnwXGmAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAYUE84P8LsrhdENdcc02guU6dOnnPpKames8899xz3jOnu+TkZO+ZnJwc75mioiLvmdMNZwoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABgWxMMZKTw83HumvLzce+bKK6/0nrn44ou9ZyRp9+7d3jMXXXSR98zrr7/uPbNv3z7vmZiYGO8ZSdq6dav3TGJiovdM06ZNvWe2b9/uPXO64UwBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAADDgng47TVq5P+zS5DF7eLi4rxnbrnlFu+Z0tJS7xlJio6O9p5p0qSJ90xYWJj3TJDXKMh2JCkzM9N7Ztu2bd4z+/fv955p3Pj7/y2VMwUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAACY7/+Sft8DQVaDdM4F2laQ1SqDbCvITHh4uPeMJFVUVASa8zVy5Ejvma+++sp75vDhw94zktSqVSvvmSArq+7evdt7JshrGwqFvGckqbi42HumrKzMe6Zp06beM1FRUd4zUrAVeoN8HuqDMwUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAMz/9IJ4DbVQXdDF7YIIusiYryALoDXUwnaSNHjwYO+ZlJQU75k1a9Z4z0RERHjPSFKzZs28Z7755hvvmX379nnPnHPOOd4zTZo08Z6Rgi+s6CvI4pKxsbGBtnXRRRd5z3z88ceBtlUXzhQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADD/0wviNdRCdUEW1goyIwVbdC7I56EhF7cbNmyY90ybNm28Z7Zt2+Y9E2QhuCALMUpSTEyM98yOHTu8Z4IsVBdkIcaSkhLvGUmKjo72nmmoxS+DysnJ8Z5hQTwAwElHFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAACY025BvKALwQURZMGrIAtrBVksLMhMQ0pNTfWe+dGPfhRoW0EWgsvPz/eeiY+P956JiorynklMTPSekaSysjLvmSDHeGxsrPdMEEEXVSwtLW2QbRUXF3vPBP267datW6C5k4EzBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAATL0XxAsPD/d+8CCLUJ3uC8EFWWAsiKSkpEBzaWlp3jNt27b1nmnRooX3TJAF3SSpqKjIe6ZZs2beM02bNvWeiYiI8J4JsoieFOxrI8jxEOQ5HThwwHvmv//9r/eMFOzzEGShzUOHDnnPBPk+KUnffvut90xmZmagbdWFMwUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAACYeq+SGmTF0yCaN28eaC7IapBxcXENMhMTE+M9k56e7j0jSbGxsd4zQVarPHjwoPdMkJUqJemss87yngnyOS8vL/eeCfL5Likp8Z6RpNLSUu+ZyMhI75ldu3Z5zwR5jYJ87iRp//793jPx8fHeMwkJCd4zxcXF3jOSlJKS4j2TmJgYaFt14UwBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABT7wXxgujZs6f3TGpqaqBtBVnULTk52XsmyKJuoVDIeybI85Gkb7/91nsmyGJhQRbwCgsL856RpKioKO+ZIIumBXltg3zuwsPDvWekYIutBTkeCgsLvWeCfC01pCDHQ5Cv2yALMUrBFi4MsoBjfXCmAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAqfeCeNnZ2d4PPnz4cO+ZDRs2eM9I0q5du7xnioqKvGeCLGZWVlbWINsJKsiiaUEW8KqoqPCekaSmTZt6zwRZfC/IYmZBFk2LiIjwnpGCLULYvHlz75nMzEzvmSDPqSGP8SCLCcbGxnrPHD582HtGCrZ/e/bsCbStunCmAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAqfeCeB9++KH3g3fu3Nl7pl27dt4zktStW7dAc77Ky8u9Z4IsOLdv3z7vmaBzhYWF3jNBFsQLskidJCUmJnrPtGnTxnsmyAJoQRbrc855z0jSZZdd5j3zySefeM8UFBR4z/Ts2dN7JioqyntGCv758xXka33Hjh2BthVkcc74+PhA26oLZwoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAAJgwV8/VpYIuZtZQgiwO1alTJ++Z1q1be8907drVeyY5Odl7Rgq2QFtcXJz3TJDjIehCZqFQyHsmyMKAGzZs8J5ZunSp98yiRYu8ZyTp8OHDgeYawsKFC71nzj///EDb2rt3r/dMkEUpg8wEWURPkkpLS71nxo0b5z1z8ODBOu/DmQIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAADMGbNKKgDg+Orz7Z4zBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwjet7R+fcydwPAMBpgDMFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAID5fxQnbhDASIeEAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating Model 4 with configuration: {'kernel_size': (3, 3), 'pool_size': (3, 3), 'activation': 'relu'}\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.3095 - accuracy: 0.8902\n",
            "Model 4 Test accuracy: 0.8902000188827515\n",
            "1/1 [==============================] - 0s 130ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating Model 5 with configuration: {'kernel_size': (5, 5), 'pool_size': (3, 3), 'activation': 'relu'}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.3196 - accuracy: 0.8862\n",
            "Model 5 Test accuracy: 0.8862000107765198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7c537919c4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 103ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating Model 6 with configuration: {'kernel_size': (3, 3), 'pool_size': (2, 2), 'activation': 'sigmoid'}\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3931 - accuracy: 0.8574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7c53793c2dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 6 Test accuracy: 0.8574000000953674\n",
            "1/1 [==============================] - 0s 78ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating Model 7 with configuration: {'kernel_size': (5, 5), 'pool_size': (2, 2), 'activation': 'tanh'}\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.2857 - accuracy: 0.9042\n",
            "Model 7 Test accuracy: 0.90420001745224\n",
            "1/1 [==============================] - 0s 67ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating Model 8 with configuration: {'kernel_size': (3, 3), 'pool_size': (2, 2), 'activation': 'relu'}\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.2710 - accuracy: 0.9012\n",
            "Model 8 Test accuracy: 0.901199996471405\n",
            "1/1 [==============================] - 0s 86ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating Model 9 with configuration: {'kernel_size': (5, 5), 'pool_size': (3, 3), 'activation': 'sigmoid'}\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.4170 - accuracy: 0.8473\n",
            "Model 9 Test accuracy: 0.8472999930381775\n",
            "1/1 [==============================] - 0s 74ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating Model 10 with configuration: {'kernel_size': (3, 3), 'pool_size': (2, 2), 'activation': 'relu'}\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.2882 - accuracy: 0.8972\n",
            "Model 10 Test accuracy: 0.8971999883651733\n",
            "1/1 [==============================] - 0s 159ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdnUlEQVR4nO3cf1TW9f3/8Qciv9EkApEyJEot0korfxZWCvkjd7bMaTulTgtdarWZdlony344V5mlDauzaccf23LVsi11Wlpm21pqVi4nmpi/0kyFBIXB9fr+4ZfnRwSE1ztFc/fbOf7hxfW83u+L6w133hdvXmHOOScAACQ1OtU7AAA4fRAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBCFk6hVq1YaOnSo/X/FihUKCwvTihUrTtk+HevYfWwIPXr00KWXXnpCH/NUPI8zWY8ePdSjR48G3ebQoUMVHx9/Qh/zVDyP77szNgqzZ89WWFiY/YuOjlbr1q01evRo7d69+1Tvnpe33npLDz/88Cndh7CwMI0ePfqU7sPJtGnTJg0YMEAJCQmKjY1V9+7dtXz58hPy2J9//rkdgwcOHAj8OE888YT+/Oc/n5B9OlFatWqlfv36nerdOGl2796tYcOGKTk5WTExMerQoYMWLFhwqnfrpDpjo1Bp0qRJmjNnjmbMmKGuXbsqLy9PXbp0UUlJSYPvy7XXXqtDhw7p2muv9Zp766239Mgjj5ykvcK2bdvUpUsXvf/++7rvvvs0efJkHTx4UNnZ2Xrvvfe+8+PPnTtXKSkpkqQ//elPgR/ndIzCmayoqEjdu3fXq6++qtzcXD311FNq0qSJBg4cqPnz55/q3TtpGp/qHTjZevfurSuvvFKSNGLECCUmJmrq1Kl64403NHjw4BpniouLFRcXd8L3pVGjRoqOjj7hj4vv5le/+pUOHDigzz77TG3atJEk3XHHHWrbtq3uvfderV69OvBjO+c0f/583XrrrdqyZYvmzZunESNGnKhdx0n0wgsvaNOmTXr77bd1/fXXS5JGjRqlzp076xe/+IUGDBigyMjIU7yXJ94Zf6ZwrMoXd8uWLZL+733MzZs3q0+fPmrSpIl+8pOfSJJCoZCmTZumzMxMRUdHq3nz5srNzdX+/furPKZzTo899pjOO+88xcbG6rrrrtP69eurbbu23yn885//VJ8+fZSQkKC4uDi1b99ezz77rO3f888/L0lV3g6rdKL38bt444031LdvX6WmpioqKkoZGRl69NFHVVFRUeP9V69era5duyomJkbp6emaOXNmtfuUlpZq4sSJuvDCCxUVFaWWLVtq/PjxKi0trXN/Nm/erM2bN9d5v5UrV+qKK66wIEhSbGys+vfvrzVr1ig/P7/Ox6jNqlWrVFBQoEGDBmnQoEF67733tH379mr3C4VCevbZZ9WuXTtFR0crKSlJN954oz766CNJR1774uJivfzyy3YMVP4OZejQoWrVqlW1x3z44YerHCuSNGvWLF1//fVKTk5WVFSULrnkEuXl5QV+fvWxcuVK3XLLLTr//PPtNbz33nt16NChGu//xRdfKCcnR3FxcUpNTdWkSZN07GLO9T3ua/Lll19qw4YN9drvpKQk+54hHfnBbuDAgfrqq6/07rvv1vkY30dn/JnCsSq/SSQmJtpt5eXlysnJUffu3fXUU08pNjZWkpSbm6vZs2dr2LBhGjt2rLZs2aIZM2Zo7dq1WrVqlSIiIiRJDz30kB577DH16dNHffr00Zo1a5Sdna2ysrI692fp0qXq16+fWrRoobvvvlspKSn6/PPP9Ze//EV33323cnNztXPnTi1dulRz5sypNt8Q+1hfs2fPVnx8vH7+858rPj5e77zzjh566CEVFRXpySefrHLf/fv3q0+fPho4cKAGDx6sV155RaNGjVJkZKR++tOfSjryhd+/f3+9//77uvPOO3XxxRfr008/1TPPPKONGzfW+VbKDTfcIEkqKCg47v1KS0uVkJBQ7fbK42D16tW66KKL6vlZqGrevHnKyMjQVVddpUsvvVSxsbH6/e9/r/vuu6/K/YYPH67Zs2erd+/eGjFihMrLy7Vy5Ur94x//0JVXXqk5c+ZoxIgRuvrqq3XnnXdKkjIyMrz3Jy8vT5mZmerfv78aN26sN998Uz/72c8UCoV01113BXqOdVmwYIFKSko0atQoJSYm6sMPP9T06dO1ffv2au/PV1RU6MYbb1Tnzp3161//WosXL9bEiRNVXl6uSZMm2f3qe9zX5Pbbb9e7775bLTTHKi0tVUxMTLXbjz4uevXq5fOp+H5wZ6hZs2Y5SW7ZsmXu66+/dtu2bXN/+MMfXGJioouJiXHbt293zjk3ZMgQJ8ndf//9VeZXrlzpJLl58+ZVuX3x4sVVbt+zZ4+LjIx0ffv2daFQyO73wAMPOEluyJAhdtvy5cudJLd8+XLnnHPl5eUuPT3dpaWluf3791fZztGPddddd7maXqqTsY+1keTuuuuu496npKSk2m25ubkuNjbWHT582G7LyspyktzTTz9tt5WWlrrLL7/cJScnu7KyMuecc3PmzHGNGjVyK1eurPKYM2fOdJLcqlWr7La0tLRqzyMtLc2lpaXV+dxuuukm16xZM1dUVFTl9i5dujhJ7qmnnqrzMWpSVlbmEhMT3S9/+Uu77dZbb3WXXXZZlfu98847TpIbO3Zstcc4+vWKi4ur8bUaMmRIjc9z4sSJ1Y6bml6jnJwcd8EFF1S5LSsry2VlZdXwrKpKS0tzffv2Pe59atrm5MmTXVhYmNu6davdVvm1OGbMGLstFAq5vn37usjISPf111875+p/3Nf2PCqPv7qMGTPGNWrUyBUUFFS5fdCgQU6SGz16dJ2P8X10xr991LNnTyUlJally5YaNGiQ4uPj9frrr+vcc8+tcr9Ro0ZV+f+CBQt01llnqVevXtq7d6/969ixo+Lj4+3KlGXLlqmsrExjxoypcqp+zz331Llva9eu1ZYtW3TPPfeoWbNmVT527Gl/TRpiH30c/VPVt99+q7179+qaa65RSUlJtdP1xo0bKzc31/4fGRmp3Nxc7dmzx97DX7BggS6++GK1bdu2yvOrPJ2v6+qggoKCOs8SpCOv/YEDB/TjH/9Ya9eu1caNG3XPPffYWze1vc1Rl0WLFumbb76p8rurwYMHa926dVXeunv11VcVFhamiRMnVnuM+hwHPo5+jQoLC7V3715lZWXpiy++UGFh4QndVk3bLC4u1t69e9W1a1c557R27dpq9z/6KrfKq97Kysq0bNkySfU/7muzYsWKOs8SpCO/gwwPD9fAgQP1wQcfaPPmzZo8ebJef/11ScGPi9PdGf/20fPPP6/WrVurcePGat68udq0aaNGjaq2sHHjxjrvvPOq3Jafn6/CwkIlJyfX+Lh79uyRJG3dulWSqr29kJSUVONbEkerfCsr6DX7DbGPPtavX68HH3xQ77zzjoqKiqp87NhvOKmpqdV+md+6dWtJR76Zd+7cWfn5+fr888+VlJRU4/Yqn9931bt3b02fPl3333+/OnToIEm68MIL9fjjj2v8+PGBr52fO3eu0tPTFRUVpU2bNkk68pZPbGys5s2bpyeeeELSkeMgNTVVZ5999gl5PsezatUqTZw4UX//+9+rXYFXWFios84664Rv88svv9RDDz2khQsXVnvP/9jjolGjRrrggguq3Hb0cSHV/7j/rtq3b6/58+dr5MiR6tatmyQpJSVF06ZN06hRo07431ScLs74KFx99dV29VFtoqKiqoUiFAopOTlZ8+bNq3Gmtm9UDel02scDBw4oKytLTZs21aRJk5SRkaHo6GitWbNGEyZMUCgU8n7MUCikdu3aaerUqTV+vGXLlt91t83o0aM1bNgwffLJJ4qMjNTll1+u3/72t5L+75uSj6KiIr355ps6fPhwjb+PmD9/vh5//PETciZQ22Mc+wv+zZs364YbblDbtm01depUtWzZUpGRkXrrrbf0zDPPBHqN6lJRUaFevXpp3759mjBhgtq2bau4uDjt2LFDQ4cODXxcNNRxP2DAAPXv31/r1q1TRUWFOnToYBeKBDkuvg/O+CgElZGRoWXLlqlbt241/rKpUlpamqQjP70c/RPO119/XeeVEJW/KPzss8/Us2fPWu9X2xd9Q+xjfa1YsULffPONXnvttSp/h1F5ldexdu7cWe3S340bN0qSXUmTkZGhdevW6YYbbjjhb6PUJC4uTl26dLH/L1u2TDExMfZToo/XXntNhw8fVl5ens4555wqH/vPf/6jBx98UKtWrVL37t2VkZGhJUuWaN++fcc9W6jtc5CQkFDjH8VVniFWevPNN1VaWqqFCxfq/PPPt9tP1B/p1eTTTz/Vxo0b9fLLL+v222+325cuXVrj/UOhkL744osq33BrOi7qc9yfKJGRkbrqqqvs/5VvYx3va/b77Iz/nUJQAwcOVEVFhR599NFqHysvL7cvwp49eyoiIkLTp0+v8j7ltGnT6txGhw4dlJ6ermnTplX7oj76sSq/cR57n4bYx/oKDw+vtt9lZWX6zW9+U+P9y8vL9cILL1S57wsvvKCkpCR17NhR0pHnt2PHDr300kvV5g8dOqTi4uLj7lN9L0mtyQcffKDXXntNw4cPD/SWyty5c3XBBRdo5MiRGjBgQJV/48aNU3x8vP2ke/PNN8s5V+MfKB57HNT0zT8jI0OFhYX65JNP7LZdu3bZe9+VanqNCgsLNWvWLO/nV181bdM5Z5dc12TGjBlV7jtjxgxFRETY1WT1Pe5rU99LUmuSn5+vmTNnql+/fpwp/K/JyspSbm6uJk+erI8//ljZ2dmKiIhQfn6+FixYoGeffVYDBgxQUlKSxo0bp8mTJ6tfv37q06eP1q5dq0WLFlX7CfFYjRo1Ul5enm666SZdfvnlGjZsmFq0aKENGzZo/fr1WrJkiSTZN8mxY8cqJydH4eHhGjRoUIPs49E++ugjPfbYY9Vu79Gjh7p27aqEhAQNGTJEY8eOVVhYmObMmVPrL/RSU1M1ZcoUFRQUqHXr1vrjH/+ojz/+WC+++KJdTnjbbbfplVde0ciRI7V8+XJ169ZNFRUV2rBhg1555RUtWbLkuG8N1veS1K1bt2rgwIHq37+/UlJStH79es2cOVPt27e39/0rVV4GOWvWrFrXWtq5c6eWL1+usWPH1vjxqKgo5eTkaMGCBXruued03XXX6bbbbtNzzz2n/Px83XjjjQqFQlq5cqWuu+46+8Vrx44dtWzZMk2dOlWpqalKT09Xp06dNGjQIE2YMEE//OEPNXbsWJWUlCgvL0+tW7fWmjVrbLvZ2dmKjIzUTTfdpNzcXB08eFAvvfSSkpOTtWvXruN+jo5n06ZNNR4XV1xxhbKzs5WRkaFx48Zpx44datq0qV599dVaz1Cjo6O1ePFiDRkyRJ06ddKiRYv017/+VQ888IC9LVTf47429b0kVZIuueQS+xuLLVu2KC8vT2effXaNf1NzxjgVlzw1hMpLUv/1r38d935DhgxxcXFxtX78xRdfdB07dnQxMTGuSZMmrl27dm78+PFu586ddp+Kigr3yCOPuBYtWriYmBjXo0cP99lnn1W7TPLYS1Irvf/++65Xr16uSZMmLi4uzrVv395Nnz7dPl5eXu7GjBnjkpKSXFhYWLXL6U7kPtZGUq3/Hn30Ueecc6tWrXKdO3d2MTExLjU11Y0fP94tWbKk2nPOyspymZmZ7qOPPnJdunRx0dHRLi0tzc2YMaPadsvKytyUKVNcZmami4qKcgkJCa5jx47ukUcecYWFhXa/73JJ6r59+9wPfvADl5KS4iIjI116erqbMGFCtUtUnXNu+vTpTpJbvHhxrY/39NNPO0nu7bffrvU+s2fPdpLcG2+84Zw78ho/+eSTrm3bti4yMtIlJSW53r17u9WrV9vMhg0b3LXXXutiYmKqXUr8t7/9zV166aUuMjLStWnTxs2dO7fGS1IXLlzo2rdv76Kjo12rVq3clClT3O9+9zsnyW3ZssXu53NJam3HxfDhw51zzv373/92PXv2dPHx8e6cc85xd9xxh1u3bp2T5GbNmmWPVfm1uHnzZpedne1iY2Nd8+bN3cSJE11FRUW1bdfnuP8ul6Q6d+Ty05YtW7rIyEiXmprqRo4c6Xbv3l2v2e+rMOfqkUsAko68dVFQUKAPP/zwVO8KcFLw9hFQT845rVixQnPnzj3VuwKcNJwpAAAMVx8BAAxRAAAYogAAMEQBAGDqffVRQywzAAA4eepzXRFnCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAABM41O9AwBOL+Hh4d4zoVDIe8Y55z0TVFRUlPdMaWmp98yFF17oPSNJmzZtCjR3MnCmAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAMMqqTgjhYWFNchMkNVBzz33XO8ZSerSpYv3zKJFi7xniouLvWdOd0FWPA3i5ptvDjQ3ZcqUE7wnwXGmAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAYUE84P8LsrhdENdcc02guU6dOnnPpKames8899xz3jOnu+TkZO+ZnJwc75mioiLvmdMNZwoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABgWxMMZKTw83HumvLzce+bKK6/0nrn44ou9ZyRp9+7d3jMXXXSR98zrr7/uPbNv3z7vmZiYGO8ZSdq6dav3TGJiovdM06ZNvWe2b9/uPXO64UwBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAADDgng47TVq5P+zS5DF7eLi4rxnbrnlFu+Z0tJS7xlJio6O9p5p0qSJ90xYWJj3TJDXKMh2JCkzM9N7Ztu2bd4z+/fv955p3Pj7/y2VMwUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAACY7/+Sft8DQVaDdM4F2laQ1SqDbCvITHh4uPeMJFVUVASa8zVy5Ejvma+++sp75vDhw94zktSqVSvvmSArq+7evdt7JshrGwqFvGckqbi42HumrKzMe6Zp06beM1FRUd4zUrAVeoN8HuqDMwUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAMz/9IJ4DbVQXdDF7YIIusiYryALoDXUwnaSNHjwYO+ZlJQU75k1a9Z4z0RERHjPSFKzZs28Z7755hvvmX379nnPnHPOOd4zTZo08Z6Rgi+s6CvI4pKxsbGBtnXRRRd5z3z88ceBtlUXzhQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADD/0wviNdRCdUEW1goyIwVbdC7I56EhF7cbNmyY90ybNm28Z7Zt2+Y9E2QhuCALMUpSTEyM98yOHTu8Z4IsVBdkIcaSkhLvGUmKjo72nmmoxS+DysnJ8Z5hQTwAwElHFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAACY025BvKALwQURZMGrIAtrBVksLMhMQ0pNTfWe+dGPfhRoW0EWgsvPz/eeiY+P956JiorynklMTPSekaSysjLvmSDHeGxsrPdMEEEXVSwtLW2QbRUXF3vPBP267datW6C5k4EzBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAATL0XxAsPD/d+8CCLUJ3uC8EFWWAsiKSkpEBzaWlp3jNt27b1nmnRooX3TJAF3SSpqKjIe6ZZs2beM02bNvWeiYiI8J4JsoieFOxrI8jxEOQ5HThwwHvmv//9r/eMFOzzEGShzUOHDnnPBPk+KUnffvut90xmZmagbdWFMwUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAACYeq+SGmTF0yCaN28eaC7IapBxcXENMhMTE+M9k56e7j0jSbGxsd4zQVarPHjwoPdMkJUqJemss87yngnyOS8vL/eeCfL5Likp8Z6RpNLSUu+ZyMhI75ldu3Z5zwR5jYJ87iRp//793jPx8fHeMwkJCd4zxcXF3jOSlJKS4j2TmJgYaFt14UwBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABT7wXxgujZs6f3TGpqaqBtBVnULTk52XsmyKJuoVDIeybI85Gkb7/91nsmyGJhQRbwCgsL856RpKioKO+ZIIumBXltg3zuwsPDvWekYIutBTkeCgsLvWeCfC01pCDHQ5Cv2yALMUrBFi4MsoBjfXCmAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAqfeCeNnZ2d4PPnz4cO+ZDRs2eM9I0q5du7xnioqKvGeCLGZWVlbWINsJKsiiaUEW8KqoqPCekaSmTZt6zwRZfC/IYmZBFk2LiIjwnpGCLULYvHlz75nMzEzvmSDPqSGP8SCLCcbGxnrPHD582HtGCrZ/e/bsCbStunCmAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAqfeCeB9++KH3g3fu3Nl7pl27dt4zktStW7dAc77Ky8u9Z4IsOLdv3z7vmaBzhYWF3jNBFsQLskidJCUmJnrPtGnTxnsmyAJoQRbrc855z0jSZZdd5j3zySefeM8UFBR4z/Ts2dN7JioqyntGCv758xXka33Hjh2BthVkcc74+PhA26oLZwoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAAJgwV8/VpYIuZtZQgiwO1alTJ++Z1q1be8907drVeyY5Odl7Rgq2QFtcXJz3TJDjIehCZqFQyHsmyMKAGzZs8J5ZunSp98yiRYu8ZyTp8OHDgeYawsKFC71nzj///EDb2rt3r/dMkEUpg8wEWURPkkpLS71nxo0b5z1z8ODBOu/DmQIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAADMGbNKKgDg+Orz7Z4zBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwjet7R+fcydwPAMBpgDMFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAID5fxQnbhDASIeEAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part3"
      ],
      "metadata": {
        "id": "Ba_-rnuVOego"
      }
    }
  ]
}